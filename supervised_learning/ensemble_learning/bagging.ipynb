{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Bagging stands for Bootstrap Aggregation. It is an ensemble learning technique that aims to improve the performance and stability of machine learning models. The core idea is to train multiple models on different subsets of the original data and then aggregate their predictions to arrive at the final prediction.\n",
    "\n",
    "### How does bagging work?\n",
    "1. Bootstrap sampling: Multiple subsets of the original data are created with replacement (called bootstrap samples). This means data points can be chosen more than once in a single sample.\n",
    "2. Model training: Each base model is trained independently on its respective bootstrap sample.\n",
    "3. Aggregation:\n",
    "    - After training, the predictions from all the base models are combined using a specific technique.\n",
    "    - For example, averaging is one technique which calculates the mean of the individual prediction (82%, 90%. and 78%) to get the final prediction (83.33%).\n",
    "    - Other common aggregation techniques include voting (for classification) and weighted averaging.\n",
    "\n",
    "### Benefits of bagging\n",
    "- Reduced variance: By averaging predictions from different models (trained on slightly different data), bagging helps in reducing the variance of the overall model. This makes the final predictions less sensitive to random fluctuations in the training data and leads to better generalization.\n",
    "- Improved accuracy: Bagging can often achieve higher accuracy compared to a single model, especially for problems with high variance.\n",
    "\n",
    "### Random Forest\n",
    "- Random Forest is a popular ensemble learning technique that follows the bagging principle.\n",
    "- It uses decision trees as base models and incorporates additional randomness during training by randomly selecting features at each split node in the tree.\n",
    "- Random Forests are known for their effectiveness in various machine learning tasks due to their ability to reduce variance and improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Is Random Forest?\n",
    "- Random Forest is an ensemble technique that relies on a multitude of decision trees for prediction.\n",
    "- To improve the overall performance, the tree within the forest need to be different from each other. This prevents the ensemble from overfitting to the training data.\n",
    "\n",
    "### Random sampling for diversity\n",
    "- Row sampling (RS): During tree creation, a random subset of data points (rows) is selected from the original dataset with replacement. This means that a single data point can be chosen multiple times for a single tree.\n",
    "- Column sampling (CS): At each node in a tree, a random subset of features (columns) is selected from the full set of features. The tree then splits the data based on the best split among these chosen features. This injects randomness into the tree structure.\n",
    "\n",
    "### Building the Random Forest\n",
    "The core components of a Random Forest are, DTs + RS + CS + Aggregation. The following is what each one of them mean,\n",
    "- DTs: The Decision Trees that make up the forest.\n",
    "- RS: Row sampling for training data selection.\n",
    "- CS: Column sampling for feature selection at each node.\n",
    "- Aggregation: Combining predictions from all trees (usually majority vote for classification, and averaging for regression).\n",
    "\n",
    "### Hyperparameters in Random Forest\n",
    "- `n_estimators`: This hyperparameter controls the number of trees to be created in the forest. More trees generally lead to better performance but also increase the training time.\n",
    "- `max_features`: This parameter specifies the number of features randomly chosen at each node for splitting. A lower value introduces more randomness and helps prevent overfitting.\n",
    "\n",
    "### What does \"Random\" in Random Forest refer to?\n",
    "- The \"Random\" in Random Forest refers to the random sampling of rows and columns during tree creation, leading to diverse trees.\n",
    "- \"Forest\" signifies the collection of multiple Decision Trees.\n",
    "\n",
    "### Example\n",
    "- With 10 features, a 20% random column sample (2 feature) and a 40% random row sample (400 rows from 1000) would be used to train 1 Tree.\n",
    "- This process is repeated for the specified number of estimators (e.g., 100 trees).\n",
    "\n",
    "### Aggregation and underfitting\n",
    "- Random Forests typically use majority voting for classification (the most frequent class predicted by the Trees wins). Averaging is used for regression.\n",
    "- Individual Trees in a Random Forest might be underfit due to limited data and random features at each split. However, the ensemble combines these weaker learners to create a more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
