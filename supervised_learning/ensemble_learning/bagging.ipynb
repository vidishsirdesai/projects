{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Bagging stands for Bootstrap Aggregation. It is an ensemble learning technique that aims to improve the performance and stability of machine learning models. The core idea is to train multiple models on different subsets of the original data and then aggregate their predictions to arrive at the final prediction.\n",
    "\n",
    "### How does bagging work?\n",
    "1. Bootstrap sampling: Multiple subsets of the original data are created with replacement (called bootstrap samples). This means data points can be chosen more than once in a single sample.\n",
    "2. Model training: Each base model is trained independently on its respective bootstrap sample.\n",
    "3. Aggregation:\n",
    "    - After training, the predictions from all the base models are combined using a specific technique.\n",
    "    - For example, averaging is one technique which calculates the mean of the individual prediction (82%, 90%. and 78%) to get the final prediction (83.33%).\n",
    "    - Other common aggregation techniques include voting (for classification) and weighted averaging.\n",
    "\n",
    "### Benefits of bagging\n",
    "- Reduced variance: By averaging predictions from different models (trained on slightly different data), bagging helps in reducing the variance of the overall model. This makes the final predictions less sensitive to random fluctuations in the training data and leads to better generalization.\n",
    "- Improved accuracy: Bagging can often achieve higher accuracy compared to a single model, especially for problems with high variance.\n",
    "\n",
    "### Random Forest\n",
    "- Random Forest is a popular ensemble learning technique that follows the bagging principle.\n",
    "- It uses decision trees as base models and incorporates additional randomness during training by randomly selecting features at each split node in the tree.\n",
    "- Random Forests are known for their effectiveness in various machine learning tasks due to their ability to reduce variance and improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Is Random Forest?\n",
    "- Random Forest is an ensemble technique that relies on a multitude of decision trees for prediction.\n",
    "- To improve the overall performance, the tree within the forest need to be different from each other. This prevents the ensemble from overfitting to the training data.\n",
    "\n",
    "### Random sampling for diversity\n",
    "- Row sampling (RS): During tree creation, a random subset of data points (rows) is selected from the original dataset with replacement. This means that a single data point can be chosen multiple times for a single tree.\n",
    "- Column sampling (CS): At each node in a tree, a random subset of features (columns) is selected from the full set of features. The tree then splits the data based on the best split among these chosen features. This injects randomness into the tree structure.\n",
    "\n",
    "### Building the Random Forest\n",
    "The core components of a Random Forest are, DTs + RS + CS + Aggregation. The following is what each one of them mean,\n",
    "- DTs: The Decision Trees that make up the forest.\n",
    "- RS: Row sampling for training data selection.\n",
    "- CS: Column sampling for feature selection at each node.\n",
    "- Aggregation: Combining predictions from all trees (usually majority vote for classification, and averaging for regression).\n",
    "\n",
    "### Hyperparameters in Random Forest\n",
    "- `n_estimators`: This hyperparameter controls the number of trees to be created in the forest. More trees generally lead to better performance but also increase the training time.\n",
    "- `max_features`: This parameter specifies the number of features randomly chosen at each node for splitting. A lower value introduces more randomness and helps prevent overfitting.\n",
    "\n",
    "### What does \"Random\" in Random Forest refer to?\n",
    "- The \"Random\" in Random Forest refers to the random sampling of rows and columns during tree creation, leading to diverse trees.\n",
    "- \"Forest\" signifies the collection of multiple Decision Trees.\n",
    "\n",
    "### Example\n",
    "- With 10 features, a 20% random column sample (2 feature) and a 40% random row sample (400 rows from 1000) would be used to train 1 Tree.\n",
    "- This process is repeated for the specified number of estimators (e.g., 100 trees).\n",
    "\n",
    "### Aggregation and underfitting\n",
    "- Random Forests typically use majority voting for classification (the most frequent class predicted by the Trees wins). Averaging is used for regression.\n",
    "- Individual Trees in a Random Forest might be underfit due to limited data and random features at each split. However, the ensemble combines these weaker learners to create a more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm\n",
    "Random Forest is a powerful ensemble learning technique widely used for classification and regression tasks. It leverages the strengths of multiple Decision Trees to create a robust and accurate model.\n",
    "\n",
    "### Core idea\n",
    "- Random Forests build a collection of Decision Trees, each trained on a random subset of the data. This creates diversity among the trees, preventing them from overfitting to the training data.\n",
    "- When making a prediction, the forest combines the predictions from all the individual trees using a voting mechanism for classification or averaging for regression. This reduces the variance of the model and improves its generalization ability.\n",
    "\n",
    "### Key components\n",
    "1. Decision Trees: These are the building blocks of the Forest. Each Tree learns a set of rules for classifying or predicting based on the features in the data.\n",
    "2. Random sampling:\n",
    "    - Row sampling (Bootstrap aggregation): During Tree creation, a random subset of data points (rows) are selected with replacement from the original dataset. This means a data point can be included in a tree multiple times.\n",
    "    - Column sampling (Feature subsetting): At each node in a Tree, a random subset of features (columns) are chosen from the full set. The Tree then splits the data based ob the best split among these chosen features. This injects randomness into the Tree structure.\n",
    "3. Aggregation: This refers to how th predictions from all the Trees are combined to make a final prediction.\n",
    "    - Classification: Majority voting is typically used. The class predicted by most Trees becomes the final prediction.\n",
    "    - Regression: Averaging the predicted values from all Trees in common.\n",
    "\n",
    "### Building the Random Forest\n",
    "1. Define the parameters: Specify the number of Trees (`n_estimators`) to create and the number of features (`max_features`) to randomly choose at each node for splitting.\n",
    "2. Create Trees:\n",
    "    - Draw a bootstrap sample of data points.\n",
    "    - Build a Decision Tree using the sampled data and chosen features (`max_features`) at each node.\n",
    "    - Repeat the above 2 steps for the specified number of Trees (`n_estimators`).\n",
    "3. Prediction:\n",
    "    - For a new data point, make predictions using each Tree in the forest.\n",
    "    - Combine the individual predictions using the chosen aggregation method (voting or averaging).\n",
    "\n",
    "### Advantages of Random Forests\n",
    "- Improved accuracy and generalizability: By combining multiple diverse Trees, Random Forests often achieves higher accuracy than a single Decision Tree and are less prone to overfitting.\n",
    "- Handles missing data well: Decision Trees can naturally handle missing data by splitting on existing features.\n",
    "- Robust to outliers: Ensemble methods like Random Forests are less sensitive to outliers in the data compared to single models.\n",
    "- Provides feature importance: Random Forests can be used to understand the relative importance of features in the model.\n",
    "\n",
    "### Disadvantages of Random Forests\n",
    "- Can be a black box: The inner workings of individual Trees and how they contribute to the final prediction can be difficult to interpret.\n",
    "- Computationally expensive: Training a Random Forest with many Trees can be computationally expensive compared to simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-Of-Bag (OOB) Score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
