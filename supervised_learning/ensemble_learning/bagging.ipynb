{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Bagging stands for Bootstrap Aggregation. It is an ensemble learning technique that aims to improve the performance and stability of machine learning models. The core idea is to train multiple models on different subsets of the original data and then aggregate their predictions to arrive at the final prediction.\n",
    "\n",
    "### How does bagging work?\n",
    "1. Bootstrap sampling: Multiple subsets of the original data are created with replacement (called bootstrap samples). This means data points can be chosen more than once in a single sample.\n",
    "2. Model training: Each base model is trained independently on its respective bootstrap sample.\n",
    "3. Aggregation:\n",
    "    - After training, the predictions from all the base models are combined using a specific technique.\n",
    "    - For example, averaging is one technique which calculates the mean of the individual prediction (82%, 90%. and 78%) to get the final prediction (83.33%).\n",
    "    - Other common aggregation techniques include voting (for classification) and weighted averaging.\n",
    "\n",
    "### Benefits of bagging\n",
    "- Reduced variance: By averaging predictions from different models (trained on slightly different data), bagging helps in reducing the variance of the overall model. This makes the final predictions less sensitive to random fluctuations in the training data and leads to better generalization.\n",
    "- Improved accuracy: Bagging can often achieve higher accuracy compared to a single model, especially for problems with high variance.\n",
    "\n",
    "### Random Forest\n",
    "- Random Forest is a popular ensemble learning technique that follows the bagging principle.\n",
    "- It uses decision trees as base models and incorporates additional randomness during training by randomly selecting features at each split node in the tree.\n",
    "- Random Forests are known for their effectiveness in various machine learning tasks due to their ability to reduce variance and improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Is Random Forest?\n",
    "- Random Forest is an ensemble technique that relies on a multitude of decision trees for prediction.\n",
    "- To improve the overall performance, the tree within the forest need to be different from each other. This prevents the ensemble from overfitting to the training data.\n",
    "\n",
    "### Random sampling for diversity\n",
    "- Row sampling (RS): During tree creation, a random subset of data points (rows) is selected from the original dataset with replacement. This means that a single data point can be chosen multiple times for a single tree.\n",
    "- Column sampling (CS): At each node in a tree, a random subset of features (columns) is selected from the full set of features. The tree then splits the data based on the best split among these chosen features. This injects randomness into the tree structure.\n",
    "\n",
    "### Building the Random Forest\n",
    "The core components of a Random Forest are, DTs + RS + CS + Aggregation. The following is what each one of them mean,\n",
    "- DTs: The Decision Trees that make up the forest.\n",
    "- RS: Row sampling for training data selection.\n",
    "- CS: Column sampling for feature selection at each node.\n",
    "- Aggregation: Combining predictions from all trees (usually majority vote for classification, and averaging for regression).\n",
    "\n",
    "### Hyperparameters in Random Forest\n",
    "- `n_estimators`: This hyperparameter controls the number of trees to be created in the forest. More trees generally lead to better performance but also increase the training time.\n",
    "- `max_features`: This parameter specifies the number of features randomly chosen at each node for splitting. A lower value introduces more randomness and helps prevent overfitting.\n",
    "\n",
    "### What does \"Random\" in Random Forest refer to?\n",
    "- The \"Random\" in Random Forest refers to the random sampling of rows and columns during tree creation, leading to diverse trees.\n",
    "- \"Forest\" signifies the collection of multiple Decision Trees.\n",
    "\n",
    "### Example\n",
    "- With 10 features, a 20% random column sample (2 feature) and a 40% random row sample (400 rows from 1000) would be used to train 1 Tree.\n",
    "- This process is repeated for the specified number of estimators (e.g., 100 trees).\n",
    "\n",
    "### Aggregation and underfitting\n",
    "- Random Forests typically use majority voting for classification (the most frequent class predicted by the Trees wins). Averaging is used for regression.\n",
    "- Individual Trees in a Random Forest might be underfit due to limited data and random features at each split. However, the ensemble combines these weaker learners to create a more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm\n",
    "Random Forest is a powerful ensemble learning technique widely used for classification and regression tasks. It leverages the strengths of multiple Decision Trees to create a robust and accurate model.\n",
    "\n",
    "### Core idea\n",
    "- Random Forests build a collection of Decision Trees, each trained on a random subset of the data. This creates diversity among the trees, preventing them from overfitting to the training data.\n",
    "- When making a prediction, the forest combines the predictions from all the individual trees using a voting mechanism for classification or averaging for regression. This reduces the variance of the model and improves its generalization ability.\n",
    "\n",
    "### Key components\n",
    "1. Decision Trees: These are the building blocks of the Forest. Each Tree learns a set of rules for classifying or predicting based on the features in the data.\n",
    "2. Random sampling:\n",
    "    - Row sampling (Bootstrap aggregation): During Tree creation, a random subset of data points (rows) are selected with replacement from the original dataset. This means a data point can be included in a tree multiple times.\n",
    "    - Column sampling (Feature subsetting): At each node in a Tree, a random subset of features (columns) are chosen from the full set. The Tree then splits the data based ob the best split among these chosen features. This injects randomness into the Tree structure.\n",
    "3. Aggregation: This refers to how th predictions from all the Trees are combined to make a final prediction.\n",
    "    - Classification: Majority voting is typically used. The class predicted by most Trees becomes the final prediction.\n",
    "    - Regression: Averaging the predicted values from all Trees in common.\n",
    "\n",
    "### Building the Random Forest\n",
    "1. Define the parameters: Specify the number of Trees (`n_estimators`) to create and the number of features (`max_features`) to randomly choose at each node for splitting.\n",
    "2. Create Trees:\n",
    "    - Draw a bootstrap sample of data points.\n",
    "    - Build a Decision Tree using the sampled data and chosen features (`max_features`) at each node.\n",
    "    - Repeat the above 2 steps for the specified number of Trees (`n_estimators`).\n",
    "3. Prediction:\n",
    "    - For a new data point, make predictions using each Tree in the forest.\n",
    "    - Combine the individual predictions using the chosen aggregation method (voting or averaging).\n",
    "\n",
    "### Advantages of Random Forests\n",
    "- Improved accuracy and generalizability: By combining multiple diverse Trees, Random Forests often achieves higher accuracy than a single Decision Tree and are less prone to overfitting.\n",
    "- Handles missing data well: Decision Trees can naturally handle missing data by splitting on existing features.\n",
    "- Robust to outliers: Ensemble methods like Random Forests are less sensitive to outliers in the data compared to single models.\n",
    "- Provides feature importance: Random Forests can be used to understand the relative importance of features in the model.\n",
    "\n",
    "### Disadvantages of Random Forests\n",
    "- Can be a black box: The inner workings of individual Trees and how they contribute to the final prediction can be difficult to interpret.\n",
    "- Computationally expensive: Training a Random Forest with many Trees can be computationally expensive compared to simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-Of-Bag (OOB) Score\n",
    "### Core idea\n",
    "- Random Forests use bootstrap aggregation (bagging) to train Decision Trees. Each Tree is trained on a random subset (m rows) of the original data (n rows).\n",
    "- The remaining data points (n - m rows) that are not used to train a paticular Tree are called Out-Of-Bag (OOB) samples for that Tree.\n",
    "\n",
    "### OOB for evaluation\n",
    "Individual Tree evaluation: OOB samples can be used to evaluate the performance of each Tree in the Forest.\n",
    "- The trained Tree predicts the class/ value for each OOB sample.\n",
    "- These predictions are compared with the actual values to calculate the error (e.g., difference for classification, squared difference for regression).\n",
    "\n",
    "### Overall OOB score\n",
    "- Average error: The OOB score is the average of the errors calculated for all OOB samples across all Trees in the Forest.\n",
    "- Cross-validation alternative: This provides an estimate of the model's generalization performance without needing a separate cross-validation process.\n",
    "\n",
    "### Benefits of OOB score\n",
    "- Efficiency: Since OOB samples are readily available during forest training, calculating the OOB score is computationally efficient.\n",
    "- Reduced bias: OOB evaluation avoids bias that can occur when using a separate validation set, as each data point is left out for evaluation at least once.\n",
    "\n",
    "### Example\n",
    "- Consider a dataset with 7 data points (A-F) and 3 Trees ($m_1$, $m_2$, $m_3$).\n",
    "- The OOB data for each model might be,\n",
    "    - $m_1$: B, C\n",
    "    - $m_2$: A, E, F\n",
    "    - $m_3$: C, D\n",
    "- For data point C (present in OOB for $m_1$ and $m_3$),\n",
    "    - Predictions can be made using both the models ($m_1(C)$ and $m_3(C)$).\n",
    "    - Average these predictions ($P(C) = \\frac{m_1(C) + m_3(C)}{2}$).\n",
    "    - Compare this average prediction with the actual value of $C$ ($y_{actual}(C)$) to calculate the error ($error_C = y_{actual}(C) - P(C)$).\n",
    "\n",
    "### Overall OOB error\n",
    "We repeat the above for all data points and all Trees where there are OOB samples. The OOB error is the sum of these individual errors across all data points,\n",
    "    - $error_{OOB} = \\sum_{i = 1}^n(error_i)$.\n",
    "\n",
    "### OOB score v. actual score\n",
    "While OOB score provides a good estimate of performance, it might not always match an explicitly calculated cross-validation score due to random sampling variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
