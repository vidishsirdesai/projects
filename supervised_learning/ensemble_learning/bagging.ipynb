{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Bagging stands for Bootstrap Aggregation. It is an ensemble learning technique that aims to improve the performance and stability of machine learning models. The core idea is to train multiple models on different subsets of the original data and then aggregate their predictions to arrive at the final prediction.\n",
    "\n",
    "### How does bagging work?\n",
    "1. Bootstrap sampling: Multiple subsets of the original data are created with replacement (called bootstrap samples). This means data points can be chosen more than once in a single sample.\n",
    "2. Model training: Each base model is trained independently on its respective bootstrap sample.\n",
    "3. Aggregation:\n",
    "    - After training, the predictions from all the base models are combined using a specific technique.\n",
    "    - For example, averaging is one technique which calculates the mean of the individual prediction (82%, 90%. and 78%) to get the final prediction (83.33%).\n",
    "    - Other common aggregation techniques include voting (for classification) and weighted averaging.\n",
    "\n",
    "### Benefits of bagging\n",
    "- Reduced variance: By averaging predictions from different models (trained on slightly different data), bagging helps in reducing the variance of the overall model. This makes the final predictions less sensitive to random fluctuations in the training data and leads to better generalization.\n",
    "- Improved accuracy: Bagging can often achieve higher accuracy compared to a single model, especially for problems with high variance.\n",
    "\n",
    "### Random Forest\n",
    "- Random Forest is a popular ensemble learning technique that follows the bagging principle.\n",
    "- It uses decision trees as base models and incorporates additional randomness during training by randomly selecting features at each split node in the tree.\n",
    "- Random Forests are known for their effectiveness in various machine learning tasks due to their ability to reduce variance and improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Is Random Forest?\n",
    "- Random Forest is an ensemble technique that relies on a multitude of decision trees for prediction.\n",
    "- To improve the overall performance, the tree within the forest need to be different from each other. This prevents the ensemble from overfitting to the training data.\n",
    "\n",
    "### Random sampling for diversity\n",
    "- Row sampling (RS): During tree creation, a random subset of data points (rows) is selected from the original dataset with replacement. This means that a single data point can be chosen multiple times for a single tree.\n",
    "- Column sampling (CS): At each node in a tree, a random subset of features (columns) is selected from the full set of features. The tree then splits the data based on the best split among these chosen features. This injects randomness into the tree structure.\n",
    "\n",
    "### Building the Random Forest\n",
    "The core components of a Random Forest are, DTs + RS + CS + Aggregation. The following is what each one of them mean,\n",
    "- DTs: The Decision Trees that make up the forest.\n",
    "- RS: Row sampling for training data selection.\n",
    "- CS: Column sampling for feature selection at each node.\n",
    "- Aggregation: Combining predictions from all trees (usually majority vote for classification, and averaging for regression).\n",
    "\n",
    "### Hyperparameters in Random Forest\n",
    "- `n_estimators`: This hyperparameter controls the number of trees to be created in the forest. More trees generally lead to better performance but also increase the training time.\n",
    "- `max_features`: This parameter specifies the number of features randomly chosen at each node for splitting. A lower value introduces more randomness and helps prevent overfitting.\n",
    "\n",
    "### What does \"Random\" in Random Forest refer to?\n",
    "- The \"Random\" in Random Forest refers to the random sampling of rows and columns during tree creation, leading to diverse trees.\n",
    "- \"Forest\" signifies the collection of multiple Decision Trees.\n",
    "\n",
    "### Example\n",
    "- With 10 features, a 20% random column sample (2 feature) and a 40% random row sample (400 rows from 1000) would be used to train 1 Tree.\n",
    "- This process is repeated for the specified number of estimators (e.g., 100 trees).\n",
    "\n",
    "### Aggregation and underfitting\n",
    "- Random Forests typically use majority voting for classification (the most frequent class predicted by the Trees wins). Averaging is used for regression.\n",
    "- Individual Trees in a Random Forest might be underfit due to limited data and random features at each split. However, the ensemble combines these weaker learners to create a more robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm\n",
    "Random Forest is a powerful ensemble learning technique widely used for classification and regression tasks. It leverages the strengths of multiple Decision Trees to create a robust and accurate model.\n",
    "\n",
    "### Core idea\n",
    "- Random Forests build a collection of Decision Trees, each trained on a random subset of the data. This creates diversity among the trees, preventing them from overfitting to the training data.\n",
    "- When making a prediction, the forest combines the predictions from all the individual trees using a voting mechanism for classification or averaging for regression. This reduces the variance of the model and improves its generalization ability.\n",
    "\n",
    "### Key components\n",
    "1. Decision Trees: These are the building blocks of the Forest. Each Tree learns a set of rules for classifying or predicting based on the features in the data.\n",
    "2. Random sampling:\n",
    "    - Row sampling (Bootstrap aggregation): During Tree creation, a random subset of data points (rows) are selected with replacement from the original dataset. This means a data point can be included in a tree multiple times.\n",
    "    - Column sampling (Feature subsetting): At each node in a Tree, a random subset of features (columns) are chosen from the full set. The Tree then splits the data based ob the best split among these chosen features. This injects randomness into the Tree structure.\n",
    "3. Aggregation: This refers to how th predictions from all the Trees are combined to make a final prediction.\n",
    "    - Classification: Majority voting is typically used. The class predicted by most Trees becomes the final prediction.\n",
    "    - Regression: Averaging the predicted values from all Trees in common.\n",
    "\n",
    "### Building the Random Forest\n",
    "1. Define the parameters: Specify the number of Trees (`n_estimators`) to create and the number of features (`max_features`) to randomly choose at each node for splitting.\n",
    "2. Create Trees:\n",
    "    - Draw a bootstrap sample of data points.\n",
    "    - Build a Decision Tree using the sampled data and chosen features (`max_features`) at each node.\n",
    "    - Repeat the above 2 steps for the specified number of Trees (`n_estimators`).\n",
    "3. Prediction:\n",
    "    - For a new data point, make predictions using each Tree in the forest.\n",
    "    - Combine the individual predictions using the chosen aggregation method (voting or averaging).\n",
    "\n",
    "### Advantages of Random Forests\n",
    "- Improved accuracy and generalizability: By combining multiple diverse Trees, Random Forests often achieves higher accuracy than a single Decision Tree and are less prone to overfitting.\n",
    "- Handles missing data well: Decision Trees can naturally handle missing data by splitting on existing features.\n",
    "- Robust to outliers: Ensemble methods like Random Forests are less sensitive to outliers in the data compared to single models.\n",
    "- Provides feature importance: Random Forests can be used to understand the relative importance of features in the model.\n",
    "\n",
    "### Disadvantages of Random Forests\n",
    "- Can be a black box: The inner workings of individual Trees and how they contribute to the final prediction can be difficult to interpret.\n",
    "- Computationally expensive: Training a Random Forest with many Trees can be computationally expensive compared to simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-Of-Bag (OOB) Score\n",
    "### Core idea\n",
    "- Random Forests use bootstrap aggregation (bagging) to train Decision Trees. Each Tree is trained on a random subset (m rows) of the original data (n rows).\n",
    "- The remaining data points (n - m rows) that are not used to train a paticular Tree are called Out-Of-Bag (OOB) samples for that Tree.\n",
    "\n",
    "### OOB for evaluation\n",
    "Individual Tree evaluation: OOB samples can be used to evaluate the performance of each Tree in the Forest.\n",
    "- The trained Tree predicts the class/ value for each OOB sample.\n",
    "- These predictions are compared with the actual values to calculate the error (e.g., difference for classification, squared difference for regression).\n",
    "\n",
    "### Overall OOB score\n",
    "- Average error: The OOB score is the average of the errors calculated for all OOB samples across all Trees in the Forest.\n",
    "- Cross-validation alternative: This provides an estimate of the model's generalization performance without needing a separate cross-validation process.\n",
    "\n",
    "### Benefits of OOB score\n",
    "- Efficiency: Since OOB samples are readily available during forest training, calculating the OOB score is computationally efficient.\n",
    "- Reduced bias: OOB evaluation avoids bias that can occur when using a separate validation set, as each data point is left out for evaluation at least once.\n",
    "\n",
    "### Example\n",
    "- Consider a dataset with 7 data points (A-F) and 3 Trees ($m_1$, $m_2$, $m_3$).\n",
    "- The OOB data for each model might be,\n",
    "    - $m_1$: B, C\n",
    "    - $m_2$: A, E, F\n",
    "    - $m_3$: C, D\n",
    "- For data point C (present in OOB for $m_1$ and $m_3$),\n",
    "    - Predictions can be made using both the models ($m_1(C)$ and $m_3(C)$).\n",
    "    - Average these predictions ($P(C) = \\frac{m_1(C) + m_3(C)}{2}$).\n",
    "    - Compare this average prediction with the actual value of $C$ ($y_{actual}(C)$) to calculate the error ($error_C = y_{actual}(C) - P(C)$).\n",
    "\n",
    "### Overall OOB error\n",
    "We repeat the above for all data points and all Trees where there are OOB samples. The OOB error is the sum of these individual errors across all data points,\n",
    "    - $error_{OOB} = \\sum_{i = 1}^n(error_i)$.\n",
    "\n",
    "### OOB score v. actual score\n",
    "While OOB score provides a good estimate of performance, it might not always match an explicitly calculated cross-validation score due to random sampling variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation Of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"employee_attrition_dataset/train.csv\")\n",
    "df2 = pd.read_csv(\"employee_attrition_dataset/test.csv\")\n",
    "df = pd.concat([df1, df2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping \"Employee ID\" column\n",
    "df.drop(columns = [\"Employee ID\"], inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding the binary columns\n",
    "binary_columns = [i for i in df.columns if df[i].dtype not in [int, float] and df[i].nunique() == 2]\n",
    "binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encoder(dataframe, column):\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[column] = labelencoder.fit_transform(dataframe[column])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in binary_columns:\n",
    "    df = label_encoder(df, i)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding the categorical columns\n",
    "ohe_columns = [i for i in df.columns if 10 >= df[i].nunique() > 2]\n",
    "ohe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(dataframe, categorical_columns, drop_first = True):\n",
    "    dataframe = pd.get_dummies(dataframe, columns = categorical_columns, drop_first=drop_first)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = one_hot_encoder(df, ohe_columns, drop_first=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.drop(columns = [\"Attrition\"])\n",
    "y = df[\"Attrition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(criterion = \"gini\")\n",
    "dt_classifier.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy score\n",
    "dt_classifier.score(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy score\n",
    "dt_classifier.score(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fitting a RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(criterion = \"gini\")\n",
    "rf_classifier.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy score\n",
    "rf_classifier.score(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy score\n",
    "rf_classifier.score(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Reducing Training Time In Random Forest\n",
    "### Challenges and considerations\n",
    "- `n_estimators`: A higher number of trees (`n_estimators`) leads to better performance but also increases training time.\n",
    "\n",
    "### Approaches for reducing training time\n",
    "- Reduce the number of trees (`n_estimators`): This is the most straightforward approach, but it might affect performance. Experiment with different values to find a good balance.\n",
    "- Limit sampling (with caution):\n",
    "    - Row sampling: Reducing the number of rows sampled for each Tree can speed up training. However, it is important to excercise caution. Excessively reducing rows can lead to underfitting due to limited data for each tree.\n",
    "    - Column sampling: Similarly, reducing the number of features (columns) considered at each split can speed up training. But again, it is important to excercise caution. Reducing features too much can limit the model's ability to learn complex relationships and lead to underfitting.\n",
    "- Leverage parallel processing: Since Random Forest Trees are independent, they can be trained in parallel using techniques like multi-processing or libraries like `joblib`. This can significantly reduce training time if there is machine with multiple cores or CPUs.\n",
    "\n",
    "### Additional tips\n",
    "- Hardware optimization: Ensure sufficient RAM and processing power on the machine for faster training.\n",
    "- Data preprocessing: Preprocess the data efficiently (e.g., handle missing values, encode categorical features) to avoid bottlenecks during training.\n",
    "- Grid search (optional): Consider using grid search (`GridSearchCV`) to find the optimal combination of hyperparameters (e.g., `n_estimators`, `max_features`) that balance performance and training time for the specific problem.\n",
    "\n",
    "The key is to find the sweet spot training time and model performance. Experiment with different approaches and monitor the impact on accuracy and training duration. Remember, a slightly lower number of trees might be acceptable if training time is a significant concern, as long as the model still performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parallelization\n",
    "### Parallelizing for faster training\n",
    "- Independent Trees: The beauty of Random Forest lies in the independence of individual Decision Trees. Each Tree is trained on a different subset of data and does not rely on the results of the other trees.\n",
    "- Multi-processing: This characteristic makes Random Forests ideal for parallelization using techniques like multi-processing.\n",
    "- `n_jobs`: In `scikit-learn` or `sklearn`, the `n_jobs` attribute in the `RandomForestClassifier` (or `RandomForestRegressor`) controls the level of parallelization.\n",
    "    - `n_jobs = 1` utilizes all avaliable CPU cores.\n",
    "    - Specify a specific number, e.g., `n_jobs = 4`, to utilize that many cores.\n",
    "    - Setting `n_jobs = 1` (default) disables parallelization.\n",
    "\n",
    "### Benefits of parallelization\n",
    "- Reducing training time: By training multiple Trees simultaneously on different cores, the overall training process becomes significantly faster. This allows to,\n",
    "    1. Train models with a larger number of Trees (potentially improving performance) without excessive waiting.\n",
    "    2. Run multiple experiments or perform hyperparameter tuning more efficiently.\n",
    "- Improved efficiency: Parallelizatio leverages the processing power of the machine more effectively, leading to faster model development and iteration.\n",
    "\n",
    "### Note\n",
    "- No impact on accuracy: Parallelization itself doesn't directly improve the model's accuracy. It focuses on speeding up training, allowing to explore more training options that might lead to better models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance In Random Forest\n",
    "Feature importance is a crucial aspect of understanding how Random Forest models make predictions. It helps in assessing which features contribute most significantly to the model's decisions.\n",
    "\n",
    "### Why feature importance matters?\n",
    "- Model interpretability: Feature importance provides insights into the features that the model relies on for classification or regression. This helps you understand the underlying relationships between features and the target variable.\n",
    "- Feature selection: Identifying the most important features can guide in selecting a relevant subset of features for further analysis or model building. This can improve model performance and reduce training time.\n",
    "\n",
    "### Approaches to feature importance in Random Forests\n",
    "There are 2 primary methods to evaluate feature importance in Random Forests,\n",
    "- Mean Decrease in Impurity (MDI):\n",
    "    - This is the default method used by `sklearn` Random Forest implementations.\n",
    "    - It measures how much a feature contributes to reducing impurity (increasing homogeneity) within a node during Tree building.\n",
    "    - For classification tasks, impurity is often measured using Gini Index or Information Gain. For regression, it might be variance reduction.\n",
    "    - MDI calculates the average decrease in impurity across all Trees in the Forest for each feature split. Features with higher MDI scores are considered more important.\n",
    "- Permutation feature importance:\n",
    "    - This approach measures the importance of a feature by shuffling its values and observing the resulting decrease in model performance.\n",
    "    - The rationale is that shuffling a feature with high importance will significantly disrupt the model's ability to learn patterns, leading to a larger performance drop compared to shuffling a less important feature.\n",
    "    - Permutation importance can be calculated in 2 ways,\n",
    "        - Out-Of-Bag (OOB) permutation importance: This method uses the OOB data points of each Tree for evaluation. For a data point, the feature values are shuffled, and the model's prediction on that data point is compared to the original prediction. The average difference across all OOB data points is the permutation importance for that feature.\n",
    "        - Full permutation importance: This method shuffles features for all data points and refits the entire Forest to evaluate the impact on the overall performance metric (e.g., accuracy, R-Squared).\n",
    "\n",
    "### Choosing the right approach\n",
    "- MDI: This is a computationally efficient method readily available in `sklearn`. It's good starting point for understanding feature importance. However, MDI might not always accurately reflect the true importance of features, especially when features are highly correlated.\n",
    "- Permutation feature importance: This method provides a more robust assessment of feature importance but can be computationally expensive, especially for large datasets.\n",
    "\n",
    "### Additional considerations\n",
    "- Feature importance scores are relative within a model. A higher score for one feature indicates its greater importance compared to other features in that particular model.\n",
    "- Feature importance can vary depending on the specific dataset and problem. It's essential to interpret the results in the context of the domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(criterion = \"gini\")\n",
    "rf_classifier.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy score\n",
    "rf_classifier.score(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy score\n",
    "rf_classifier.score(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "names = [x_train.columns[i] for i in indices]\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(x_train_scaled.shape[1]), feature_importances[indices])\n",
    "plt.xticks(range(x_train_scaled.shape[1]), names, rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters In Random Forest\n",
    "These are parameters that control the learning process of the model but are not directly learned from the data. They need to be set before training time Forest.\n",
    "\n",
    "### Common hyperparameters\n",
    "- `n_estimators`: The number of trees in the Forest. More Trees generally improve performance but increase training time.\n",
    "-  `max_samples`: Row samples ratio (0 to 1). Defines the proportion of rows (data points) randomly chosen for training each Tree.\n",
    "- `max_features`: Column sampling (values: `\"log2\"`, `\"sqrt\"`, `None`), Defines the number of features randomly chosen at each split in a Tree.\n",
    "\n",
    "### Regularization in Decision Trees (CCP Alpha)\n",
    "- Linear models use L1 or L2 regularization to penalize model complexity. Decision Trees do not have weights like linear models.\n",
    "- CCP Alpha (Cost Complexity Pruning): This hyperparameter penalizes the model based on the number of terminal nodes (leaves). Fewer nodes generally lead to simpler, less overfitting models.\n",
    "    - Higher `ccp_alpha` values lead to more pruning (fewer nodes), potentially causing underfitting.\n",
    "    - Lower values allow for more complex Trees (more nodes), potentially leading to overfitting.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "Finding the optimal combination of hyperparameters is crucial for good model performance. Manually trying different values can be time consuming and inefficient, especially with multiple parameters.\n",
    "\n",
    "### `sklearn` techniques\n",
    "- `GridSearchCV` This approach creates a grid of all possible combinations of hyperparameter values specified by the user. It trains a model for each combination using cross-validation to evaluate performance. Finally, it selects the combination with the best cross-validation score.\n",
    "    - Advantage: Provides an exhaustive search, guaranteeing to find the best combination within the defined grid.\n",
    "    - Disadvantage: Can become computationally expensive with many hyperparameters and values.\n",
    "- `RandomSearchCV`: This approach randomly samples a subset of combinations from the defined grid and trains models for those. It uses cross-validation to evaluate performance and selects the best combination among those explored.\n",
    "    - Advantage: More efficient that `GridSearchCV`, especially with many parameters.\n",
    "    - Disadvantage: May miss the absolute best combination, but often provides a good balance between performance and efficiency.\n",
    "\n",
    "### Additional considerations\n",
    "- CV parameter: This parameter in `GridSearchCV` and `RandomSearchCV` controls the number of cross-validation folds used to evaluate models. More folds generally provide a more robust estimate of performance but increase training time.\n",
    "- Advanced methods: Bayesian optimization is more sophisticated technique that iteratively explores the hyperparameter space based on past performance, potentially converging to an optimal solution faster. However, it's typically used for complex problems with many hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "params = {\n",
    "    \"n_estimators\": [100, 200, 300, 400],\n",
    "    \"max_depth\": [3, 5, 10, 20],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "    \"bootstrap\": [True, False],\n",
    "    \"max_features\": [8, 9, 10],\n",
    "    \"ccp_alpha\": [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "grid_search_rf_classifier = GridSearchCV(\n",
    "    estimator = rf_classifier,\n",
    "    param_grid = params,\n",
    "    scoring = \"accuracy\",\n",
    "    cv = 3,\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "grid_search_rf_classifier.fit(x_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf_classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf_classifier.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-Off In Random Forest\n",
    "Decision Trees tend to have low bias and high variance.\n",
    "- Low bias: Decision Trees flexible enough to capture complex relationships in data, leading to low bias.\n",
    "- High variance: However, they are also prone to overfitting, especially when grown deeply. Small changes in the training data can lead to significantly different trees (hihg variance).\n",
    "\n",
    "By combining multiple Decision Trees, Random Forests achieve a better balance beween bias and variance.\n",
    "\n",
    "### How do Random Forests reduce variance?\n",
    "- Ensemble learning: Combining predictions from multiple trees (ensemble) reduces the variance of the overall model. Individual Tree overfitting is \"averaged out\" by the ensemble.\n",
    "- Bootstrap aggregation (Bagging): Random Forests use Bagging. Each Tree is trained on a different subset of data (bootstrap sample), leading to diversity among the Trees.\n",
    "- Row sampling and column sampling: Trees do not rely on the same data points or features for splitting, preventing them from becoming too similar and overfitting the same patterns.\n",
    "\n",
    "### Bias-variance and errors\n",
    "- High bias: High bias models underfit the training data. This leads to high errors on both the training and testing sets, as the model fails to capture the underlying relationships.\n",
    "- High variance: High variance models overfit the training data. They perform well on the training set with low errors but perform poorly on unseen data (testing set) with high errors. There's a large gap between training and testing performance.\n",
    "\n",
    "### Random Forests and errors\n",
    "Random Forests aim for a sweet spot with low bias (due to their flexibility) and reduced variance (due to ensemble averaging and diversity). This helps achieve lower errors on both training and testing sets, leading to better generalization.\n",
    "\n",
    "### Non-reducible errors\n",
    "There will always be some inherent error in any model due to factors like noise in the data or limitations of the model itself. There are non-reducible errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
