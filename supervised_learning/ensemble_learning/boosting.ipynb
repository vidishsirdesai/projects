{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Boosting is a powerful ensemble Machine Learning technique used in both classification and regression tasks. It combines the predictions from multiple weak learners (oftentimes Decision Trees) to create a strong learner with improved performance.\n",
    "\n",
    "### Core idea\n",
    "- Boosting iteratively trains weak learners, where each learner focuses on correcting the errors of the previous ones.\n",
    "- Imagine a group of average students (weak learners) working together to solve a problem. Each student learns from the mistakes of the others, ultimately leading to a better understanding of the problem.\n",
    "\n",
    "### Boosting algorithm\n",
    "1. Initialize weights: Each data point in the training set is assigned and equal weight.\n",
    "2. Train weak learner: A weak learner (e.g., Decision Tree) is trained on the weighted data.\n",
    "3. Calculate error: The error of the weak learner is calculated based on the assigned weights. Misclassified points receive higher weights, focusing the next learner on those challenging examples.\n",
    "4. Adjust weights: Weights of the data points are adjusted based on the errors. More weight is given to points that the previous learners got wrong.\n",
    "5. Repeat: Steps 2 to 4 are repeated for multiple iterations, with each new learner focusing on the most difficult cases from the previous learner.\n",
    "6. Final prediction: The final prediction is made by combining the predictions from all the weak learners in the ensemble, often using a weighted voting (for classification) or averaging (for regression) approach.\n",
    "\n",
    "### Benefits of Boosting\n",
    "- Improved accuracy: By combining weaker models, boosting can achieve higher accuracy compared to individual learners.\n",
    "- Can handle complex problems: Boosting can learn complex relationships in the data that might be challenging for a single model.\n",
    "- Handles imbalanced data: Some boosting algorithms can effectively handle imbalanced datasets where certain classes have fewer data points.\n",
    "\n",
    "### Common Boosting algorithms\n",
    "- AdaBoost (Adaptive Boosting): A popular boosting algorithm that focuses on improving the weights of misclassified examples.\n",
    "- Gradient Boosting: A more general framework where the focus is on minimizing a loss function (e.g., squared error for regression) in each iteration. Common examples include,\n",
    "    - XGBoost: A powerful and scalable gradient boosting algorithm known for its performance.\n",
    "    - LightGBM: Another efficient gradient boosting algorithm with good performance and speed.\n",
    "\n",
    "### Considerations\n",
    "- Overfitting: Boosting algorithms can be prone to overfitting if not carefully tuned. Techniques like regularization can be used to mitigate this risk.\n",
    "- Computational cost: Training a boosted ensemble can be computationally expensive compared to a single model, especially with many iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging V. Boosting\n",
    "### Boosting for high bias and low variance models\n",
    "- Boositng is often used when the base learners (e.g., Decision Trees) tend to have high bias (underfitting) and low variance (low model complexity).\n",
    "- Bagging would not be ideal in this scenario because it focuses on averaging predictions from diverse models (high variance). Averaging underfitting models won't significantly improve performance.\n",
    "\n",
    "### Additive combining in Boosting\n",
    "Boosting addresses the high bias by sequentially training models (like Decision Trees) in an \"additive\" fashion. Each subsequent model \"boosts\" the overall performance by focusing on the erros of the previous model. Consider the following examples,\n",
    "- Imagine a dataset with target values (predicted v. actual values).\n",
    "- The first model (weak learner) might underfit the data, leading to sigificant errors (differences between predicted and actual values).\n",
    "- The second model is trained specifically on these errors, trying to learn from the mistakes of the first model. It essentially adds its corrective predictions to the first model's predictions.\n",
    "- This process continues iteratively, with each subsequent model focusing on the remaining errors from the previous ensemble.\n",
    "\n",
    "### Comparison with Bagging\n",
    "- Boosting is a sequential process. Each model builds upon the previous one.\n",
    "- Bagging, on the other hand, trains models in parallel on different data subsets (bootstrap samples).\n",
    "\n",
    "### Addressing bias in Boosting\n",
    "- Boosting achieves bias reduction by focusing on the errors of previous models. Each iteration aims to learn from the shortcomings of the ensemble so far, gradually reducing the overall bias.\n",
    "- Additionally, Boosting algorithms often adjust the weights of data points during training. Points that were misclassified by previous models receive higher weights, forcing the next model to pay more attention to those challenging examples.\n",
    "\n",
    "### Boosting algorithms and considerations\n",
    "- Common Boosting algorithms like AdaBoost and Gradient Boosting implement these principles in different ways.\n",
    "- While Boosting offers advantages, it can be computationally expensive due to the sequential nature of training. However, the performance gains can often outweigh the increased training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive Combining In Boosting\n",
    "### Step-by-step breakdown\n",
    "1. Simple model and residuals:\n",
    "    - The average or the mean model is the simple starting point ($M_0$).\n",
    "    - It predicts the average target value ($y_{cap}$) abd the residuals ($error_{i0}$) are calculated for each data point ($y_i$) as the difference between the actual value and the average prediction.\n",
    "2. Model on residuals:\n",
    "    - A second model ($M_1$) is trained on the residuals ($error_{i0}$). $M_1$ aims to fit these errors.\n",
    "    - In Boosting terminology, M1 is called a weak larner. It typically has high bias (underfitting) and low variance (low complexity), focusing on specific patterns in the errors.\n",
    "3. Additive prediction: The final prediction for each data point is achieved by adding the predictiona from $M_0$ (the average model) and $M_1$ (error model).\n",
    "    - $h_0(x_i) + h_1(x_i)$.\n",
    "    - This is where the additive aspect comes in. The corrections from each model are progressively added.\n",
    "4. Iterative process (optional):\n",
    "    - Boosting algorithms can continue this process by training additional models ($M_2$, $M_3$, etc) on the remaining errors from the previous ensemble.\n",
    "    - Each subsequent model focuses on the errors the ensemble has not yet captured effectively.\n",
    "\n",
    "### Example\n",
    "Say that out of 100 data points, 80 data points have been correctly predicted by $M_0$. $M_1$ tries to learn from the remaining 20 data points. Now say that $M_1$ has been able to predict 16 out of 20 data points correctly. The sum of $M_0$'s prediction and $M_1$'s corrections (16 out of 20) provides a potentially better overall prediction.\n",
    "\n",
    "### Classification v. regression\n",
    "- In regression, residuals represent the difference between the actual target value and the predicted value.\n",
    "- In classification, boosting algorithms might use probability differences instead of residuals for error calculations.\n",
    "\n",
    "### Addressing bias\n",
    "By iteratively focusing on the errors of previous models, boosting gradually reduces the overall bias. Each model in the ensemble adds it corrective power to improve the final prediction.\n",
    "\n",
    "### High bias v. high variance\n",
    "- High bias: High training error and high testing error (model underfits the data).\n",
    "- High variance: Low training error but high testing error (model overfits the training data).\n",
    "- Boosting is particualarly effective for models with high bias because it sequentially refines the predictions to reduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps In Boosting\n",
    "1. Mean model and error:\n",
    "    - Say that the mean weight of 67 has been calculated as the initial prediction ($M_0$ or $h_0(x)$) for all data points.\n",
    "    - The residuals ($error_0$) are computed as the difference between the actual weight and the mean weight for each data point.\n",
    "2. Model on residuals:\n",
    "    - The next model ($M_1$) uses the residuals ($error_0$) as its target variable. This is a crucial aspect of boosting.\n",
    "    - Building a simple Decision Tree (depth 1) based on gender to predict these residuals is a valid approach. $M_1$ acts as a weak learner focusing on patterns in the errors.\n",
    "3. Additive predictions and residuals\n",
    "    - $M_1$'s predictions for the residuals are added to the initial predictions ($M_0$) to create new predictions. This is the \"additive combining\" concept.\n",
    "    - Calculating new residuals based on these updated predictions is also correct.\n",
    "4. Overfitting and stopping (optional): Repeateadly adding models can lead to overfitting. Boosting algorithms typically use techniques like cross-validation or a stopping criterion (e.g., maximum number of models) to prevent this.\n",
    "5. Gamma ($\\gamma$) for model weights:\n",
    "    - Gamma ($\\gamma$), controls the influence of each model in the ensemble.\n",
    "    - The final prediction is therefore, $f_m(x) = (h_0(x) + \\gamma_1) * (h_1(x) + \\gamma_1) * (h_2(x) + \\gamma_2) * ... * (h_n(x) + \\gamma_n)$.\n",
    "    - This weighting factor is crucial. Boosting algorithms often determine these weights based on the performance of each model on the validation set. Models with lower errors get higher weights in the final prediction.\n",
    "\n",
    "### Differences from Linear Regression\n",
    "- In Linear Regression, all weights are determined in one step using the entire dataset.\n",
    "- In Boosting, weights (gamma) are assigned to each model dequentially based on their performance in correcting errors of previous models.\n",
    "\n",
    "### Challenges of Boosting\n",
    "- Boosting models are generally not parallelized due to the sequential nature of training each model based on the previous ones. This can be slower compared to some parallel Machine Learning algorithms (Bagging).\n",
    "- Improving underfitting models can be more challenging than dealing with overfitting. Boosting works best when the base learners have some level of bias (underfitting) that can be progressively reduced through additive corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting From Regression Perspective\n",
    "### Loss function and predictions\n",
    "Mean Squared Error (MSE) is used as the loss function for regression problems. It measures the squared difference between actual values ($y_i$) and predicted values ($y_{cap}$).\n",
    "\n",
    "### Gradient and pseudo-residuals\n",
    "- The key concept is the connection between gradients and psuedo-residuals.\n",
    "- Taking the partial derivative of the loss function (MSE) with respect to the predicted value ($y_{cap}$) gives the negative gradient.\n",
    "- This negative gradient is proportional to the actual residual ($y_i - y_{cap}$). It indicates the direction and maginitude for improvement in terms of reducing the loss.\n",
    "- However, calculating the full residual repeatedly can be computationally expensive.\n",
    "\n",
    "### Pseudo-residuals for efficiency\n",
    "- A pseudo-residual is the negative gradient of the loss function with respect to the predicted value ($y_{cap}$). It directly relates to the direction for loss reduction.\n",
    "- By using pseudo-residuals instead of full residuals in subsequent models, gradient boosting achieves computational efficiency.\n",
    "\n",
    "### Optimizing for pseudo-residuals\n",
    "- Optimizing for pseudo-residuals translates to optimizing for the overall loss function.\n",
    "- Since the pseudo-residual is proportional to the negative gradient, focusing on minimizing the pseudo-residuals drives the model in the direction of reducing the overall MSE.\n",
    "\n",
    "### Summary of Gradient Boosting algorithm\n",
    "1. Initialize: Start with an initial prediction (often the mean of the target variable). Assign equal weights to all data points.\n",
    "2. Iteratively train models: In each iteration,\n",
    "    - Fit a weak learner (e.g., Decision Tree) on the current data using the pseudo-residuals as the target variable.\n",
    "    - Calculate the model's weight based on its performance in reducing the loss (e.g., MSE).\n",
    "3. Update predictions: Update the overall prediction for each data point by adding the weighted prediction from the new learner to the previous ensemble prediction.\n",
    "\n",
    "### Overall benefit\n",
    "By iteratively fitting models on pseudo-residuals and adding their predictions, gradient boosting progressively reduces the overall loss function, leading to improved performance compared to a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "### Loss functions and gradients\n",
    "The common loss functions for regression (MSE/ RMSE) and classification (Log-Loss).\n",
    "\n",
    "### Pseudo-residuals for efficiency\n",
    "- Calculating full residuals repeateadly can be computationally expensive.\n",
    "- Pseudo-residuals, derived from the negative gradient of the loss function with respect to the prediction at the previous stage ($k - 1$), offer an efficient alternative.\n",
    "\n",
    "### Gradient boosting and pseudo-residuals\n",
    "- GBDT leverages pseudo-residuals to guide the training process.\n",
    "- At each stage ($k$), the gradient of the loss function is calculated with respect to the output at the previous stage ($k - 1$). This gradient serves as the pseudo-residual for the current stage.\n",
    "- Focusing on minimizing these pseudo-residuals during model fitting ensures the overall loss function (e.g., MSE or Log-Loss) is reduced iteratively.\n",
    "\n",
    "### Why Gradient Boosting?\n",
    "- By iteratively fitting models on pseudo-residuals and adding their predictions, GBDT progressively improves the model's performance compared to a single model.\n",
    "- This approach helps address the bias of weak learners (e.g., Decision Trees) by sequentially correcting their errors.\n",
    "\n",
    "### Additional notes\n",
    "- Different GBDT algorithms might use different techniques to calculate the optimal step size (learning rate) when applying the pseudo-residuals to update the predictions.\n",
    "- While GBDT is commonly used with Decision Trees as weak learners, the concept of pseudo-residuals can be applied with other weak learning algorithms as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Decision Tree (GBDT) Algorithm\n",
    "1. Data preparation: Prepare the training data by handling missing values, scaling features if necessary, and splitting it into training and validation sets (optional).\n",
    "2. Initialization:\n",
    "    - Initialize the model prediction for each data point (often the mean of the target variable in regression or a simple prediction rule in classification).\n",
    "    - Assign equal weights to all data points in the training set.\n",
    "3. For each iteration:\n",
    "    - Calculate pseudo-residuals:\n",
    "        - Based on the current ensemble prediction (at stage ($t - 1$)), calculate the loss function's gradient (e.g., negative gradient of MSE for regression or negative gradient of Log-Loss for classification) with respect to the prediction at the previous stage ($t - 1$).\n",
    "        - These gradients are the pseudo-residuals for the current iteration.\n",
    "    - Fit a weak learner:\n",
    "        - Train a weak learner (e.g., a shallow Decision Tree) on the training data using the calculated pseudo-residuals as the target variable.\n",
    "        - The goal of a weak learner is to improve upon the current ensemble prediction by focusing on the areas with high pseudo-residuals (large errors from the previous stage).\n",
    "    - Model weighting:\n",
    "        - Based on the weak learner's performance in reducing the loss function on the training set (e.g., using a learning rate), determine a weight for this learner.\n",
    "        - This weight reflects how much influence the learner's predictions will have in the final ensemble.\n",
    "4. Update ensemble prediction:\n",
    "    - For each data point, add the weighted prediction from the newly trained weak learner to the current ensemble prediction (additive combining).\n",
    "    - This creates an updated ensemble prediction that incorporates the improvements from the new weak learner.\n",
    "5. Stopping: Continue iteratively training models until a stopping criteria is met. Common criteria include,\n",
    "    - Reaching a predefined number of iterations.\n",
    "    - The validation error (error on a held-out set) starts to increase, indicating overfitting.\n",
    "6. Prediction for new data: To make a prediction for a new data point, apply the final ensemble model. This typically involves,\n",
    "    - Calculating the predictions from each weak learner in the ensemble.\n",
    "    - Adding the weighted predictions from all weak learner (similar to the update step during training).\n",
    "\n",
    "### Key points\n",
    "- GBDT iteratively improves the model by focusing on the errors (pseudo-residuals) from the previous models.\n",
    "- Weak learners (e.g., Decision Trees) are used as building blocks for the ensemble.\n",
    "- Pseudo-residuals, derived from the loss function gradient, guide the training process efficiently.\n",
    "- Model weights control the influence of each weak learner in the final prediction.\n",
    "- GBDT excels at addressing bias in weak learners by sequentially correcting their errors.\n",
    "\n",
    "### Additional considerations\n",
    "- GBDT models can be computationally expensive due to the sequential training process.\n",
    "- Tuning hyperparameters like the number of iterations, learning rate, and weak learner complexity is crucial for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm For Pseudo-Residuals In GBDT\n",
    "### Initialization\n",
    "- The argmin is used to find the initial prediction that minimizes the loss function ($L$) for all data points ($n$) with respect to a parameter ($\\mu$). Howeve, this parameter typically represents the initial prediction iteself (often the mean for regression).\n",
    "- So the minimization is to find the best constant value for the initial prediction ($F_0$) that minimizes the overall loss.\n",
    "\n",
    "### Core steps\n",
    "1. Pseudo-residual calculation:\n",
    "    - The derivative of the loss function is not calculated directly with respect to the entire model prediction ($f(x_i)$).\n",
    "    - In GBDT, the pseudo-residual for iteration m is calculated as the negative gradient of the loss function with respect to the prediction at the previous stage ($f_{(m - 1)}(x_i)$) for each data point $i$.\n",
    "    - Thiss reflects the direction for improvement in reducing the loss based on the previous ensemble prediction.\n",
    "2. Weak learner training:\n",
    "    - A weak learner (e.g., a shallow Decision Tree) is trained on these pseudo-learners.\n",
    "    - The goal of this weak learner is to learn patterns in the errors (residuals) from the previous ensemble prediction.\n",
    "3. Model weighting: While some algorithms might use a line search or optimization techniques, the core idea is to assign a weight to this weak learner based on its performance in reducing the loss on the training data. This weight reflects the learner's importance in the final ensemble.\n",
    "4. Ensemble update:\n",
    "    - The update of the ensemble prediction ($F_m(x)$) is done by adding the weighted prediction from the newly trained weak learner ($h_m(x)$) to the previous ensemble prediction (F_{(m - 1)}(x)).\n",
    "    - This additive combining is a key aspect of GBDT.\n",
    "\n",
    "### Overall process\n",
    "The core concept is to iteratively improve the ensemble prediction. In each iteration,\n",
    "- Calculate pseudo-residuals based on the previous ensemble prediction's errors.\n",
    "- Train a weak learner on these pseudo-residuals.\n",
    "- Assign a weight to the weak learner based on its performance.\n",
    "- Update the ensemble prediction by adding the weighted prediction from the weak learner.\n",
    "\n",
    "### Number of iterations:\n",
    "- The process is repeated for a predefined number of iterations (m).\n",
    "- The stopping criteria typically involves reaching a maximum number of iterations or observing signs of overfitting on a validation set.\n",
    "\n",
    "### Optimization:\n",
    "GBDT optimizes for 2 things,\n",
    "- The weak learners ($h_m(x)$) themselves during training to fit the pseudo-residuals.\n",
    "- The weights ($\\gamma_m$) assigned to each weak learner to determine their influence in the final ensemble prediction. These weights are often calculated based on the learner's performance in reducing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-Off In GBDT\n",
    "### Bias-variance in GBDT\n",
    "- GBDT leverages weak learners (often shallow Decision Trees) with high bias (underfitting) and low variance (low model complexity).\n",
    "- The goal is to iteratively reduce the bias of the ensemble model by combining the predictions from multile weak learners that focus on the errors (residuals) from previous stages.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "There are 2 crucial hyperparameters in GBDT that affect the bias-variance trade-off,\n",
    "1. Number of boosting stages ($m$): This controls the number of weak learners used in the ensemble.\n",
    "    - Increasing $m$ allows the model to capture more complex patterns, potentially reducing bias.\n",
    "    - However, a very high m can lead to overfitting as the model becomes sensitive to training data noise (high variance).\n",
    "2. Base learner complexity (depth): This determines the complexity of individual Decision Trees (weak learners).\n",
    "    - Shallower trees have lower variance but may not capture complex relationships (higher bias).\n",
    "    - Deeper Trees have the potential to reduce bias but can also lead to overfitting if they become too specific to the training data (high variance).\n",
    "\n",
    "### Finding the right balance:\n",
    "- The challenge lies in finding the optimal combination of $m$ and the depth that achieves a good balance between bias and variance.\n",
    "- Lower bias and lower variance generally lead to better generalization performance (performance on unseen data).\n",
    "\n",
    "### Additional considerations\n",
    "- Other hyperparameters like learning rate can also influence the bias-variance trade-off.\n",
    "- Techniques like cross-validation can be used to evaluate different hyperparameter configurations and identify the one that results in the best performance o a held out validation set.\n",
    "\n",
    "### Strategies to tune hyperparameters in GBDT\n",
    "- Grid search: evaluate a predefined grid of hyperparameter values and choose the combination with the best validation performance.\n",
    "- Random search: Sample hyperparameter values randomly from a defined range and select the best performing combination.\n",
    "- Early stopping: Stop training the model if the validation error starts to increase, preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect Of Outliers In GBDT\n",
    "### Potential benefits\n",
    "- Compared to some simpler models (e.g., Linear Regression), GBDT exhibits some level of robustness to outliers.\n",
    "- During each iteration, GBDT focuses on the pseudo-residuals, which are the gradients of the loss function with respect to the predictions from the previous stage.\n",
    "- Outliers with significant deviations from the overall trend will have larger pseudo-residuals.\n",
    "- The weak learner in that iteration can potentially capture this pattern and adjust the model's predictions to account for those outliers to some extent.\n",
    "\n",
    "### Potential drawbacks\n",
    "- GBDT's focus on pseudo-residuals can also be a downside in the presence of outliers.\n",
    "- If the weak learner in an iteration overfits to a few extreme outliers, it can introduce unnecessary complexity into the model.\n",
    "- This can lead to a decrease in the model's generalizability (performance on unseen data) as it focuses too much on fitting the outliers in the training data.\n",
    "\n",
    "### Overall effect\n",
    "The net effect of outliers on GBDT depends on several factors,\n",
    "- Number of outliers: A few outliers might be handled by the model, but a large number of outliers can significantly impact performance.\n",
    "- Distribution of outliers: Outliers far from the main cluster are more likely to cause problems.\n",
    "- Model complexity (hyperparameters):\n",
    "    - A model with a high number of boosting stages ($m$) or deep Decision Trees might be more susceptable to overfitting to outliers.\n",
    "    - Conversely, a simpler model with fewer stages or shallow Trees might not capture the underlying patterns even in the presence of some outliers (higher bias).\n",
    "\n",
    "### Mitigation strategies\n",
    "- Outlier detection and handling: Consider identifying and handling outliers before training the GBDT model (e.g., capping extreme values, removing outliers if justifiable).\n",
    "- Hyperparameter tuning: Carefully tune hyperparameters like $m$ and depth to find a balance between reducing bias and avoiding overfitting to outliers. Techniques like cross-validation can be helpful for this purpose.\n",
    "- Alternative loss functions: Explore loss functions less sesitive to outliers, such as Huber Loss or Trimmed Mean Squared Error, especially for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages And Disadvantages Of GBDT\n",
    "### Advantages\n",
    "- High accuracy and flexibility: GBDT can achieve excellent accuracy on various regression and classification tasks due to its ability to learn complex patterns by combining mukltiple weak learners.\n",
    "- Handles diverse data types: GBDT can work effectively with different data types, including numerical and categorical features.\n",
    "- Robust to outliers: GBDT is relatively insensitive to outliers in the data compared to some simpler models.\n",
    "- Interpretability: While not as interpretable as linear models, GBDT models can be partially interpreted by analyzing the features used in the Decision Trees at each stage. Feature importance scores can be helpful for understanding which features contribute most to the model's predictions.\n",
    "- Handles missing data: GBDT can handle missing data inherently by splitting data points based on existing features in the Decision Trees.\n",
    "- Reduces bias: By iteratively focusing on errors from previous models, GBDT progressively reduces the overall bias of the ensemble.\n",
    "\n",
    "### Disadvantages\n",
    "- Computationally expensive: Training GBDT models can be computationally expensive due to the sequential training of multiple weak learners.\n",
    "- Prone to overfitting: If not tuned properly (especially with high $m$ or depth), GBDT models can overfit the training data and perform poorly on unseen data. Careful hyperparameter tuning and techniques like early stopping are crucial to mitigate this rist.\n",
    "- Black box nature: While partially interpretable, GBDT models can be complex, making it challenging to fully understand the internal logic behind their predictions.\n",
    "- Prone to high variance: With a large number of weak learners or high-depth Trees, GBDT models can become sensitive to training data noise, leading to high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation Of GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading employee_attrition_dataset\n",
    "import pickle\n",
    "\n",
    "with open(\"employee_attrition_dataset/x_sm.pkl\", \"rb\") as file:\n",
    "    x_train = pickle.load(file)\n",
    "\n",
    "with open(\"employee_attrition_dataset/y_sm.pkl\", \"rb\") as file:\n",
    "    y_train = pickle.load(file)\n",
    "\n",
    "with open(\"employee_attrition_dataset/x_test.pkl\", \"rb\") as file:\n",
    "    x_test = pickle.load(file)\n",
    "    \n",
    "with open(\"employee_attrition_dataset/y_test.pkl\", \"rb\") as file:\n",
    "    y_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(max_depth=2, n_estimators=150)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GradientBoostingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">?<span>Documentation for GradientBoostingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GradientBoostingClassifier(max_depth=2, n_estimators=150)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(max_depth=2, n_estimators=150)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbdt_classifier = GradientBoostingClassifier(n_estimators = 150, max_depth = 2, loss = \"log_loss\")\n",
    "gbdt_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9474260679079957"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt_classifier.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8641304347826086"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt_classifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMG Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "XGBoost stands for eXtreme Gradient Boosting, is an optimized implementation of the Gradient Boosting Decision Tree (GBDT) algorithm. It's known for its efficiency, accuracy, and scalability, making it popular choice for various Machine Learning tasks, especially regression and classification.\n",
    "\n",
    "### Core principles\n",
    "- XGBoost follows the core principles of GBDT, using multiple weak learners (often Decision Trees) to build a powerful ensemble model.\n",
    "- It iteratively trains models by focusing on the errors (pseudo-residuals) from previous stages to progressively improve the ensemble's performance.\n",
    "\n",
    "### Optimizations and improvements\n",
    "- RegualarizationL XGBoost incorporates various regularization techniques to prevent overfitting. These techniques penalize overly complex models, encouraging simpler Decision Trees in the ensemble.\n",
    "- Sparsity: XGBoost encourages sparse Decision Trees, meaning many of the splits in the Trees can involve only a few features. This promotes interpretability and reduces computational cost.\n",
    "- Parallelization: XGBoost is designed for efficient parallel and distributed computing, making it suitable for handling large datasets.\n",
    "- Improved objective functions: XGBoost offers built-in support for various objective functions (loss functions) tailored for different tasks (e.g., regression, classification, ranking).\n",
    "\n",
    "### Advantages of XGBoost\n",
    "- High performance: XGBoost often achieves state-of-the-art performance on various Machine Learning benchmarks.\n",
    "- Scalability: It can handle large datasets efficiently due to its optimized implementation and support for parallelization.\n",
    "- Flexibility: It suppports various objective functions and offers a wide range of hyperparameters to tune the model for specific tasks.\n",
    "- Regularization: Built-in regularization techniques help prevent overfitting and improve model generalizability.\n",
    "\n",
    "### Disadvantages of XGBoost\n",
    "- Complexity: Tuning XGBoost can be more complex compared to simpler algorithms due to its numerous hyperparameters.\n",
    "- Black box nature: While partially interpretable through feature importance scores, XGBoost models can be difficult to fully understand due to their ensemble nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters In XGBoost\n",
    "### General parameters\n",
    "- `n_estimators`: Controls the number of weak learners (Decision Trees) used in the ensemble. Increasing this value can reduce bias but also increase variance and risk of overfitting.\n",
    "- `learning_rate`: Determines the step size for updating the model with each iteration. A lower learning rate can help prevent overfitting but might require more training iterations.\n",
    "- `max_depth`: Limits the maximum depth of the individual Decision Trees. Deeper Trees can capture more complex patterns but are more prone to overfitting.\n",
    "\n",
    "### Regularization parameters:\n",
    "- `reg_lambda` (L2 regularization): Penalizes model complexity by adding a penalty term based on square of the model weights. Higher values encourage simpler models and reduce overfitting.\n",
    "- `reg_alpha` (L1 regularization): Penalizes model complexity by adding a penalty term based on the absolute value of the model weights. Can lead to sparse models with fewer features.\n",
    "- `gamma` (minimum loss reduction for a split): Controls the minimum improvement in the loss function required to make a split in the Decision Trees. Higher values can help prevent overfitting by avoiding unnecessary splits.\n",
    "\n",
    "### Tree-specific parameters\n",
    "- `min-child_weight` (minimum sum of weights of instances in a child node): Sets the minimum number of samples required in a leaf node. Higher values can prevent overfitting by avoiding overly specific splits.\n",
    "- `subsample` (sampling ratio): Randomly samples a proportion of training data for each boosting iteration. Can help improve model generalizability by reducing variance.\n",
    "- `colsample_bytree` (feature sampling ratio): Randomly samples a subset of features for each Tree in the ensemble. Promotes model diversity and reduces overfitting.\n",
    "\n",
    "### Task-specific parameters\n",
    "- `objective`: Specifies the objective function (loss function) to be optimized for the task. XGBoost provides options for regression (e.g., squared error), classification (e.g., Logistic Regression), and ranking tasks.\n",
    "\n",
    "### Choosing hyperparameters\n",
    "- There's no single best set of hyperparameters for all XGBoost models.\n",
    "- It's essential to experiment with different configurations and evaluate their performance on a validation set to find the optimal values for the sepcific data and task.\n",
    "\n",
    "### Techniques for hyperparameter tuning\n",
    "- Grid search: Evaluate a predefined grid of hyperparameter values and choose the combination with the best validation performance.\n",
    "- Random search: Sample hyperparameter values randomly from a defined range and select the best performing combination. This can be more efficient for exploring a large hyperparameter space.\n",
    "- Bayesian optimization: Uses a probablistic model to guide the search for optimal hyperparameters, focusing on configurations with higher predicted performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "LightGBM (Light Gradinet Boosting Machine) is an efficient implementation of the Gradient Boosting Decision Tree (GBDT) algorithm with several optimizations for speed and performance.\n",
    "\n",
    "### Gradient Boosting framework\n",
    "- LightGBM follows the core principle of GBDT, which involves building an ensemble of weak learners (typically shallow Decision Trees) in a sequential manner.\n",
    "- Each weak learner is trained to improve upon the predictions of the previous ensemble by focusong on the residual (errors) from the prior stage.\n",
    "- This iterative process progressively reduces the overall error of the ensemble, leading to a more accurate model.\n",
    "\n",
    "### Key optimizations in LightGBM\n",
    "- Histogram-based algorithm:\n",
    "    - LightGBM utilizes a histogram-based approach to find optimal split points in the Decision Trees.\n",
    "    - Instead of sorting the data for each feature at every split, it builds histograms to efficiently count data points falling into different value bins.\n",
    "    - This significantly reduces the computational cost compared to traditional GBDT which relies on feature sorting.\n",
    "- Gradient based one-sided sampling (GOSS):\n",
    "    - LightGBM employs a sampling technique called GOSS to focus training on data points with larger gradients (errors).\n",
    "    - This prioritizes instances that contribute more significantly to improve the model training time.\n",
    "- Exclusive feature bundling (EFB):\n",
    "    - LightGBM can group mutually exclusive features (features that cannot take on the same value for a given instance) into bundles.\n",
    "    - This reduces the number of features considered at each split, leading to faster Decision Tree construction and potentially reducing overfitting.\n",
    "- Efficient Tree learning algorithms: LightGBM implements efficient algorithms for Decision Tree learning, including level-wise algorithms and other optimizations for finding the best splits.\n",
    "- Parallelization and early stopping:\n",
    "    - LightGBM supports efficient parallelization acros multiple cores or machines to handle large datasets.\n",
    "    - It incorporates early stopping techniques to prevent overfitting by stopping training when the validation performance starts to deteriorate.\n",
    "\n",
    "### Overall algorithm steps\n",
    "1. Initialize: Start with an initial prediction model (often a simple constant value).\n",
    "2. Iteration loop:\n",
    "    - Calculate the residuals (errors) for each data point based on the current ensemble predictions.\n",
    "    - Use GOSS to sample a subset of data points with larger residuals.\n",
    "    - Build a new shallow Decision Tree using histogram-based algorithm and EFB (optional) to improve predictions on the residuals.\n",
    "    - Update the ensemble model by adding the newly trained Decision Tree.\n",
    "3. Repeat step 2: Continue iteratively building new trees until a stopping criterion (e.g., maximum number of iterations, early sropping) is met.\n",
    "\n",
    "LightGBM's optimizations make it a powerful and efficient alternative to traditional GBDT algorithms, particularly for large datasets and computationally intensive tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
