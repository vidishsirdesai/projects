{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Boosting is a powerful ensemble Machine Learning technique used in both classification and regression tasks. It combines the predictions from multiple weak learners (oftentimes Decision Trees) to create a strong learner with improved performance.\n",
    "\n",
    "### Core idea\n",
    "- Boosting iteratively trains weak learners, where each learner focuses on correcting the errors of the previous ones.\n",
    "- Imagine a group of average students (weak learners) working together to solve a problem. Each student learns from the mistakes of the others, ultimately leading to a better understanding of the problem.\n",
    "\n",
    "### Boosting algorithm\n",
    "1. Initialize weights: Each data point in the training set is assigned and equal weight.\n",
    "2. Train weak learner: A weak learner (e.g., Decision Tree) is trained on the weighted data.\n",
    "3. Calculate error: The error of the weak learner is calculated based on the assigned weights. Misclassified points receive higher weights, focusing the next learner on those challenging examples.\n",
    "4. Adjust weights: Weights of the data points are adjusted based on the errors. More weight is given to points that the previous learners got wrong.\n",
    "5. Repeat: Steps 2 to 4 are repeated for multiple iterations, with each new learner focusing on the most difficult cases from the previous learner.\n",
    "6. Final prediction: The final prediction is made by combining the predictions from all the weak learners in the ensemble, often using a weighted voting (for classification) or averaging (for regression) approach.\n",
    "\n",
    "### Benefits of Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
