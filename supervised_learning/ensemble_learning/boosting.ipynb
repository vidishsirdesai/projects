{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Boosting is a powerful ensemble Machine Learning technique used in both classification and regression tasks. It combines the predictions from multiple weak learners (oftentimes Decision Trees) to create a strong learner with improved performance.\n",
    "\n",
    "### Core idea\n",
    "- Boosting iteratively trains weak learners, where each learner focuses on correcting the errors of the previous ones.\n",
    "- Imagine a group of average students (weak learners) working together to solve a problem. Each student learns from the mistakes of the others, ultimately leading to a better understanding of the problem.\n",
    "\n",
    "### Boosting algorithm\n",
    "1. Initialize weights: Each data point in the training set is assigned and equal weight.\n",
    "2. Train weak learner: A weak learner (e.g., Decision Tree) is trained on the weighted data.\n",
    "3. Calculate error: The error of the weak learner is calculated based on the assigned weights. Misclassified points receive higher weights, focusing the next learner on those challenging examples.\n",
    "4. Adjust weights: Weights of the data points are adjusted based on the errors. More weight is given to points that the previous learners got wrong.\n",
    "5. Repeat: Steps 2 to 4 are repeated for multiple iterations, with each new learner focusing on the most difficult cases from the previous learner.\n",
    "6. Final prediction: The final prediction is made by combining the predictions from all the weak learners in the ensemble, often using a weighted voting (for classification) or averaging (for regression) approach.\n",
    "\n",
    "### Benefits of Boosting\n",
    "- Improved accuracy: By combining weaker models, boosting can achieve higher accuracy compared to individual learners.\n",
    "- Can handle complex problems: Boosting can learn complex relationships in the data that might be challenging for a single model.\n",
    "- Handles imbalanced data: Some boosting algorithms can effectively handle imbalanced datasets where certain classes have fewer data points.\n",
    "\n",
    "### Common Boosting algorithms\n",
    "- AdaBoost (Adaptive Boosting): A popular boosting algorithm that focuses on improving the weights of misclassified examples.\n",
    "- Gradient Boosting: A more general framework where the focus is on minimizing a loss function (e.g., squared error for regression) in each iteration. Common examples include,\n",
    "    - XGBoost: A powerful and scalable gradient boosting algorithm known for its performance.\n",
    "    - LightGBM: Another efficient gradient boosting algorithm with good performance and speed.\n",
    "\n",
    "### Considerations\n",
    "- Overfitting: Boosting algorithms can be prone to overfitting if not carefully tuned. Techniques like regularization can be used to mitigate this risk.\n",
    "- Computational cost: Training a boosted ensemble can be computationally expensive compared to a single model, especially with many iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging V. Boosting\n",
    "### Boosting for high bias and low variance models\n",
    "- Boositng is often used when the base learners (e.g., Decision Trees) tend to have high bias (underfitting) and low variance (low model complexity).\n",
    "- Bagging would not be ideal in this scenario because it focuses on averaging predictions from diverse models (high variance). Averaging underfitting models won't significantly improve performance.\n",
    "\n",
    "### Additive combining in Boosting\n",
    "Boosting addresses the high bias by sequentially training models (like Decision Trees) in an \"additive\" fashion. Each subsequent model \"boosts\" the overall performance by focusing on the erros of the previous model. Consider the following examples,\n",
    "- Imagine a dataset with target values (predicted v. actual values).\n",
    "- The first model (weak learner) might underfit the data, leading to sigificant errors (differences between predicted and actual values).\n",
    "- The second model is trained specifically on these errors, trying to learn from the mistakes of the first model. It essentially adds its corrective predictions to the first model's predictions.\n",
    "- This process continues iteratively, with each subsequent model focusing on the remaining errors from the previous ensemble.\n",
    "\n",
    "### Comparison with Bagging\n",
    "- Boosting is a sequential process. Each model builds upon the previous one.\n",
    "- Bagging, on the other hand, trains models in parallel on different data subsets (bootstrap samples).\n",
    "\n",
    "### Addressing bias in Boosting\n",
    "- Boosting achieves bias reduction by focusing on the errors of previous models. Each iteration aims to learn from the shortcomings of the ensemble so far, gradually reducing the overall bias.\n",
    "- Additionally, Boosting algorithms often adjust the weights of data points during training. Points that were misclassified by previous models receive higher weights, forcing the next model to pay more attention to those challenging examples.\n",
    "\n",
    "### Boosting algorithms and considerations\n",
    "- Common Boosting algorithms like AdaBoost and Gradient Boosting implement these principles in different ways.\n",
    "- While Boosting offers advantages, it can be computationally expensive due to the sequential nature of training. However, the performance gains can often outweigh the increased training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive Combining In Boosting\n",
    "### Step-by-step breakdown\n",
    "1. Simple model and residuals:\n",
    "    - The average or the mean model is the simple starting point ($M_0$).\n",
    "    - It predicts the average target value ($y_{cap}$) abd the residuals ($error_{i0}$) are calculated for each data point ($y_i$) as the difference between the actual value and the average prediction.\n",
    "2. Model on residuals:\n",
    "    - A second model ($M_1$) is trained on the residuals ($error_{i0}$). $M_1$ aims to fit these errors.\n",
    "    - In Boosting terminology, M1 is called a weak larner. It typically has high bias (underfitting) and low variance (low complexity), focusing on specific patterns in the errors.\n",
    "3. Additive prediction: The final prediction for each data point is achieved by adding the predictiona from $M_0$ (the average model) and $M_1$ (error model).\n",
    "    - $h_0(x_i) + h_1(x_i)$.\n",
    "    - This is where the additive aspect comes in. The corrections from each model are progressively added.\n",
    "4. Iterative process (optional):\n",
    "    - Boosting algorithms can continue this process by training additional models ($M_2$, $M_3$, etc) on the remaining errors from the previous ensemble.\n",
    "    - Each subsequent model focuses on the errors the ensemble has not yet captured effectively.\n",
    "\n",
    "### Example\n",
    "Say that out of 100 data points, 80 data points have been correctly predicted by $M_0$. $M_1$ tries to learn from the remaining 20 data points. Now say that $M_1$ has been able to predict 16 out of 20 data points correctly. The sum of $M_0$'s prediction and $M_1$'s corrections (16 out of 20) provides a potentially better overall prediction.\n",
    "\n",
    "### Classification v. regression\n",
    "- In regression, residuals represent the difference between the actual target value and the predicted value.\n",
    "- In classification, boosting algorithms might use probability differences instead of residuals for error calculations.\n",
    "\n",
    "### Addressing bias\n",
    "By iteratively focusing on the errors of previous models, boosting gradually reduces the overall bias. Each model in the ensemble adds it corrective power to improve the final prediction.\n",
    "\n",
    "### High bias v. high variance\n",
    "- High bias: High training error and high testing error (model underfits the data).\n",
    "- High variance: Low training error but high testing error (model overfits the training data).\n",
    "- Boosting is particualarly effective for models with high bias because it sequentially refines the predictions to reduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps In Boosting\n",
    "1. Mean model and error:\n",
    "    - Say that the mean weight of 67 has been calculated as the initial prediction ($M_0$ or $h_0(x)$) for all data points.\n",
    "    - The residuals ($error_0$) are computed as the difference between the actual weight and the mean weight for each data point.\n",
    "2. Model on residuals:\n",
    "    - The next model ($M_1$) uses the residuals ($error_0$) as its target variable. This is a crucial aspect of boosting.\n",
    "    - Building a simple Decision Tree (depth 1) based on gender to predict these residuals is a valid approach. $M_1$ acts as a weak learner focusing on patterns in the errors.\n",
    "3. Additive predictions and residuals\n",
    "    - $M_1$'s predictions for the residuals are added to the initial predictions ($M_0$) to create new predictions. This is the \"additive combining\" concept.\n",
    "    - Calculating new residuals based on these updated predictions is also correct.\n",
    "4. Overfitting and stopping (optional): Repeateadly adding models can lead to overfitting. Boosting algorithms typically use techniques like cross-validation or a stopping criterion (e.g., maximum number of models) to prevent this.\n",
    "5. Gamma ($\\gamma$) for model weights:\n",
    "    - Gamma ($\\gamma$), controls the influence of each model in the ensemble.\n",
    "    - The final prediction is therefore, $f_m(x) = (h_0(x) + \\gamma_1) * (h_1(x) + \\gamma_1) * (h_2(x) + \\gamma_2) * ... * (h_n(x) + \\gamma_n)$.\n",
    "    - This weighting factor is crucial. Boosting algorithms often determine these weights based on the performance of each model on the validation set. Models with lower errors get higher weights in the final prediction.\n",
    "\n",
    "### Differences from Linear Regression\n",
    "- In Linear Regression, all weights are determined in one step using the entire dataset.\n",
    "- In Boosting, weights (gamma) are assigned to each model dequentially based on their performance in correcting errors of previous models.\n",
    "\n",
    "### Challenges of Boosting\n",
    "- Boosting models are generally not parallelized due to the sequential nature of training each model based on the previous ones. This can be slower compared to some parallel Machine Learning algorithms (Bagging).\n",
    "- Improving underfitting models can be more challenging than dealing with overfitting. Boosting works best when the base learners have some level of bias (underfitting) that can be progressively reduced through additive corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting From Regression Perspective\n",
    "### Loss function and predictions\n",
    "Mean Squared Error (MSE) is used as the loss function for regression problems. It measures the squared difference between actual values ($y_i$) and predicted values ($y_{cap}$).\n",
    "\n",
    "### Gradient and pseudo-residuals\n",
    "- The key concept is the connection between gradients and psuedo-residuals.\n",
    "- Taking the partial derivative of the loss function (MSE) with respect to the predicted value ($y_{cap}$) gives the negative gradient.\n",
    "- This negative gradient is proportional to the actual residual ($y_i - y_{cap}$). It indicates the direction and maginitude for improvement in terms of reducing the loss.\n",
    "- However, calculating the full residual repeatedly can be computationally expensive.\n",
    "\n",
    "### Pseudo-residuals for efficiency\n",
    "- A pseudo-residual is the negative gradient of the loss function with respect to the predicted value ($y_{cap}$). It directly relates to the direction for loss reduction.\n",
    "- By using pseudo-residuals instead of full residuals in subsequent models, gradient boosting achieves computational efficiency.\n",
    "\n",
    "### Optimizing for pseudo-residuals\n",
    "- Optimizing for pseudo-residuals translates to optimizing for the overall loss function.\n",
    "- Since the pseudo-residual is proportional to the negative gradient, focusing on minimizing the pseudo-residuals drives the model in the direction of reducing the overall MSE.\n",
    "\n",
    "### Summary of Gradient Boosting algorithm\n",
    "1. Initialize: Start with an initial prediction (often the mean of the target variable). Assign equal weights to all data points.\n",
    "2. Iteratively train models: In each iteration,\n",
    "    - Fit a weak learner (e.g., Decision Tree) on the current data using the pseudo-residuals as the target variable.\n",
    "    - Calculate the model's weight based on its performance in reducing the loss (e.g., MSE).\n",
    "3. Update predictions: Update the overall prediction for each data point by adding the weighted prediction from the new learner to the previous ensemble prediction.\n",
    "\n",
    "### Overall benefit\n",
    "By iteratively fitting models on pseudo-residuals and adding their predictions, gradient boosting progressively reduces the overall loss function, leading to improved performance compared to a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
