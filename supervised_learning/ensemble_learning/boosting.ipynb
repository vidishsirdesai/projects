{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Boosting is a powerful ensemble Machine Learning technique used in both classification and regression tasks. It combines the predictions from multiple weak learners (oftentimes Decision Trees) to create a strong learner with improved performance.\n",
    "\n",
    "### Core idea\n",
    "- Boosting iteratively trains weak learners, where each learner focuses on correcting the errors of the previous ones.\n",
    "- Imagine a group of average students (weak learners) working together to solve a problem. Each student learns from the mistakes of the others, ultimately leading to a better understanding of the problem.\n",
    "\n",
    "### Boosting algorithm\n",
    "1. Initialize weights: Each data point in the training set is assigned and equal weight.\n",
    "2. Train weak learner: A weak learner (e.g., Decision Tree) is trained on the weighted data.\n",
    "3. Calculate error: The error of the weak learner is calculated based on the assigned weights. Misclassified points receive higher weights, focusing the next learner on those challenging examples.\n",
    "4. Adjust weights: Weights of the data points are adjusted based on the errors. More weight is given to points that the previous learners got wrong.\n",
    "5. Repeat: Steps 2 to 4 are repeated for multiple iterations, with each new learner focusing on the most difficult cases from the previous learner.\n",
    "6. Final prediction: The final prediction is made by combining the predictions from all the weak learners in the ensemble, often using a weighted voting (for classification) or averaging (for regression) approach.\n",
    "\n",
    "### Benefits of Boosting\n",
    "- Improved accuracy: By combining weaker models, boosting can achieve higher accuracy compared to individual learners.\n",
    "- Can handle complex problems: Boosting can learn complex relationships in the data that might be challenging for a single model.\n",
    "- Handles imbalanced data: Some boosting algorithms can effectively handle imbalanced datasets where certain classes have fewer data points.\n",
    "\n",
    "### Common Boosting algorithms\n",
    "- AdaBoost (Adaptive Boosting): A popular boosting algorithm that focuses on improving the weights of misclassified examples.\n",
    "- Gradient Boosting: A more general framework where the focus is on minimizing a loss function (e.g., squared error for regression) in each iteration. Common examples include,\n",
    "    - XGBoost: A powerful and scalable gradient boosting algorithm known for its performance.\n",
    "    - LightGBM: Another efficient gradient boosting algorithm with good performance and speed.\n",
    "\n",
    "### Considerations\n",
    "- Overfitting: Boosting algorithms can be prone to overfitting if not carefully tuned. Techniques like regularization can be used to mitigate this risk.\n",
    "- Computational cost: Training a boosted ensemble can be computationally expensive compared to a single model, especially with many iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging V. Boosting\n",
    "### Boosting for high bias and low variance models\n",
    "- Boositng is often used when the base learners (e.g., Decision Trees) tend to have high bias (underfitting) and low variance (low model complexity).\n",
    "- Bagging would not be ideal in this scenario because it focuses on averaging predictions from diverse models (high variance). Averaging underfitting models won't significantly improve performance.\n",
    "\n",
    "### Additive combining in Boosting\n",
    "Boosting addresses the high bias by sequentially training models (like Decision Trees) in an \"additive\" fashion. Each subsequent model \"boosts\" the overall performance by focusing on the erros of the previous model. Consider the following examples,\n",
    "- Imagine a dataset with target values (predicted v. actual values).\n",
    "- The first model (weak learner) might underfit the data, leading to sigificant errors (differences between predicted and actual values).\n",
    "- The second model is trained specifically on these errors, trying to learn from the mistakes of the first model. It essentially adds its corrective predictions to the first model's predictions.\n",
    "- This process continues iteratively, with each subsequent model focusing on the remaining errors from the previous ensemble.\n",
    "\n",
    "### Comparison with Bagging\n",
    "- Boosting is a sequential process. Each model builds upon the previous one.\n",
    "- Bagging, on the other hand, trains models in parallel on different data subsets (bootstrap samples).\n",
    "\n",
    "### Addressing bias in Boosting\n",
    "- Boosting achieves bias reduction by focusing on the errors of previous models. Each iteration aims to learn from the shortcomings of the ensemble so far, gradually reducing the overall bias.\n",
    "- Additionally, Boosting algorithms often adjust the weights of data points during training. Points that were misclassified by previous models receive higher weights, forcing the next model to pay more attention to those challenging examples.\n",
    "\n",
    "### Boosting algorithms and considerations\n",
    "- Common Boosting algorithms like AdaBoost and Gradient Boosting implement these principles in different ways.\n",
    "- While Boosting offers advantages, it can be computationally expensive due to the sequential nature of training. However, the performance gains can often outweigh the increased training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive Combining In Boosting\n",
    "### Step-by-step breakdown\n",
    "1. Simple model and residuals:\n",
    "    - The average or the mean model is the simple starting point ($M_0$).\n",
    "    - It predicts the average target value ($y_{cap}$) abd the residuals ($error_{i0}$) are calculated for each data point ($y_i$) as the difference between the actual value and the average prediction.\n",
    "2. Model on residuals:\n",
    "    - A second model ($M_1$) is trained on the residuals ($error_{i0}$). $M_1$ aims to fit these errors.\n",
    "    - In Boosting terminology, M1 is called a weak larner. It typically has high bias (underfitting) and low variance (low complexity), focusing on specific patterns in the errors.\n",
    "3. Additive prediction: The final prediction for each data point is achieved by adding the predictiona from $M_0$ (the average model) and $M_1$ (error model).\n",
    "    - $h_0(x_i) + h_1(x_i)$.\n",
    "    - This is where the additive aspect comes in. The corrections from each model are progressively added.\n",
    "4. Iterative process (optional):\n",
    "    - Boosting algorithms can continue this process by training additional models ($M_2$, $M_3$, etc) on the remaining errors from the previous ensemble.\n",
    "    - Each subsequent model focuses on the errors the ensemble has not yet captured effectively.\n",
    "\n",
    "### Example\n",
    "Say that out of 100 data points, 80 data points have been correctly predicted by $M_0$. $M_1$ tries to learn from the remaining 20 data points. Now say that $M_1$ has been able to predict 16 out of 20 data points correctly. The sum of $M_0$'s prediction and $M_1$'s corrections (16 out of 20) provides a potentially better overall prediction.\n",
    "\n",
    "### Classification v. regression\n",
    "- In regression, residuals represent the difference between the actual target value and the predicted value.\n",
    "- In classification, boosting algorithms might use probability differences instead of residuals for error calculations.\n",
    "\n",
    "### Addressing bias\n",
    "By iteratively focusing on the errors of previous models, boosting gradually reduces the overall bias. Each model in the ensemble adds it corrective power to improve the final prediction.\n",
    "\n",
    "### High bias v. high variance\n",
    "- High bias: High training error and high testing error (model underfits the data).\n",
    "- High variance: Low training error but high testing error (model overfits the training data).\n",
    "- Boosting is particualarly effective for models with high bias because it sequentially refines the predictions to reduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps In Boosting\n",
    "1. Mean model and error:\n",
    "    - Say that the mean weight of 67 has been calculated as the initial prediction ($M_0$ or $h_0(x)$) for all data points.\n",
    "    - The residuals ($error_0$) are computed as the difference between the actual weight and the mean weight for each data point.\n",
    "2. Model on residuals:\n",
    "    - The next model ($M_1$) uses the residuals ($error_0$) as its target variable. This is a crucial aspect of boosting.\n",
    "    - Building a simple Decision Tree (depth 1) based on gender to predict these residuals is a valid approach. $M_1$ acts as a weak learner focusing on patterns in the errors.\n",
    "3. Additive predictions and residuals\n",
    "    - $M_1$'s predictions for the residuals are added to the initial predictions ($M_0$) to create new predictions. This is the \"additive combining\" concept.\n",
    "    - Calculating new residuals based on these updated predictions is also correct.\n",
    "4. Overfitting and stopping (optional): Repeateadly adding models can lead to overfitting. Boosting algorithms typically use techniques like cross-validation or a stopping criterion (e.g., maximum number of models) to prevent this.\n",
    "5. Gamma ($\\gamma$) for model weights:\n",
    "    - Gamma ($\\gamma$), controls the influence of each model in the ensemble.\n",
    "    - The final prediction is therefore, $f_m(x) = (h_0(x) + \\gamma_1) * (h_1(x) + \\gamma_1) * (h_2(x) + \\gamma_2) * ... * (h_n(x) + \\gamma_n)$.\n",
    "    - This weighting factor is crucial. Boosting algorithms often determine these weights based on the performance of each model on the validation set. Models with lower errors get higher weights in the final prediction.\n",
    "\n",
    "### Differences from Linear Regression\n",
    "- In Linear Regression, all weights are determined in one step using the entire dataset.\n",
    "- In Boosting, weights (gamma) are assigned to each model dequentially based on their performance in correcting errors of previous models.\n",
    "\n",
    "### Challenges of Boosting\n",
    "- Boosting models are generally not parallelized due to the sequential nature of training each model based on the previous ones. This can be slower compared to some parallel Machine Learning algorithms (Bagging).\n",
    "- Improving underfitting models can be more challenging than dealing with overfitting. Boosting works best when the base learners have some level of bias (underfitting) that can be progressively reduced through additive corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting From Regression Perspective\n",
    "### Loss function and predictions\n",
    "Mean Squared Error (MSE) is used as the loss function for regression problems. It measures the squared difference between actual values ($y_i$) and predicted values ($y_{cap}$).\n",
    "\n",
    "### Gradient and pseudo-residuals\n",
    "- The key concept is the connection between gradients and psuedo-residuals.\n",
    "- Taking the partial derivative of the loss function (MSE) with respect to the predicted value ($y_{cap}$) gives the negative gradient.\n",
    "- This negative gradient is proportional to the actual residual ($y_i - y_{cap}$). It indicates the direction and maginitude for improvement in terms of reducing the loss.\n",
    "- However, calculating the full residual repeatedly can be computationally expensive.\n",
    "\n",
    "### Pseudo-residuals for efficiency\n",
    "- A pseudo-residual is the negative gradient of the loss function with respect to the predicted value ($y_{cap}$). It directly relates to the direction for loss reduction.\n",
    "- By using pseudo-residuals instead of full residuals in subsequent models, gradient boosting achieves computational efficiency.\n",
    "\n",
    "### Optimizing for pseudo-residuals\n",
    "- Optimizing for pseudo-residuals translates to optimizing for the overall loss function.\n",
    "- Since the pseudo-residual is proportional to the negative gradient, focusing on minimizing the pseudo-residuals drives the model in the direction of reducing the overall MSE.\n",
    "\n",
    "### Summary of Gradient Boosting algorithm\n",
    "1. Initialize: Start with an initial prediction (often the mean of the target variable). Assign equal weights to all data points.\n",
    "2. Iteratively train models: In each iteration,\n",
    "    - Fit a weak learner (e.g., Decision Tree) on the current data using the pseudo-residuals as the target variable.\n",
    "    - Calculate the model's weight based on its performance in reducing the loss (e.g., MSE).\n",
    "3. Update predictions: Update the overall prediction for each data point by adding the weighted prediction from the new learner to the previous ensemble prediction.\n",
    "\n",
    "### Overall benefit\n",
    "By iteratively fitting models on pseudo-residuals and adding their predictions, gradient boosting progressively reduces the overall loss function, leading to improved performance compared to a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "### Loss functions and gradients\n",
    "The common loss functions for regression (MSE/ RMSE) and classification (Log-Loss).\n",
    "\n",
    "### Pseudo-residuals for efficiency\n",
    "- Calculating full residuals repeateadly can be computationally expensive.\n",
    "- Pseudo-residuals, derived from the negative gradient of the loss function with respect to the prediction at the previous stage ($k - 1$), offer an efficient alternative.\n",
    "\n",
    "### Gradient boosting and pseudo-residuals\n",
    "- GBDT leverages pseudo-residuals to guide the training process.\n",
    "- At each stage ($k$), the gradient of the loss function is calculated with respect to the output at the previous stage ($k - 1$). This gradient serves as the pseudo-residual for the current stage.\n",
    "- Focusing on minimizing these pseudo-residuals during model fitting ensures the overall loss function (e.g., MSE or Log-Loss) is reduced iteratively.\n",
    "\n",
    "### Why Gradient Boosting?\n",
    "- By iteratively fitting models on pseudo-residuals and adding their predictions, GBDT progressively improves the model's performance compared to a single model.\n",
    "- This approach helps address the bias of weak learners (e.g., Decision Trees) by sequentially correcting their errors.\n",
    "\n",
    "### Additional notes\n",
    "- Different GBDT algorithms might use different techniques to calculate the optimal step size (learning rate) when applying the pseudo-residuals to update the predictions.\n",
    "- While GBDT is commonly used with Decision Trees as weak learners, the concept of pseudo-residuals can be applied with other weak learning algorithms as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Decision Tree (GBDT) Algorithm\n",
    "1. Data preparation: Prepare the training data by handling missing values, scaling features if necessary, and splitting it into training and validation sets (optional).\n",
    "2. Initialization:\n",
    "    - Initialize the model prediction for each data point (often the mean of the target variable in regression or a simple prediction rule in classification).\n",
    "    - Assign equal weights to all data points in the training set.\n",
    "3. For each iteration:\n",
    "    - Calculate pseudo-residuals:\n",
    "        - Based on the current ensemble prediction (at stage ($t - 1$)), calculate the loss function's gradient (e.g., negative gradient of MSE for regression or negative gradient of Log-Loss for classification) with respect to the prediction at the previous stage ($t - 1$).\n",
    "        - These gradients are the pseudo-residuals for the current iteration.\n",
    "    - Fit a weak learner:\n",
    "        - Train a weak learner (e.g., a shallow Decision Tree) on the training data using the calculated pseudo-residuals as the target variable.\n",
    "        - The goal of a weak learner is to improve upon the current ensemble prediction by focusing on the areas with high pseudo-residuals (large errors from the previous stage).\n",
    "    - Model weighting:\n",
    "        - Based on the weak learner's performance in reducing the loss function on the training set (e.g., using a learning rate), determine a weight for this learner.\n",
    "        - This weight reflects how much influence the learner's predictions will have in the final ensemble.\n",
    "4. Update ensemble prediction:\n",
    "    - For each data point, add the weighted prediction from the newly trained weak learner to the current ensemble prediction (additive combining).\n",
    "    - This creates an updated ensemble prediction that incorporates the improvements from the new weak learner.\n",
    "5. Stopping: Continue iteratively training models until a stopping criteria is met. Common criteria include,\n",
    "    - Reaching a predefined number of iterations.\n",
    "    - The validation error (error on a held-out set) starts to increase, indicating overfitting.\n",
    "6. Prediction for new data: To make a prediction for a new data point, apply the final ensemble model. This typically involves,\n",
    "    - Calculating the predictions from each weak learner in the ensemble.\n",
    "    - Adding the weighted predictions from all weak learner (similar to the update step during training).\n",
    "\n",
    "### Key points\n",
    "- GBDT iteratively improves the model by focusing on the errors (pseudo-residuals) from the previous models.\n",
    "- Weak learners (e.g., Decision Trees) are used as building blocks for the ensemble.\n",
    "- Pseudo-residuals, derived from the loss function gradient, guide the training process efficiently.\n",
    "- Model weights control the influence of each weak learner in the final prediction.\n",
    "- GBDT excels at addressing bias in weak learners by sequentially correcting their errors.\n",
    "\n",
    "### Additional considerations\n",
    "- GBDT models can be computationally expensive due to the sequential training process.\n",
    "- Tuning hyperparameters like the number of iterations, learning rate, and weak learner complexity is crucial for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm For Pseudo-Residuals In GBDT\n",
    "### Initialization\n",
    "- The argmin is used to find the initial prediction that minimizes the loss function ($L$) for all data points ($n$) with respect to a parameter ($\\mu$). Howeve, this parameter typically represents the initial prediction iteself (often the mean for regression).\n",
    "- So the minimization is to find the best constant value for the initial prediction ($F_0$) that minimizes the overall loss.\n",
    "\n",
    "### Core steps\n",
    "1. Pseudo-residual calculation:\n",
    "    - The derivative of the loss function is not calculated directly with respect to the entire model prediction ($f(x_i)$).\n",
    "    - In GBDT, the pseudo-residual for iteration m is calculated as the negative gradient of the loss function with respect to the prediction at the previous stage ($f_{(m - 1)}(x_i)$) for each data point $i$.\n",
    "    - Thiss reflects the direction for improvement in reducing the loss based on the previous ensemble prediction.\n",
    "2. Weak learner training:\n",
    "    - A weak learner (e.g., a shallow Decision Tree) is trained on these pseudo-learners.\n",
    "    - The goal of this weak learner is to learn patterns in the errors (residuals) from the previous ensemble prediction.\n",
    "3. Model weighting: While some algorithms might use a line search or optimization techniques, the core idea is to assign a weight to this weak learner based on its performance in reducing the loss on the training data. This weight reflects the learner's importance in the final ensemble.\n",
    "4. Ensemble update:\n",
    "    - The update of the ensemble prediction ($F_m(x)$) is done by adding the weighted prediction from the newly trained weak learner ($h_m(x)$) to the previous ensemble prediction (F_{(m - 1)}(x)).\n",
    "    - This additive combining is a key aspect of GBDT.\n",
    "\n",
    "### Overall process\n",
    "The core concept is to iteratively improve the ensemble prediction. In each iteration,\n",
    "- Calculate pseudo-residuals based on the previous ensemble prediction's errors.\n",
    "- Train a weak learner on these pseudo-residuals.\n",
    "- Assign a weight to the weak learner based on its performance.\n",
    "- Update the ensemble prediction by adding the weighted prediction from the weak learner.\n",
    "\n",
    "### Number of iterations:\n",
    "- The process is repeated for a predefined number of iterations (m).\n",
    "- The stopping criteria typically involves reaching a maximum number of iterations or observing signs of overfitting on a validation set.\n",
    "\n",
    "### Optimization:\n",
    "GBDT optimizes for 2 things,\n",
    "- The weak learners ($h_m(x)$) themselves during training to fit the pseudo-residuals.\n",
    "- The weights ($\\gamma_m$) assigned to each weak learner to determine their influence in the final ensemble prediction. These weights are often calculated based on the learner's performance in reducing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-Off In GBDT\n",
    "### Bias-variance in GBDT\n",
    "- GBDT leverages weak learners (often shallow Decision Trees) with high bias (underfitting) and low variance (low model complexity).\n",
    "- The goal is to iteratively reduce the bias of the ensemble model by combining the predictions from multile weak learners that focus on the errors (residuals) from previous stages.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "There are 2 crucial hyperparameters in GBDT that affect the bias-variance trade-off,\n",
    "1. Number of boosting stages ($m$): This controls the number of weak learners used in the ensemble.\n",
    "    - Increasing $m$ allows the model to capture more complex patterns, potentially reducing bias.\n",
    "    - However, a very high m can lead to overfitting as the model becomes sensitive to training data noise (high variance).\n",
    "2. Base learner complexity (depth): This determines the complexity of individual Decision Trees (weak learners).\n",
    "    - Shallower trees have lower variance but may not capture complex relationships (higher bias).\n",
    "    - Deeper Trees have the potential to reduce bias but can also lead to overfitting if they become too specific to the training data (high variance).\n",
    "\n",
    "### Finding the right balance:\n",
    "- The challenge lies in finding the optimal combination of $m$ and the depth that achieves a good balance between bias and variance.\n",
    "- Lower bias and lower variance generally lead to better generalization performance (performance on unseen data).\n",
    "\n",
    "### Additional considerations\n",
    "- Other hyperparameters like learning rate can also influence the bias-variance trade-off.\n",
    "- Techniques like cross-validation can be used to evaluate different hyperparameter configurations and identify the one that results in the best performance o a held out validation set.\n",
    "\n",
    "### Strategies to tune hyperparameters in GBDT\n",
    "- Grid search: evaluate a predefined grid of hyperparameter values and choose the combination with the best validation performance.\n",
    "- Random search: Sample hyperparameter values randomly from a defined range and select the best performing combination.\n",
    "- Early stopping: Stop training the model if the validation error starts to increase, preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect Of Outliers In GBDT\n",
    "### Potential benefits\n",
    "- Compared to some simpler models (e.g., Linear Regression), GBDT exhibits some level of robustness to outliers.\n",
    "- During each iteration, GBDT focuses on the pseudo-residuals, which are the gradients of the loss function with respect to the predictions from the previous stage.\n",
    "- Outliers with significant deviations from the overall trend will have larger pseudo-residuals.\n",
    "- The weak learner in that iteration can potentially capture this pattern and adjust the model's predictions to account for those outliers to some extent.\n",
    "\n",
    "### Potential drawbacks\n",
    "- GBDT's focus on pseudo-residuals can also be a downside in the presence of outliers.\n",
    "- If the weak learner in an iteration overfits to a few extreme outliers, it can introduce unnecessary complexity into the model.\n",
    "- This can lead to a decrease in the model's generalizability (performance on unseen data) as it focuses too much on fitting the outliers in the training data.\n",
    "\n",
    "### Overall effect\n",
    "The net effect of outliers on GBDT depends on several factors,\n",
    "- Number of outliers: A few outliers might be handled by the model, but a large number of outliers can significantly impact performance.\n",
    "- Distribution of outliers: Outliers far from the main cluster are more likely to cause problems.\n",
    "- Model complexity (hyperparameters):\n",
    "    - A model with a high number of boosting stages ($m$) or deep Decision Trees might be more susceptable to overfitting to outliers.\n",
    "    - Conversely, a simpler model with fewer stages or shallow Trees might not capture the underlying patterns even in the presence of some outliers (higher bias).\n",
    "\n",
    "### Mitigation strategies\n",
    "- Outlier detection and handling: Consider identifying and handling outliers before training the GBDT model (e.g., capping extreme values, removing outliers if justifiable).\n",
    "- Hyperparameter tuning: Carefully tune hyperparameters like $m$ and depth to find a balance between reducing bias and avoiding overfitting to outliers. Techniques like cross-validation can be helpful for this purpose.\n",
    "- Alternative loss functions: Explore loss functions less sesitive to outliers, such as Huber Loss or Trimmed Mean Squared Error, especially for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantages And Disadvantages Of GBDT\n",
    "### Advantages\n",
    "- High accuracy and flexibility: GBDT can achieve excellent accuracy on various regression and classification tasks due to its ability to learn complex patterns by combining mukltiple weak learners.\n",
    "- Handles diverse data types: GBDT can work effectively with different data types, including numerical and categorical features.\n",
    "- Robust to outliers: GBDT is relatively insensitive to outliers in the data compared to some simpler models.\n",
    "- Interpretability: While not as interpretable as linear models, GBDT models can be partially interpreted by analyzing the features used in the Decision Trees at each stage. Feature importance scores can be helpful for understanding which features contribute most to the model's predictions.\n",
    "- Handles missing data: GBDT can handle missing data inherently by splitting data points based on existing features in the Decision Trees.\n",
    "- Reduces bias: By iteratively focusing on errors from previous models, GBDT progressively reduces the overall bias of the ensemble.\n",
    "\n",
    "### Disadvantages\n",
    "- Computationally expensive: Training GBDT models can be computationally expensive due to the sequential training of multiple weak learners.\n",
    "- Prone to overfitting: If not tuned properly (especially with high $m$ or depth), GBDT models can overfit the training data and perform poorly on unseen data. Careful hyperparameter tuning and techniques like early stopping are crucial to mitigate this rist.\n",
    "- Black box nature: While partially interpretable, GBDT models can be complex, making it challenging to fully understand the internal logic behind their predictions.\n",
    "- Prone to high variance: With a large number of weak learners or high-depth Trees, GBDT models can become sensitive to training data noise, leading to high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "XGBoost stands for eXtreme Gradient Boosting, is an optimized implementation of the Gradient Boosting Decision Tree (GBDT) algorithm. It's known for its efficiency, accuracy, and scalability, making it popular choice for various Machine Learning tasks, especially regression and classification.\n",
    "\n",
    "### Core principles\n",
    "- XGBoost follows the core principles of GBDT, using multiple weak learners (often Decision Trees) to build a powerful ensemble model.\n",
    "- It iteratively trains models by focusing on the errors (pseudo-residuals) from previous stages to progressively improve the ensemble's performance.\n",
    "\n",
    "### Optimizations and improvements\n",
    "- RegualarizationL XGBoost incorporates various regularization techniques to prevent overfitting. These techniques penalize overly complex models, encouraging simpler Decision Trees in the ensemble.\n",
    "- Sparsity: XGBoost encourages sparse Decision Trees, meaning many of the splits in the Trees can involve only a few features. This promotes interpretability and reduces computational cost.\n",
    "- Parallelization: XGBoost is designed for efficient parallel and distributed computing, making it suitable for handling large datasets.\n",
    "- Improved objective functions: XGBoost offers built-in support for various objective functions (loss functions) tailored for different tasks (e.g., regression, classification, ranking).\n",
    "\n",
    "### Advantages of XGBoost\n",
    "- High performance: XGBoost often achieves state-of-the-art performance on various Machine Learning benchmarks.\n",
    "- Scalability: It can handle large datasets efficiently due to its optimized implementation and support for parallelization.\n",
    "- Flexibility: It suppports various objective functions and offers a wide range of hyperparameters to tune the model for specific tasks.\n",
    "- Regularization: Built-in regularization techniques help prevent overfitting and improve model generalizability.\n",
    "\n",
    "### Disadvantages of XGBoost\n",
    "- Complexity: Tuning XGBoost can be more complex compared to simpler algorithms due to its numerous hyperparameters.\n",
    "- Black box nature: While partially interpretable through feature importance scores, XGBoost models can be difficult to fully understand due to their ensemble nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters In XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
