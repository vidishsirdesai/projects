{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Boosting is a powerful ensemble Machine Learning technique used in both classification and regression tasks. It combines the predictions from multiple weak learners (oftentimes Decision Trees) to create a strong learner with improved performance.\n",
    "\n",
    "### Core idea\n",
    "- Boosting iteratively trains weak learners, where each learner focuses on correcting the errors of the previous ones.\n",
    "- Imagine a group of average students (weak learners) working together to solve a problem. Each student learns from the mistakes of the others, ultimately leading to a better understanding of the problem.\n",
    "\n",
    "### Boosting algorithm\n",
    "1. Initialize weights: Each data point in the training set is assigned and equal weight.\n",
    "2. Train weak learner: A weak learner (e.g., Decision Tree) is trained on the weighted data.\n",
    "3. Calculate error: The error of the weak learner is calculated based on the assigned weights. Misclassified points receive higher weights, focusing the next learner on those challenging examples.\n",
    "4. Adjust weights: Weights of the data points are adjusted based on the errors. More weight is given to points that the previous learners got wrong.\n",
    "5. Repeat: Steps 2 to 4 are repeated for multiple iterations, with each new learner focusing on the most difficult cases from the previous learner.\n",
    "6. Final prediction: The final prediction is made by combining the predictions from all the weak learners in the ensemble, often using a weighted voting (for classification) or averaging (for regression) approach.\n",
    "\n",
    "### Benefits of Boosting\n",
    "- Improved accuracy: By combining weaker models, boosting can achieve higher accuracy compared to individual learners.\n",
    "- Can handle complex problems: Boosting can learn complex relationships in the data that might be challenging for a single model.\n",
    "- Handles imbalanced data: Some boosting algorithms can effectively handle imbalanced datasets where certain classes have fewer data points.\n",
    "\n",
    "### Common Boosting algorithms\n",
    "- AdaBoost (Adaptive Boosting): A popular boosting algorithm that focuses on improving the weights of misclassified examples.\n",
    "- Gradient Boosting: A more general framework where the focus is on minimizing a loss function (e.g., squared error for regression) in each iteration. Common examples include,\n",
    "    - XGBoost: A powerful and scalable gradient boosting algorithm known for its performance.\n",
    "    - LightGBM: Another efficient gradient boosting algorithm with good performance and speed.\n",
    "\n",
    "### Considerations\n",
    "- Overfitting: Boosting algorithms can be prone to overfitting if not carefully tuned. Techniques like regularization can be used to mitigate this risk.\n",
    "- Computational cost: Training a boosted ensemble can be computationally expensive compared to a single model, especially with many iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging V. Boosting\n",
    "### Boosting for high bias and low variance models\n",
    "- Boositng is often used when the base learners (e.g., Decision Trees) tend to have high bias (underfitting) and low variance (low model complexity).\n",
    "- Bagging would not be ideal in this scenario because it focuses on averaging predictions from diverse models (high variance). Averaging underfitting models won't significantly improve performance.\n",
    "\n",
    "### Additive combining in Boosting\n",
    "Boosting addresses the high bias by sequentially training models (like Decision Trees) in an \"additive\" fashion. Each subsequent model \"boosts\" the overall performance by focusing on the erros of the previous model. Consider the following examples,\n",
    "- Imagine a dataset with target values (predicted v. actual values).\n",
    "- The first model (weak learner) might underfit the data, leading to sigificant errors (differences between predicted and actual values).\n",
    "- The second model is trained specifically on these errors, trying to learn from the mistakes of the first model. It essentially adds its corrective predictions to the first model's predictions.\n",
    "- This process continues iteratively, with each subsequent model focusing on the remaining errors from the previous ensemble.\n",
    "\n",
    "### Comparison with Bagging\n",
    "- Boosting is a sequential process. Each model builds upon the previous one.\n",
    "- Bagging, on the other hand, trains models in parallel on different data subsets (bootstrap samples).\n",
    "\n",
    "### Addressing bias in Boosting\n",
    "- Boosting achieves bias reduction by focusing on the errors of previous models. Each iteration aims to learn from the shortcomings of the ensemble so far, gradually reducing the overall bias.\n",
    "- Additionally, Boosting algorithms often adjust the weights of data points during training. Points that were misclassified by previous models receive higher weights, forcing the next model to pay more attention to those challenging examples.\n",
    "\n",
    "### Boosting algorithms and considerations\n",
    "- Common Boosting algorithms like AdaBoost and Gradient Boosting implement these principles in different ways.\n",
    "- While Boosting offers advantages, it can be computationally expensive due to the sequential nature of training. However, the performance gains can often outweigh the increased training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive Combining In Boosting\n",
    "### Step-by-step breakdown\n",
    "1. Simple model and residuals:\n",
    "    - The average or the mean model is the simple starting point ($M_0$).\n",
    "    - It predicts the average target value ($y_{cap}$) abd the residuals ($error_{i0}$) are calculated for each data point ($y_i$) as the difference between the actual value and the average prediction.\n",
    "2. Model on residuals:\n",
    "    - A second model ($M_1$) is trained on the residuals ($error_{i0}$). $M_1$ aims to fit these errors.\n",
    "    - In Boosting terminology, M1 is called a weak larner. It typically has high bias (underfitting) and low variance (low complexity), focusing on specific patterns in the errors.\n",
    "3. Additive prediction: The final prediction for each data point is achieved by adding the predictiona from $M_0$ (the average model) and $M_1$ (error model).\n",
    "    - $h_0(x_i) + h_1(x_i)$.\n",
    "    - This is where the additive aspect comes in. The corrections from each model are progressively added.\n",
    "4. Iterative process (optional):\n",
    "    - Boosting algorithms can continue this process by training additional models ($M_2$, $M_3$, etc) on the remaining errors from the previous ensemble.\n",
    "    - Each subsequent model focuses on the errors the ensemble has not yet captured effectively.\n",
    "\n",
    "### Example\n",
    "Say that out of 100 data points, 80 data points have been correctly predicted by $M_0$. $M_1$ tries to learn from the remaining 20 data points. Now say that $M_1$ has been able to predict 16 out of 20 data points correctly. The sum of $M_0$'s prediction and $M_1$'s corrections (16 out of 20) provides a potentially better overall prediction.\n",
    "\n",
    "### Classification v. regression\n",
    "- In regression, residuals represent the difference between the actual target value and the predicted value.\n",
    "- In classification, boosting algorithms might use probability differences instead of residuals for error calculations.\n",
    "\n",
    "### Addressing bias\n",
    "By iteratively focusing on the errors of previous models, boosting gradually reduces the overall bias. Each model in the ensemble adds it corrective power to improve the final prediction.\n",
    "\n",
    "### High bias v. high variance\n",
    "- High bias: High training error and high testing error (model underfits the data).\n",
    "- High variance: Low training error but high testing error (model overfits the training data).\n",
    "- Boosting is particualarly effective for models with high bias because it sequentially refines the predictions to reduce bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
