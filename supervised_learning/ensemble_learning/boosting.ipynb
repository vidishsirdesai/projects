{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Boosting is a powerful ensemble Machine Learning technique used in both classification and regression tasks. It combines the predictions from multiple weak learners (oftentimes Decision Trees) to create a strong learner with improved performance.\n",
    "\n",
    "### Core idea\n",
    "- Boosting iteratively trains weak learners, where each learner focuses on correcting the errors of the previous ones.\n",
    "- Imagine a group of average students (weak learners) working together to solve a problem. Each student learns from the mistakes of the others, ultimately leading to a better understanding of the problem.\n",
    "\n",
    "### Boosting algorithm\n",
    "1. Initialize weights: Each data point in the training set is assigned and equal weight.\n",
    "2. Train weak learner: A weak learner (e.g., Decision Tree) is trained on the weighted data.\n",
    "3. Calculate error: The error of the weak learner is calculated based on the assigned weights. Misclassified points receive higher weights, focusing the next learner on those challenging examples.\n",
    "4. Adjust weights: Weights of the data points are adjusted based on the errors. More weight is given to points that the previous learners got wrong.\n",
    "5. Repeat: Steps 2 to 4 are repeated for multiple iterations, with each new learner focusing on the most difficult cases from the previous learner.\n",
    "6. Final prediction: The final prediction is made by combining the predictions from all the weak learners in the ensemble, often using a weighted voting (for classification) or averaging (for regression) approach.\n",
    "\n",
    "### Benefits of Boosting\n",
    "- Improved accuracy: By combining weaker models, boosting can achieve higher accuracy compared to individual learners.\n",
    "- Can handle complex problems: Boosting can learn complex relationships in the data that might be challenging for a single model.\n",
    "- Handles imbalanced data: Some boosting algorithms can effectively handle imbalanced datasets where certain classes have fewer data points.\n",
    "\n",
    "### Common Boosting algorithms\n",
    "- AdaBoost (Adaptive Boosting): A popular boosting algorithm that focuses on improving the weights of misclassified examples.\n",
    "- Gradient Boosting: A more general framework where the focus is on minimizing a loss function (e.g., squared error for regression) in each iteration. Common examples include,\n",
    "    - XGBoost: A powerful and scalable gradient boosting algorithm known for its performance.\n",
    "    - LightGBM: Another efficient gradient boosting algorithm with good performance and speed.\n",
    "\n",
    "### Considerations\n",
    "- Overfitting: Boosting algorithms can be prone to overfitting if not carefully tuned. Techniques like regularization can be used to mitigate this risk.\n",
    "- Computational cost: Training a boosted ensemble can be computationally expensive compared to a single model, especially with many iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
