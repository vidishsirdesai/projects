{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. It's known for its effectiveness in handling high dimensional data and its ability to perform well even with limited training data.\n",
    "\n",
    "### Classification with SVMs\n",
    "- The core idea is to find an optimal hyperplane that separates the data points of different classes with the maximum margin.\n",
    "- A hyperplane is a decision boundary in n-dimensional space (n = number of features).\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class called the support vectors.\n",
    "- SVMs aim to maximize this margin, which intuitively leads to a better separation between classes and potentially a better generalization to unseen data.\n",
    "\n",
    "### Key components\n",
    "- Support vectors: These are the data points closest to the hyperplane that define the margin. They are crucial for training the model and influence the classification of new data points.\n",
    "- Kernel trick: This technique allows SVMs to handle non-linearly separable data. It essentially transforms the data into a higher-dimensional space where a linear separation might be possible. Common kernels include, linear, polynomial, and radial basis function (RBF).\n",
    "\n",
    "### Advantages of SVMs\n",
    "- Effective in high-dimensional spaces: SVMs can perform well even with a large number of features, making them suitable for complex datasets.\n",
    "- Robust to overfitting: The focus on maximizing the margin can help reduce overfitting, especially when dealing with limited training data.\n",
    "- Interpretability: In some cases, the decision boundary learned by the SVM can be visualized and interpreted, providing insights into the model's behavior.\n",
    "\n",
    "### Disadvantages of SVMs\n",
    "- Can be computationally expensive: Training SVMs can be slower than some other algorithms, especially for large datasets.\n",
    "- Parameter tuning: Choosing the right kernel and its hyperparameters is crucial for optimal performance and can involve experimentation.\n",
    "- Not ideal for very high-dimensional data: While SVMs can handle high dimensions, extremely high dimensionality can still pose challenges.\n",
    "\n",
    "### Applications of SVMs\n",
    "- Text classification (spam detection, sentiment analysis).\n",
    "- Image classification (object detection, handwriting recognition).\n",
    "- Bioinformatic data analysis (gene expression analysis).\n",
    "- Anomaly detection (fraud detection, system intrusion detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Algorithm\n",
    "### 1. Data representation\n",
    "- Each data point is represented as a vector of features ($x_i$) with a corresponding class label ($y_i$).\n",
    "- For example, if classifying emails as spam or ham, features might include frequencies, and class labels would be 1 (spam) or 0 (ham).\n",
    "\n",
    "### 2. Hyperplane\n",
    "- The goal is to find a hyperplane (a decision boundary) in the feature space that separates the data points of different classes with the maximum margin.\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class, called support vectors.\n",
    "\n",
    "### 3. Support vectors\n",
    "- These are the most critical training instances that define the margin.\n",
    "- They are typically the data points closest to the hyperplane on either side, one for each class.\n",
    "- The intuition is that these points have the most influence on the classification of new data points.\n",
    "\n",
    "### 4. Maximizing the margin\n",
    "- The SVM algorithm aims to maximize the margin between the hyperplane and the support vectors.\n",
    "- A larger margin intuitively leads to a better separation between classes and potentially better generalization to unseen data.\n",
    "\n",
    "### 5. Kernel trick (for non-linear data)\n",
    "- In some cases, the data might not be linearly separable in the original feature space.\n",
    "- The kernel trick addresses this by transforming the data into a higher-dimensional space where a linear separation might be possible.\n",
    "- Common kernel functions include,\n",
    "    - Linear kernel (for already linearly separable data).\n",
    "    - Polynomial kernel (transforms data to a higher-dimensional polynomial space).\n",
    "    - Radial Basis Function (RBF) kernel (projects data into a high-dimensional space using a Radial Basis Function).\n",
    "\n",
    "### 6. Classification of new data points\n",
    "Once the SVM is trained (hyperplane and support vectors identified), a new data point is classified by,\n",
    "- Transforming the data points into the same feature space as the training data (if using a kernel).\n",
    "- Calculating the distaance from the new point to the hyperplane.\n",
    "- Assigning the class label based on which side of the hyperplane the new point falls on.\n",
    "\n",
    "### Mathematical formulation (simplified)\n",
    "The decision for an SVM with a linear kernel can be expressed as, $f(x) = w^T * x + w_0$. Where,\n",
    "- $w$ = Weight vector (normal to the hyperplane).\n",
    "- $w_0$ = Bias term.\n",
    "- $x$ = New data point.\n",
    "\n",
    "The goal is to find $w$ and $w_0$ that maximize the margin while correctly classifying all training points. This involves solving a constrained optimization proble,\n",
    "\n",
    "### Learning algorithms\n",
    "Several algorithms are used to train SVMs, including, Sequential Minimal Optimization (SMO), a popular algorithm that efficiently solves the optimization problem for finding the optimal hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Margin Classifier\n",
    "### Problem\n",
    "The task is to classify data points (positive - green, negative - red) into separate classes using a hyperplace (decision boundary).\n",
    "\n",
    "### Challenge\n",
    "Choosing the \"best\" hyperplane among many possible ones.\n",
    "\n",
    "### Traditional approach\n",
    "Previously, algorithms might have considered all data points and evaluated different hyperplanes based on some criteria.\n",
    "\n",
    "### Maximum Margin Classifier (MMC)\n",
    "This approach focuses on maximizing the margin between the hyperplane and the closest data points (support vectors) from each class.\n",
    "\n",
    "### Key points\n",
    "1. Margin: The distance between the hyperplane and the closest data points (one from each class) on either side.\n",
    "2. Support vectors: These are the data points closest to the hyperplane and define the margin.\n",
    "3. Intuition: A larger margin intuitively leads to better separation between classes and potentially better generalization to unseen data.\n",
    "\n",
    "### Why support vectors?\n",
    "Instead of considering all data points, MMC only focuses on support vectors because,\n",
    "- They have the most influence on the classification of new data points.\n",
    "- Maximizing the margin around them ensures a good separation between classes for most other points as well.\n",
    "\n",
    "### Finding the best hyperplane\n",
    "1. Consider 2 hyperplanes ($\\pi_1$ and $\\pi_2$) and their corresponding support vectors.\n",
    "2. Create parallel lines ($\\pi+$ and $\\pi-$) on either side of each hyperplane, touching the support vectors.\n",
    "3. Calculate the distance between these parallel lines ($d_1$ and $d_2$) for each hyperplane.\n",
    "4. The hyperplane with the larger distance (larger margin) between its parallel lines is considered better (higher $d$ is better).\n",
    "\n",
    "### Formalization and optimization\n",
    "- The equation of a hyperplane can be expressed as, $w^T * x + w_0 = 0$. Where, $w$ = weight vector, and $w_0$ = bias term.\n",
    "- The margin has to be maximized, which can be classified as $\\frac{2 * k}{||w||}$. Where, k = distance between the hyperplane and its parallel lines.\n",
    "- However, the objective founction becomes, $\\argmin_{w, w_0} \\frac{||w||}{2}$ due to the inverse relationship.\n",
    "\n",
    "### Why not just minimize $||w||$?\n",
    "Minimizing $||w||$ alone does not gurantee a good separation. The hyperplane could collapse onto the support vectors, leading to poor performance on unseen data.\n",
    "\n",
    "### Constraints\n",
    "- To prevent the hyperplane from collapsing and to ensure a good separation, additional constraints are required (introduced in Hard Margin SVM, a specific MMC implementation).\n",
    "- These constraints typically penalize misclassifications of support vectors.\n",
    "\n",
    "### Summary\n",
    "The Maximum Margin Classifier focuses on maximizing the margin between the hyperplane and the closest data points (support vectors) to achieve good separation between classes and potentially better generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Margin Classifier\n",
    "### Labeling in SVM\n",
    "SVM uses labels +1 and -1 for positive and negative classes respectively. This is just a convention and doesn't affect the underlying concepts.\n",
    "\n",
    "### Constraints for hard margin\n",
    "- A hard margin classifier enforces stricter constraints compared to Maximum Margin Classifier (MMC).\n",
    "- It requires no points to be misclassified and lie between the hyperplane and the support vectors.\n",
    "\n",
    "### Formalizing the constraints\n",
    "- Two hyperplanes are defines, $\\pi+$ (above the decision boundary) and $\\pi-$ (below the decision boundary).\n",
    "- Every green point should satisfy $(w^T . x_i) * y_i >= 1$ (where $y_i$ is +1 for green points). This ensures they lie above $\\pi+$.\n",
    "- Every red point should satisfy $(w^T . x_i) * y_i <= 1$ (where $y_i$ is -1 for red points). This ensures they lie below $\\pi-$.\n",
    "\n",
    "### Functional margin\n",
    "- The functional margin is defined as the margin multiplied by the label ($y_i$).\n",
    "- This ensures both green points (positive label) and red points (negative label) have a positive functional margin.\n",
    "\n",
    "### Verifying correct classification\n",
    "- To check for misclassification, a point is multiplied with its label and is plugged into the equation.\n",
    "- A correctly classified point (either green or red) will always result in a positive value after this multiplication.\n",
    "\n",
    "### Challenges of hard margin\n",
    "- The strict constraint of no misclassification can be difficuly to achieve in practive, especially for datasets that are not perfectly linearly separable.\n",
    "- If the data is not linearly separable, a Hard Margin Classifier might not be able to find a valid hyperplane that satisfies the constraint for all points.\n",
    "\n",
    "### Why \"hard\" margin?\n",
    "- This approach is called as Hard Margin Classifier because it enforces a strict \"all or nothing\" rule. There is no room for even a single misclassification.\n",
    "- This makes it ideal only for scenarios with perfectly linearly separable data points, which can be rare in real-world datasets.\n",
    "\n",
    "### Limitations and Soft Margin Classifiers\n",
    "- Due to its rigid constraints, Hard Margin Classifiers can be impractical for real-world problems.\n",
    "- Soft Margin Classifiers addresses this by allowing a certain degree of misclassification. This allows them to handle non-lineraly separable data while still aiming for a good margin.\n",
    "\n",
    "### Summary\n",
    "Hard Margin Classifier is a powerful concept but has limitations due to its strict requirement of perfect separation. While it serves as a theoretical foundation for understanding maximum margin classification, Soft Margin Classifiers offer a more practical solution for mst real-world tasks where data might not be perfectly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Margin Classifier\n",
    "### Limitations of Hard Margin Classifiers\n",
    "- Hard Margin Classifiers enforce a strict constraints of no misclassifications.\n",
    "- This can be unrealistic for real-world datasets that might not be perfectly linearly separable.\n",
    "\n",
    "### Soft margins\n",
    "- Soft Margin Classifiers address this limitation by allowing a certain degree of misclassification.\n",
    "- This allows for more flexibility in handling non-ideal data while still aiming for a good separation between classes.\n",
    "\n",
    "### Error calculation\n",
    "- In soft margin classification, an error term ($E_i$) is defined for each data point.\n",
    "- This error represents the deviation of a point from its ideal distance to the margin (typically 1 unit).\n",
    "- This error is calculated as, $E_i = 1 - Z_i$ , where $Z_i$ is the output of $(w^T . x_i) + w_0$ for the point.\n",
    "\n",
    "### Constraint relaxation\n",
    "- Unlike Hard Margin Classifiers where, $Z_i * y_i >= 1$ must always hold, Soft Margin Classifiers introduce a slack variable ($\\xi_i$) (pronounced, xi).\n",
    "- The constraint becomes $Z_i * y_i >= 1 - E_i$ (with $\\xi_i >= 0$). This allows points to violate the margin by a certain amount ($E_i$) as long as the slack variable ($\\xi_i$) is non-negative.\n",
    "\n",
    "### Loss function with soft margin\n",
    "The objective function in Soft Margin Classifiers aims to minimize two things,\n",
    "- The norm of the weight vector (w) - similar to Hard Margins (encourages a large margin).\n",
    "- The total error caused by misclassifications (sum of $E_i$).\n",
    "\n",
    "### Ignoring irrelevant errors\n",
    "- The points that are correctly classfied are not penalized even if they fall within the margin ($E_i < 0$).\n",
    "- The slack variable ($\\xi_i$) ensures a loss of zero for such points (since $\\xi_i$ is non-negative).\n",
    "\n",
    "### Trade-off parameter ($C$)\n",
    "- The degree to which misclassifications are tolerated is controlled by a hyperparameter called $C$.\n",
    "- A higher $C$ emphasizes a larger margin, potentially leading to stricter classification but might be less robust to noise.\n",
    "- A lower $C$ allows for more misclassifications but might result in a smaller margin and potentially lower performance on unseen data.\n",
    "\n",
    "### Summary\n",
    "Soft Margin Classifiers offer a more practical approach for real-world data by allowing some degree of misclassification. They introduce a trade-off between maximizing the margin and minimizing errors, controlled by the hyperparameter $C$. This flexibility makes them widely used in various machine learning tasks, particularly when dealing with non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition Of Hinge Loss\n",
    "### Scenario\n",
    "Imagine a line ($\\pi$) representing the decision boundary in an SVM with positive margins ($\\pi+$) and negative margins ($\\pi-$). Say that there are 4 data points ($x_1$, $x_2$, $x_3$, $x_4$) labeled positive ($y_i = +1$).\n",
    "\n",
    "### Points and their classification\n",
    "- $x_1$: Lies above $\\pi+$ (correctly classified).\n",
    "- $x_2$: Lies on $\\pi+$ (correctly classified on the margin).\n",
    "- $x_3$: Lies on $\\pi$ (correctly classifies on the decision boundary).\n",
    "- $x_4$: Lies on $\\pi-$ (misclassified on the negative margin).\n",
    "\n",
    "### Hinge loss and errors\n",
    "- Hinge loss penalizes points that fall within the margin or are misclassified. The further a point deviates from the correct side of the margin, the higher the loss.\n",
    "- Errors ($E_i$) are calculated as, $E_i = 1 - Z_i$ , where $Z_i$ is the output of $(w^T . x_i) + w_0$ for the point.\n",
    "\n",
    "### Observations\n",
    "- $x_1$ is correctly classified ($Z_i > 1$), so its error is $E_i = 0$ and hinge loss is 0 (no penatly).\n",
    "- $x_2$ is also correctly classified ($Z_i = 1$), so its error is $E_i = 0$ and hinge loss is 0 (no penatly).\n",
    "- $x_3$ lies on the decision boundary ($Z_i = 0$), leading to $E_i = 1$ and a hinge loss of 1 (maximum penalty for points on the boundary).\n",
    "- $X_4$ is misclassified ($Z_i < 0$), resulting in positive error ($E_i$) and a hinge loss that increases as $Z_i$ becomes more negative.\n",
    "\n",
    "### Hinge loss function\n",
    "Based on the observations, the observation, the hinge loss function can be simplified as,\n",
    "- $\\text{Hinge Loss}(Z_i) = \\max(0, (1 - Z_i))$.\n",
    "- This function ensures no penalty for correctly classified points ($Z_i >= 1$) and increasing penalties for points deviating from the correct side of the margin.\n",
    "\n",
    "### Comparison with Logistic Regression\n",
    "- Logistic Regression uses Log-Loss, which assigns probabilities (0 to 1) to classes. It penalizes the model based on the difference between predicted and actual probabilities.\n",
    "- SVM uses hinge loss, which works with labels (-1, +1) and focuses on maximizing the margin. It penalizes points that violate the margin or are misclassified.\n",
    "\n",
    "### Summary\n",
    "Hinge loss is a core concept in SVMs. It encourages a larger margin by heavily penalizing points that fall within the margin or are misclassified. This focus on the margin helps SVMs achieve good separation between classes, especially for data that might not be perfectly linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation Of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_clean.csv\", encoding = \"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidishsirdesai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vidishsirdesai/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vidishsirdesai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple text processing function\n",
    "import re\n",
    "\n",
    "def clean_tokenized_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Performs basic cleaning of tokenized sentence.\n",
    "    \"\"\"\n",
    "    # an empty string to store the processes sentence\n",
    "    cleaned_sentence = \"\"\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        # convert to lowercase\n",
    "        cleaned_word = word.lower()\n",
    "        # remove punctuations by substitution\n",
    "        cleaned_word = re.sub(r\"[^\\w\\s]\", \"\", cleaned_word)\n",
    "\n",
    "        # remove stopwords\n",
    "        if cleaned_word != \"\" and cleaned_word not in nltk.corpus.stopwords.words(\"english\"):\n",
    "            # append the processed words to new list\n",
    "            cleaned_sentence = cleaned_sentence + \" \" + cleaned_word\n",
    "        \n",
    "    return (cleaned_sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>spam</td>\n",
       "      <td>Last Chance! Claim ur ï¿½150 worth of discount...</td>\n",
       "      <td>last chance claim ur ï½150 worth discount vouc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>ham</td>\n",
       "      <td>Gud mrng dear hav a nice day</td>\n",
       "      <td>gud mrng dear hav nice day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>ham</td>\n",
       "      <td>No I'm good for the movie, is it ok if I leave...</td>\n",
       "      <td>good movie ok leave hourish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm at home. Please call</td>\n",
       "      <td>home please call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ugh my leg hurts. Musta overdid it on mon.</td>\n",
       "      <td>ugh leg hurts musta overdid mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>ham</td>\n",
       "      <td>I met you as a stranger and choose you as my f...</td>\n",
       "      <td>met stranger choose friend long world stands f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3907</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sounds like a plan! Cardiff is still here and ...</td>\n",
       "      <td>sounds like plan cardiff still still cold sitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT This is our 2nd attempt to contact U. Y...</td>\n",
       "      <td>urgent 2nd attempt contact u ï½900 prize yeste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>ham</td>\n",
       "      <td>Neshanth..tel me who r u?</td>\n",
       "      <td>neshanth tel r u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ill be there on  &amp;lt;#&amp;gt;  ok.</td>\n",
       "      <td>ill lt gt ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                            message  \\\n",
       "800   spam  Last Chance! Claim ur ï¿½150 worth of discount...   \n",
       "3731   ham                       Gud mrng dear hav a nice day   \n",
       "1113   ham  No I'm good for the movie, is it ok if I leave...   \n",
       "4231   ham                           I'm at home. Please call   \n",
       "5199   ham         Ugh my leg hurts. Musta overdid it on mon.   \n",
       "594    ham  I met you as a stranger and choose you as my f...   \n",
       "3907   ham  Sounds like a plan! Cardiff is still here and ...   \n",
       "3891  spam  URGENT This is our 2nd attempt to contact U. Y...   \n",
       "2397   ham                          Neshanth..tel me who r u?   \n",
       "1503   ham                    Ill be there on  &lt;#&gt;  ok.   \n",
       "\n",
       "                                        cleaned_message  \n",
       "800   last chance claim ur ï½150 worth discount vouc...  \n",
       "3731                         gud mrng dear hav nice day  \n",
       "1113                        good movie ok leave hourish  \n",
       "4231                                   home please call  \n",
       "5199                    ugh leg hurts musta overdid mon  \n",
       "594   met stranger choose friend long world stands f...  \n",
       "3907  sounds like plan cardiff still still cold sitt...  \n",
       "3891  urgent 2nd attempt contact u ï½900 prize yeste...  \n",
       "2397                                   neshanth tel r u  \n",
       "1503                                       ill lt gt ok  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the text cleaning function to the dataset\n",
    "df[\"cleaned_message\"] = df[\"message\"].apply(clean_tokenized_sentence)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_message\"] = df[\"cleaned_message\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   type             5572 non-null   object\n",
      " 1   cleaned_message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# dropping the \"message\" column\n",
    "df.drop(columns = [\"message\"], inplace = True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3319</th>\n",
       "      <td>0</td>\n",
       "      <td>eh sorry leh din c ur msg sad already lar watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>0</td>\n",
       "      <td>place get rooms cheap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>0</td>\n",
       "      <td>yo sorry shower sup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>0</td>\n",
       "      <td>message text missing sender name missing numbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>1</td>\n",
       "      <td>currently message awaiting collection collect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4554</th>\n",
       "      <td>0</td>\n",
       "      <td>7 wonders world 7th 6th ur style 5th ur smile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>0</td>\n",
       "      <td>free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>1</td>\n",
       "      <td>phony ï½350 award todays voda numbers ending x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0</td>\n",
       "      <td>take post come must 1000s texts happy reading ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>1</td>\n",
       "      <td>want new video phone 750 anytime network mins ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                    cleaned_message\n",
       "3319     0  eh sorry leh din c ur msg sad already lar watc...\n",
       "4015     0                              place get rooms cheap\n",
       "4594     0                                yo sorry shower sup\n",
       "1139     0  message text missing sender name missing numbe...\n",
       "2943     1  currently message awaiting collection collect ...\n",
       "4554     0  7 wonders world 7th 6th ur style 5th ur smile ...\n",
       "3362     0                                               free\n",
       "4989     1  phony ï½350 award todays voda numbers ending x...\n",
       "341      0  take post come must 1000s texts happy reading ...\n",
       "4154     1  want new video phone 750 anytime network mins ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the target column \"type\"\n",
    "def encode_type(i):\n",
    "    encode = {\n",
    "        \"spam\": 1,\n",
    "        \"ham\": 0\n",
    "    }\n",
    "    return encode[i]\n",
    "\n",
    "df[\"type\"] = df[\"type\"].apply(encode_type)\n",
    "# df[\"type\"] = df[\"type\"].apply({\"spam\": 1, \"ham\": 0})\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4179,), (1393,), (4179,), (1393,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"cleaned_message\"], df[\"type\"], test_size = 0.25, random_state = 42)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4179, 7612), (1393, 7612))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to bag of words and then to features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "# converts to BoW and then to features\n",
    "x_test = vectorizer.transform(x_test)\n",
    "# size of the bag\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4179, 7612), (1393, 7612))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(with_mean = False)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=SVC(class_weight={0: 0.1, 1: 0.5}, kernel=&#x27;linear&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: [0.0001, 0.001, 0.01, 0.1, 1, 10]}, scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=3,\n",
       "             estimator=SVC(class_weight={0: 0.1, 1: 0.5}, kernel=&#x27;linear&#x27;),\n",
       "             param_grid={&#x27;C&#x27;: [0.0001, 0.001, 0.01, 0.1, 1, 10]}, scoring=&#x27;f1&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: SVC</label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=0.001, class_weight={0: 0.1, 1: 0.5}, kernel=&#x27;linear&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=0.001, class_weight={0: 0.1, 1: 0.5}, kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=SVC(class_weight={0: 0.1, 1: 0.5}, kernel='linear'),\n",
       "             param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10]}, scoring='f1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Classifier (SVC)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"C\": [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "sv_classifier = SVC(class_weight = {0: 0.1, 1: 0.50}, kernel = \"linear\")\n",
    "sv_classifier_grid_search = GridSearchCV(sv_classifier, params, scoring = \"f1\", cv = 3)\n",
    "sv_classifier_grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'C': 0.0001}\n",
      "Mean score: 0.6497817092458725\n",
      "Rank: 6\n",
      "\n",
      "Parameters: {'C': 0.001}\n",
      "Mean score: 0.7839609357761171\n",
      "Rank: 1\n",
      "\n",
      "Parameters: {'C': 0.01}\n",
      "Mean score: 0.7788439544939494\n",
      "Rank: 2\n",
      "\n",
      "Parameters: {'C': 0.1}\n",
      "Mean score: 0.7692010027948709\n",
      "Rank: 3\n",
      "\n",
      "Parameters: {'C': 1}\n",
      "Mean score: 0.7692010027948709\n",
      "Rank: 3\n",
      "\n",
      "Parameters: {'C': 10}\n",
      "Mean score: 0.7692010027948709\n",
      "Rank: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = sv_classifier_grid_search.cv_results_\n",
    "\n",
    "for i in range(len(res[\"params\"])):\n",
    "    print(f\"Parameters: {res['params'][i]}\")\n",
    "    print(f\"Mean score: {res['mean_test_score'][i]}\")\n",
    "    print(f\"Rank: {res['rank_test_score'][i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.001, class_weight={0: 0.1, 1: 0.5}, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(C=0.001, class_weight={0: 0.1, 1: 0.5}, kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.001, class_weight={0: 0.1, 1: 0.5}, kernel='linear')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing SVC with the best hyperparameter as found\n",
    "sv_classifier = SVC(C = 0.001, class_weight = {0: 0.1, 1:0.5}, kernel = \"linear\")\n",
    "sv_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sv_classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.88)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "np.round(f1_score(y_test, y_pred), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal And Dual Formulations Of SVM\n",
    "### Primal form of SVM\n",
    "- Equation: $\\min(w, b) \\frac{||w||}{2} + \\frac{C}{N} \\sum_{i = 1}^{n} E_i \\text{, such that } (w.T * x_{query} + b) * y_i >= 1 - E_i \\text{, for all } i = 1 to n$.\n",
    "- The target is to minimize a function that has 2 parts,\n",
    "    - Regularization term: $\\frac{||w||}{2}$ penalizes large values of the weight vector w. This helps to prevent overfitting by keeping the model complexity under control.\n",
    "    - Hinge loss term: $\\frac{C}{N} \\sum_{i = 1}^{n} E_i$ measures the total error made by the model on the training data. $E_i$ represents the slack variable for data points $i$, indicating the amount by which the model's prediction deviates from the true label for that point. $C$ is a regularization parameter that controls the trade-off between maximizing the margin and allowing for some misclassification.\n",
    "- The constraints: $(w^T * x_i + b) * y_i >= 1 - E_i$ ensures that for all data points $i$,\n",
    "    - The decision boundary separates correctly classified points by a margin of atleast 1.\n",
    "    - The slack variable ($E_i$) allows for some misclassification by permitting violations of the margin with a penalty in the loss term.\n",
    "- Solving the primal form can be computationally expensive, especially for large datasets. This is because it involves optimizing a non-smooth function (hinge loss) with respect to $w$ and $b$.\n",
    "\n",
    "### Dual form of SVM\n",
    "- Equation: $\\max(\\alpha_i) (\\sum_{i = 1}^{n}\\alpha_i) - (\\frac{1}{2} \\sum_{i = 1}^{n} \\sum_{j = 1}^{n}\\alpha_i * \\alpha_j * y_i * y_j * x_i^T . x_j), \\text{ such that } 0 <= α_i <= C, \\text{ and } \\sum_{i = 1}^{n} \\alpha_i * y_i = 0$.\n",
    "- The dual form of the SVM provides an alternative way to solve the same optimization problem. It utilizes a concept called Lagrange duality to introduce Lagrange multipliers ($\\alpha_i$) and convert the problem into maximizing a function with respect to these multipliers. This new formulation often leads to more efficient optimization process.\n",
    "- Breakdown of dual form:\n",
    "    - The dual form involves maximizing a function based on the Lagrange multipliers ($\\alpha_i$).\n",
    "    - The dual form avoids explicit calculations of $w$ and $w_0$ and instead focuses on the $\\alpha_i$ which are associated with each data point.\n",
    "    - The dual form also includes constraints on the $\\alpha_i$ to ensure they fall within a specific range and satisfy a specific condition.\n",
    "\n",
    "### Dual form and support vectors (SVs)\n",
    "The key connection between dual form and SVs lies in the Lagrange multipliers ($\\alpha_i$). These multipliers play a crucial role in identifying the most important data points for defining the decision boundary.\n",
    "- Non-zero $\\alpha_i$: Data points with $\\alpha_i$ greater than 0 are considered SVs. These are points that lie closest to the decision boundary or violate the margin in the primal form. They are the most informative points for the model and significantly influence the classification.\n",
    "- Zero $\\alpha_i$: Data point with $\\alpha_i$ equal to zero are irrelevant for the decision boundary in the dual form. They either lie far away from the margin or are correctly classified with a large margin. These points contribute minimally to the model's prediction.\n",
    "\n",
    "### Misclassification and the soft margin\n",
    "The primal form allows for some misclassifications through the slack variables $E_i$ and the $C$ parameter.This is known as Soft Margin SVM. By introducing these slack variables, the model can tolerate some errors on the training data in exchange for a potentially better generalization to unseen data. The $C$ parameter controls the severity of the penalty for misclassifications. A higher $C$ enforces a larger margin and reduces misclassification, while a lower $C$ allows for more misclassification on the training data but might lead to better generalization.\n",
    "\n",
    "### Prediction in the dual form\n",
    "For a new data point ($x_{query}$), the prediction is made using a weighted sum of the features of the support vectors ($x_i$). The weights are determined by the corresponding Lagrange multipliers ($\\alpha_i$) and the labels ($y_i$) of the support vectors. Only the support vectors contribute to the prediction since their $\\alpha_i$ values are non-zero.\n",
    "\n",
    "### Summary\n",
    "The dual form of SVM offers a computationally efficient way to solve the SVM optimization problem. It identifies a subset of data points (support vectors) that are most crucial for defining the decision boundary. By allowing for some misclassifications with the soft margin approach, the model can achieve better generalization on unseen data. In practice, during prediction, only the features and labels of the support vectors and the corresponding Lagrange multipliers are needed, making the model memory-efficient.\n",
    "\n",
    "### Additional notes\n",
    "The primal form is useful for interpreting the model's decision boundary. However, the dual form is often preferred for solving optimization problems, especially when dealing with large datasets or high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "The concept of kernels in SVMs hinges on the idea of replacing the dot product with a more general similarity measure.\n",
    "\n",
    "### Dot product and similarity\n",
    "The dot product of two vectors ($x_i^T * x_j$) captures their linear similarity. A higher dot product indicates greater alognment in the same direction.\n",
    "\n",
    "### Limitations of linear SVMs\n",
    "- Linear SVMs can only create linear decision boundaries, which are essentially straight lines or hyperplanes in higher dimensions.\n",
    "- Real-world data often exhibits non-linear relationships that cannot be separated by a single straight line.\n",
    "\n",
    "### Feature transformation for non-linearity\n",
    "- One approach to handle non-linear data is to manually transform the features into a higher-dimensional space where linear separation becomes possible.\n",
    "- For example, you could create new features based on polynomial combinations of the original features like, $x_1^2$, $x_1 * x_2$, etc.\n",
    "- However, this approach can be computationally expensive and requires careful selection of the right transformations.\n",
    "\n",
    "### The kernel trick\n",
    "- Kernels provide a way to implicitly perform feature transformation without explicitly calculating the new features.\n",
    "- The kernel function $K(x_i, x_j)$ takes two data points ($x_i$ and $x_j$) as input and outputs a similarity measure.\n",
    "- This similarity measure is equivalent to the dot product that would be calculated in the higher-dimensional space after the feature transformation.\n",
    "\n",
    "### Benefits of kernels\n",
    "- Kernels allow SVMs to learn non-linear decision boundaries without explicitly computing the high-dimensional feature space.\n",
    "- This can significantly improve the model's performance on non-linearly separable data.\n",
    "- Some kernels, like the Radial Basis Function (RBF) kernel, have fewer hyperparameters to tune compared to polynomial kernels.\n",
    "\n",
    "### Popular kernels\n",
    "- Polynomial kernel: This kernel raises the dot product of the original features to a power (e.g., ($(x_i^T . x_j + c)^m$)). It effectively creates new features based on polynomial combinations of the original features. The degree $m$ is a hyperparameter that needs to be tuned.\n",
    "- Radial Basis Function (RBF) kernel: This kernel uses a Gaussian function to measure similarity based on the distance between data points in the original space. The spread of the Gaussian is controlled by a hyperparameter $\\sigma$. Unlike the polynomial kernel, it does not require choosing a specific feature transformation dimension.\n",
    "\n",
    "### Trade-offs\n",
    "- Choosing the right kernel and its hyperparameters is crucial for optimal performance.\n",
    "- Kernels can increase computational cost during training compared to the linear SVM due to the kernel function evaluations.\n",
    "\n",
    "### Prediction with kernels\n",
    "- During prediction, the model only needs to compute the kernel function between the query point and the support vectors.\n",
    "- This can be more efficient than calculating distances in the original space (like KNN) for large datasets, especially when the number of support vectors is small.\n",
    "- In essence, kernels offer a powerful way to equip SVMs with the ability to learn non-linear decision boundaries while maintaining computational efficiency during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Kernel Transformation\n",
    "Linear SVMs can only create linear decision boundaries, which are insufficient for handling non-linear data.\n",
    "\n",
    "### Manual feature transformation\n",
    "- One approach is to manually transform the original features into a higher-dimensional space where linear separation becomes possible.\n",
    "- This involves creating new features based on polynomial combinations of the original features e.g., ($x_1^2$), ($x_1 * x_2$), etc.\n",
    "\n",
    "### The polynomial kernel\n",
    "The polynomial kernel offers an alternative to manual feature transformation. It implicitly performs the transformation without explicitly calculating the new features.\n",
    "\n",
    "### How does it work?\n",
    "1. The kernel function: The polynomial kernel takes two data points ($x_i$ and $x_j$) as input and computes their similarity using the formula,\n",
    "    - $K(x_i, x_j) = ((x_i^T * x_j) + c )^m$.\n",
    "    - Here, $m$ is the degree of the polynomial (hyperparameter) and $c$ is the constant (often set to 1).\n",
    "2. Implicit transformation: Raising the dot product to a power creates features that capture interactions between the original features. For example, a quadratic kernel ($m$ = 2) considers features like, ($x_1^2$), ($x_2^2$) and ($x_1 * x_2$).\n",
    "3. Equivalent to higher-dimensional dot product: The kernl function essentially calculates the dot product between the data points in the higher-dimensional feature space created by the polynomial transformation.\n",
    "\n",
    "### Example with quadratic kernel\n",
    "Consider 2 data points $x_1$ and $x_2$ with 2 features each. Applying the quadratic kernel,\n",
    "- $K(x_1, x_2) = (1 + (x_{11} * x_{21}) + (x_{12} * x_{22}))^2$.\n",
    "- This expands to a combination of terms representing interations between original features, $1 + (x_{11}^2 + x_{12}^2) + (x_{21}^2 + x_{22}^2) + (2 * x_{11} * x_{21}) + (2 * x_{11} * x_{21} * x_{12} * x_{22})$.\n",
    "\n",
    "### Benefits\n",
    "Polynomial kernels allow SVMs to learn non-linear decision boundaries without explicitly computing the high-dimensional space.\n",
    "\n",
    "### Drawbacks\n",
    "- Choosing the degree $m$ is crucial. A high degree can lead to overfitting due to the explosion of the features.\n",
    "- The computational cost can increase with the degree of the polynomial.\n",
    "\n",
    "### Conclusion\n",
    "The polynomial kernel is a powerful tool for handling non-linear data in SVMs. However, careful selection of the degree and awareness of potential overfitting are essential for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basis Function (RBF)\n",
    "Polynomial kernels offer a way to capture non-linearity, but they require choosing the degree of the polynomial, which can be challenging. The RBF kernel provides an alternative.\n",
    "\n",
    "### The RBF kernel function\n",
    "The RBF kernel measures similarity between data points using a Gaussian function,\n",
    "- $k_{RBF}(x1, x2) = e^{\\frac{-||x1 - x2||^2}{2*(σ^2}}$.\n",
    "- Here, $||x_1 - x_2||$ is the Euclidean distance between the data poins, and $\\sigma$ is a hyperparameter that controls the spread of the Gaussian.\n",
    "\n",
    "### Similarity based on distance\n",
    "- The RBF kernel assigns higher similarity scores to closer data points (smaller distance) due to the higher value of the Gaussian function.\n",
    "- Conversely, points farther apart receive lower similarity scores.\n",
    "- Points very close together will have a similarity score close to 1.\n",
    "\n",
    "### Benefits of RBF kernel\n",
    "- Unlike polynomial kernels, RBF kernels don't require defining a specific feature soace dimension. This eliminates the need to choose a polynomial degree.\n",
    "- Compared to polynomial kernels with their degree parameter, RBF kernels often require tuning only one hyperparameter ($\\sigma$).\n",
    "\n",
    "### Understanding the $\\sigma$ parameter\n",
    "- A smaller $\\sigma$ value creates a steeper Gaussian curve, making similarity highly dependent on being very close in the original space.\n",
    "- A larger $\\sigma$ value leads to a smoother, flatter Gaussian curve, where points farther apart can still have some degree of similarity.\n",
    "\n",
    "### Connection to KNN\n",
    "The concept behind the $\\sigma$ parameter in the RBF kernel is similar to the k parameter in K-Nearest Neighbors (KNN). Both influence how localized the decision boundary is based on the nearest neighbors.\n",
    "\n",
    "### RBF kernel and prediction efficiency\n",
    "- During prediction, SVMs with RBF kernels only need to compute the kernel function for the query point and the support vectors.\n",
    "- This can be more efficient than calculating distances to all data points, especially for large datasets, as the number of support vectors is typically much smaller than the entire dataset.\n",
    "- In essence, the RBF kernel offers a powerful way to capture non-linearity in SVMs without explicitly transforming features. It relies on a single hyperparameter ($\\sigma$) to control the influence of nearby data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters In SVM\n",
    "Support Vector Machines (SVMs) are powerful algorithms for classification tasks. However, their performance heavily relies on tuning certain parameters called hyperparameters.\n",
    "\n",
    "### Kernel\n",
    "- Function: The kernel function defines how the data is transformed before applying the linear decision boundary.\n",
    "- Common choices:\n",
    "    - Linear: Suitable for linearly separable data.\n",
    "    - Polynomial: Can handle non-linear data by transforming it into higher dimesions.\n",
    "    - RBF (Radial Basis Function): A popular choice for non-linear data, offering good flexibility.\n",
    "- Impact: Choosing the right kernel significantly affects the model's ability to learn complex relationships in the data.\n",
    "\n",
    "### Regularization parameter ($C$)\n",
    "- Function: Controls the trade-off between maximizing the margin and allowing for misclassifications.\n",
    "- Higher $C$: Enforces a larger margin, potentially leading to stricter classification but might be less robust to noise or outliers.\n",
    "- Lower $C$: Allows for more misclassification but might result in smaller margin and potentially lower performance on unseen data.\n",
    "- Impact: Tuning C is crucial to achieve a balance between accuracy and generalization.\n",
    "\n",
    "### Gamma (for RBF kernel)\n",
    "- Function: Controls the influence of a single data point on the overall decision boundary (only relevant for the RBF kernel).\n",
    "- Higher Gamma: Leads to a more complex decision boundary, potentially fitting the training data well but at the risk of overfitting.\n",
    "- Lower Gamma: Results in a smoother decision boundary, potentially leading to better generalization but might underfit complex data.\n",
    "- Impact: Finding the right gamma value helps the model capture the appropriate level of detail in the data.\n",
    "\n",
    "### Epsilon ($epsilon$)\n",
    "- Function: Introducted in Soft Margin SVMs, it defines the tolerance for misclassifications.\n",
    "- Higher Epsilon: Allows for more errors, potentially useful for noisy data.\n",
    "- Lower Epsilon: Less tolerant of errors, aiming for stricter classification.\n",
    "- Impact: Epsilon helps control the flexibility of the model and the number of misclassifications allowed during training.\n",
    "\n",
    "### Tuning hyperparameters\n",
    "There's no single \"best\" value for these hyperparameters. Finding the optimal combination often involves techniques like grid search or random search, which involve trying different combinations and evaluating their performance on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics To Evaluate SVM\n",
    "The choice of metrics to evaluate an SVM model depends on the specific problem that the model is being used to address, and the characteristics of the data.\n",
    "\n",
    "### Accuracy\n",
    "- This is the most basic metric, representing the proportion of correct predictions made by the model. It's calculated as the number of correctly classified samples divided by the total number of samples.\n",
    "- Advantage: Easy to understand and interpret.\n",
    "- Disadvantage: Can be misleading in imbalanced datasets where the majority class might dominate.\n",
    "\n",
    "### Precision\n",
    "- Precision measures the proportion of positive predictions that are actually correct. It tells how good the model is at identifying actual positives from the data it predicts as positive.\n",
    "- Advantage: Useful when dealing with imbalanced datasets and the cost of false positives is high.\n",
    "- Disadvantage: Does not consider the number of true negatives, which can be important depending on the problem.\n",
    "\n",
    "### Recall\n",
    "- Recall measures the proportion of actual positive cases that are correctly identified by the model. It tells how good the model is at finding all the relevant positive cases.\n",
    "- Advantage: Useful when the cost of missing true positive is high (e.g., missing fraudulent transactions).\n",
    "- Disadvantage: Does not consider the number of false positives, which can be a concern for some applications.\n",
    "\n",
    "### F1-Score\n",
    "- The F1 score is the harmonic mean of precision and recall. It provides a balance between the 2 metrics, giving a single score that considers both aspects.\n",
    "- Advantage: Useful for summarizing overall performance when both precision and recall are important.\n",
    "- Disadvantage: Can be lower than both precision and recall if they are significantly different.\n",
    "\n",
    "### ROC AUC\n",
    "- ROC curve plots the True Positive Rate (TPR) and the False Positive Rate (FPR) at different classification thresholds. AUC represents the total area under this curve.\n",
    "- Advantage: ROC AUC is a good measure of model performance that is independent of class imbalance and classification thresholds. It considers all possible classification thresholds.\n",
    "- Disadvantage: Does not directly provide insights into precision or recall at a specific threshold.\n",
    "\n",
    "### Confusion Matrix\n",
    "- This is a table that shows the number of correct and incorrect predictions made by the model for each class.\n",
    "- Advantage: Provides a detailed breakdown of the model's performance on each class, including true positives, true negatives, false positives, and false negatives.\n",
    "- Disadvantage: Can be overwhelming for datasets with many classes.\n",
    "\n",
    "### Choosing the right metric\n",
    "The best metric for the SVM model depends on the specific context. Consider,\n",
    "- Data balance: If the dataset is imbalanced, accuracy might not be a good measure. Consider precision, recall, or F1-score.\n",
    "- Cost of errors: If the cost of misclassifying a particular class is high, prioritize metrics like precision (for reducing flse positives) or recall (for reducing false negatives) depending on the costlier error type.\n",
    "- Overall performance: If a single metric that summarizes the overall performance is needed, then F1 score can be a good choice.\n",
    "- Detailed analysis: Use the confusion matrix for a deeper understanding of the model's strengths and weaknesses for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-Off In SVM\n",
    "SVMs aim to find a hyperplane (decision boundary) that minimizes the margin between the two classes in the training data. However, there is a trade-off between,\n",
    "- Low bias: This is achieved by using a complex model (e.g., a non-linear kernel) that can fit the training data more closely. However, this can also lead to high variance if the model becomes too sensitive to the specific data points.\n",
    "- Low variance: This is achieved by using a simpler model (e.g., a linear kernel) that might not capture all the complexities of the data. This reduces the risk of overfitting but can lead to higher bias if the model is too simple to represent the true relationship between features and target variables.\n",
    "\n",
    "### Controlling the trade-off in SVMs\n",
    "The choice of kernel function plays a significant role,\n",
    "- Linear kernel: Leads to a simpler model with lower varince but might have higher bias for non-linear data.\n",
    "- Non-linear kernel (e.g., RBF): Allows for more complex decision boundaries, potentially reducing bias for non-linear data, but can also increase variance if not regularized properly.\n",
    "- Regularization parameter ($C$): This parameter controls the trade-off between maximizing the margin and allowing for misclassifications in the training data.\n",
    "- Higher $C$: Enforces a larger margin, potentially leading to a more complex model and lower bias but also potentially increasing variance.\n",
    "- Lower $C$: Allows for more misclassifications, resulting in a simpler model with lower variance but potentially higher bias.\n",
    "\n",
    "### Finding the optimal balance\n",
    "The goal is to find the sweet spot between bias and variace for your specific SVM model. Techniques like hyperparameter tuning with tools like `GridSearchCV` can help to identify the kernel function and regularization parameter ($C$) that minimizes the overall prediction error on unseen data.\n",
    "\n",
    "### Additional considerations\n",
    "- There's no single best model for all situation. The optimal balance between bias and variance depends on the characteristics of the data and the specific problem that is being addressed.\n",
    "- In some cases, even with careful tuning, SVMs might not be the best choice if the data is inherently very high-dimensional or complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting And Underfitting In SVM\n",
    "Overfitting and underfitting are common challenges faced when training SVMs.\n",
    "\n",
    "### Overfitting\n",
    "- Scenario: An SVM overfits when it learns the specific details and noise present in the training data too well. This can lead to a model that performs very well on the training data but fails to generalize well to unseen data.\n",
    "- Symptoms:\n",
    "    - High accuracy on the training set but significantly lower accuracy on the testing set.\n",
    "    - The decision boundary becomes too complex, trying to fit every training data point exactly, including noise.\n",
    "- Causes:\n",
    "    - High model complexity: Using a non-linear kernel without proper regularization can lead to a complex decision boundary that memorizes noise.\n",
    "    - Low regularization parameter ($C$): A low $C$ value allows for more misclassifications in the training data, potentially leading to a more flexible model that overfits.\n",
    "    - Limited training data: With a small dataset, the model might not have enough generalizable patterns to learn from, leading it to focus on specific details of the training data.\n",
    "\n",
    "### Underfitting\n",
    "- Scenario: An SVM underfits when it fails to capture the underlying relationships between features and the target variable in the training data. This results in a model with a high bias that performs poorly on both the training and testing sets.\n",
    "- Symptoms:\n",
    "    - Low accuracy on both the training and testing sets.\n",
    "    - The decision boundary is too simple, potentially a linear separation even for a non-linear data.\n",
    "- Causes:\n",
    "    - Low model complexity: Using a linear kernel for non-linear data might not be able to capture the true decision boundary, leading to underfitting.\n",
    "    - High regularization parameter ($C$): A high $C$ value enforces a larger margin, potentially leading to a simpler model that might underfit complex data.\n",
    "    - Irrelevant features: If the training data contains irrelevant features, the model might struggle to find the true relationships, leading to overfitting.\n",
    "\n",
    "### Mitigating overfitting and underfitting\n",
    "The following are some strategies to address these issues,\n",
    "- Hyperparameter tuning: Use techniques like `GridSearchCV` to find the optimal combination of kernel function and regularization parameter ($C$) that balances model complexity and generalization.\n",
    "- Data preprocesing: Techniques like normalization, scaling, and feature selection can improve the quality of the data and help the model learn more effectively.\n",
    "- Early stopping: Stop training the model when the validation accuracy starts to decrease, preventing it from memorizing noise in the training data.\n",
    "- Collect more data: If possible, increasing the size and diversity of the training data can help the model learn more generalizable patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect Of Outliers On SVM\n",
    "Support Vector Machines (SVMs) are powerful classification algorithms, but they can be sensitive to outliers in the data.\n",
    "\n",
    "### SVMs and the margin\n",
    "- SVMs aim to find a hyperplane (decision boundary) that maximizes the margin between the two classes.\n",
    "- The margin is the distance between the closest data points of each class, called the Support Vectors (SVs).\n",
    "\n",
    "### The problem with outliers\n",
    "- Outliers are data points that deviate significantly from the majority of the data.\n",
    "- If an outlier belongs to the majority class and lies close to the minority class, it can pull the decision boundary towards itself.\n",
    "- This can significantly reduce the margin for the minority class and lead to misclassifications.\n",
    "\n",
    "### How do outliers effect SVMs?\n",
    "- Reduced margin: Outliers can shrink the margin between classes, making it harder for the SVM to find a good separation.\n",
    "- Misclassifications: The shifted decision boundary due to outliers can lead to misclassifying actual data points, especially those near the original margin.\n",
    "- Increased training time: Outliers can make the optimization process of finding the optimal hyperplane more complex and time consuming.\n",
    "\n",
    "### Strategies to handle outliers\n",
    "- Data cleaning: If possible, identify and remove outliers that are genuine errors or noise in the data. However, it is important to excersice caution and not remove valid data points that might just be on the fringes of the distribution.\n",
    "- Capping or winsorizing: Instead of removing outliers entirely, their values can be capped to a certain threshold or use winsorizing, which replaces extreme values with values from the tails of the distribution.\n",
    "- Robust kernels: Some kernel functions used in SVMs, like Huber or Laplacian kernels are less sensitive to outliers compared to the standard linear kernel.\n",
    "- One-class SVMs: For anomaly detection tasks, you can use one-class SVMs that focus on modeling the normal data distribution and identifying outliers that deviate significantly.\n",
    "\n",
    "### Choosing the right approach\n",
    "The best approach to handle outliers depends on the nature of the data and the specific problem that is being addressed. Consider the trade-off between removing potentially valuable data and the impact of outliers on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect Of Data Imbalance On SVM\n",
    "Support Vector Machines (SVMs) are powerful classification algorithm, but their performance can be significantly affected by data imbalance.\n",
    "\n",
    "### The challenge\n",
    "- SVMs primarily focus on the support vectors, which are the data points closest to the decision boundary.\n",
    "- In a balanced dataset, these support vectors represent both classes fairly.\n",
    "\n",
    "### The problem with imbalance\n",
    "- When a dataset is imbalanced, the majority class has many more data points than the minority class.\n",
    "- This can lead to the SVM focusing primarily on the majority class during training,\n",
    "- The decision boundary might be biased towards the majority class, neglecting the minority class and potentially misclassifying them.\n",
    "\n",
    "### Consequences of imbalance\n",
    "- Reduced accuracy for minority class: The model might perform well on the majority class but poorly on the minority class, leading to a misleading overall accuracy score.\n",
    "- Unreliable predctions: Predictions for the minority class becomes less trustworthy due to the model's limited exposure to those data points.\n",
    "\n",
    "### Strategies to mitigate the effect\n",
    "- Data oversamplingL Duplicate data points from the minority class to create a more balanced dataset. Be careful of overfitting in this case.\n",
    "- Data undersampling: Randomly remove data points from the majority class to achieve balance. This reduces training data but can be computationally efficient.\n",
    "- Cost-sensitive SVM: Assign higher weights to misclassifications of minority class during training. The penalizes the model more for mistakes on the minority class.\n",
    "- Class weighting: Similar to cost-sensitive SVM, assign higher weights to the minority class during training.\n",
    "\n",
    "### Choosing the right approach\n",
    "The best approach depends on the specific dataset and the relative importance of accuracy for each class. Oversampling might not be ideal if the minority class data is already limited, while undersampling can discard potentially valuable information from the majority class.\n",
    "\n",
    "### Additional considerations\n",
    "- Evaluation metrics: Accuracy might not be the best metric for imbalanced dataset. Consider metrics like precision, recall, F1-score to assess performance for each class.\n",
    "- SMOTE (Synthetic Minority Oversampling Technique): This technique generates synthetic data points for the minority class to achieve balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time And Space Complexity Of SVM\n",
    "The time and space complexity of SVMs depend on several factors, including,\n",
    "- Training method: The most common training methods for SVMs are based on Quadratic Programming (QP) which dominates the overall complexity.\n",
    "- Dataset size: The number of data points ($n$) has a significant impact on both time and space complexity.\n",
    "- Number of features ($p$): While not as dominant as $n$, the number of features in the data can also affct complexity.\n",
    "\n",
    "### Time complexity\n",
    "- The time complexity of training an SVM using a QP solver is generally considered to be $O(n^3)$ in the worst case. This means the training time increases cubically with the number of data points.\n",
    "- However, in practice, the complexity can vary depending on the specific implementation and the characteristics of the data. Some algorithms might achieve slightly better complexity (e.g. close to $O(n^{2.5})$).\n",
    "\n",
    "### Space complexity\n",
    "The space complexity of training an SVM is typically $O(n^2)$. This is because the QP solver might need to store the entire kernel matrix, which scales quadratically with the number of data points.\n",
    "\n",
    "### Factors affecting complexity\n",
    "- Kernel choice: Kernel functions used in SVMs can also influence complexity. Linear kernels have lower complexity compared to non-linear kernels (e.g., RBF) which might involve additional computations.\n",
    "- Sparsity: If the data is sparse (meaning many features have zero values), the complexity might be lower due to reduced computations in the kernel matrix.\n",
    "\n",
    "### Additional notes\n",
    "- Prediction time: Predicting the class label for a new data point is generally faster than training. It typically involves a dot product operation with the support vectors and has a complexity of $O(sv * p)$, where $sv$ is the number of support vectors and $p$ is the number of features.\n",
    "- Scalability: Due to the cubic time complexity in the worst case, SVMs can become computationally expensive for very large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
