{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. It's known for its effectiveness in handling high dimensional data and its ability to perform well even with limited training data.\n",
    "\n",
    "### Classification with SVMs\n",
    "- The core idea is to find an optimal hyperplane that separates the data points of different classes with the maximum margin.\n",
    "- A hyperplane is a decision boundary in n-dimensional space (n = number of features).\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class called the support vectors.\n",
    "- SVMs aim to maximize this margin, which intuitively leads to a better separation between classes and potentially a better generalization to unseen data.\n",
    "\n",
    "### Key components\n",
    "- Support vectors: These are the data points closest to the hyperplane that define the margin. They are crucial for training the model and influence the classification of new data points.\n",
    "- Kernel trick: This technique allows SVMs to handle non-linearly separable data. It essentially transforms the data into a higher-dimensional space where a linear separation might be possible. Common kernels include, linear, polynomial, and radial basis function (RBF).\n",
    "\n",
    "### Advantages of SVMs\n",
    "- Effective in high-dimensional spaces: SVMs can perform well even with a large number of features, making them suitable for complex datasets.\n",
    "- Robust to overfitting: The focus on maximizing the margin can help reduce overfitting, especially when dealing with limited training data.\n",
    "- Interpretability: In some cases, the decision boundary learned by the SVM can be visualized and interpreted, providing insights into the model's behavior.\n",
    "\n",
    "### Disadvantages of SVMs\n",
    "- Can be computationally expensive: Training SVMs can be slower than some other algorithms, especially for large datasets.\n",
    "- Parameter tuning: Choosing the right kernel and its hyperparameters is crucial for optimal performance and can involve experimentation.\n",
    "- Not ideal for very high-dimensional data: While SVMs can handle high dimensions, extremely high dimensionality can still pose challenges.\n",
    "\n",
    "### Applications of SVMs\n",
    "- Text classification (spam detection, sentiment analysis).\n",
    "- Image classification (object detection, handwriting recognition).\n",
    "- Bioinformatic data analysis (gene expression analysis).\n",
    "- Anomaly detection (fraud detection, system intrusion detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah nt think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     0  U dun say so early hor... U c already then say...   \n",
       "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4          nah nt think goes usf lives around though  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_processed.csv\", encoding = \"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Algorithm\n",
    "### 1. Data representation\n",
    "- Each data point is represented as a vector of features ($x_i$) with a corresponding class label ($y_i$).\n",
    "- For example, if classifying emails as spam or ham, features might include frequencies, and class labels would be 1 (spam) or 0 (ham).\n",
    "\n",
    "### 2. Hyperplane\n",
    "- The goal is to find a hyperplane (a decision boundary) in the feature space that separates the data points of different classes with the maximum margin.\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class, called support vectors.\n",
    "\n",
    "### 3. Support vectors\n",
    "- These are the most critical training instances that define the margin.\n",
    "- They are typically the data points closest to the hyperplane on either side, one for each class.\n",
    "- The intuition is that these points have the most influence on the classification of new data points.\n",
    "\n",
    "### 4. Maximizing the margin\n",
    "- The SVM algorithm aims to maximize the margin between the hyperplane and the support vectors.\n",
    "- A larger margin intuitively leads to a better separation between classes and potentially better generalization to unseen data.\n",
    "\n",
    "### 5. Kernel trick (for non-linear data)\n",
    "- In some cases, the data might not be linearly separable in the original feature space.\n",
    "- The kernel trick addresses this by transforming the data into a higher-dimensional space where a linear separation might be possible.\n",
    "- Common kernel functions include,\n",
    "    - Linear kernel (for already linearly separable data).\n",
    "    - Polynomial kernel (transforms data to a higher-dimensional polynomial space).\n",
    "    - Radial Basis Function (RBF) kernel (projects data into a high-dimensional space using a Radial Basis Function).\n",
    "\n",
    "### 6. Classification of new data points\n",
    "Once the SVM is trained (hyperplane and support vectors identified), a new data point is classified by,\n",
    "- Transforming the data points into the same feature space as the training data (if using a kernel).\n",
    "- Calculating the distaance from the new point to the hyperplane.\n",
    "- Assigning the class label based on which side of the hyperplane the new point falls on.\n",
    "\n",
    "### Mathematical formulation (simplified)\n",
    "The decision for an SVM with a linear kernel can be expressed as, $f(x) = w^T * x + w_0$. Where,\n",
    "- $w$ = Weight vector (normal to the hyperplane).\n",
    "- $w_0$ = Bias term.\n",
    "- $x$ = New data point.\n",
    "\n",
    "The goal is to find $w$ and $w_0$ that maximize the margin while correctly classifying all training points. This involves solving a constrained optimization proble,\n",
    "\n",
    "### Learning algorithms\n",
    "Several algorithms are used to train SVMs, including, Sequential Minimal Optimization (SMO), a popular algorithm that efficiently solves the optimization problem for finding the optimal hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Margin Classifier\n",
    "### Problem\n",
    "The task is to classify data points (positive - green, negative - red) into separate classes using a hyperplace (decision boundary).\n",
    "\n",
    "### Challenge\n",
    "Choosing the \"best\" hyperplane among many possible ones.\n",
    "\n",
    "### Traditional approach\n",
    "Previously, algorithms might have considered all data points and evaluated different hyperplanes based on some criteria.\n",
    "\n",
    "### Maximum Margin Classifier (MMC)\n",
    "This approach focuses on maximizing the margin between the hyperplane and the closest data points (support vectors) from each class.\n",
    "\n",
    "### Key points\n",
    "1. Margin: The distance between the hyperplane and the closest data points (one from each class) on either side.\n",
    "2. Support vectors: These are the data points closest to the hyperplane and define the margin.\n",
    "3. Intuition: A larger margin intuitively leads to better separation between classes and potentially better generalization to unseen data.\n",
    "\n",
    "### Why support vectors?\n",
    "Instead of considering all data points, MMC only focuses on support vectors because,\n",
    "- They have the most influence on the classification of new data points.\n",
    "- Maximizing the margin around them ensures a good separation between classes for most other points as well.\n",
    "\n",
    "### Finding the best hyperplane\n",
    "1. Consider 2 hyperplanes ($\\pi_1$ and $\\pi_2$) and their corresponding support vectors.\n",
    "2. Create parallel lines ($\\pi+$ and $\\pi-$) on either side of each hyperplane, touching the support vectors.\n",
    "3. Calculate the distance between these parallel lines ($d_1$ and $d_2$) for each hyperplane.\n",
    "4. The hyperplane with the larger distance (larger margin) between its parallel lines is considered better (higher $d$ is better).\n",
    "\n",
    "### Formalization and optimization\n",
    "- The equation of a hyperplane can be expressed as, $w^T * x + w_0 = 0$. Where, $w$ = weight vector, and $w_0$ = bias term.\n",
    "- The margin has to be maximized, which can be classified as $\\frac{2 * k}{||w||}$. Where, k = distance between the hyperplane and its parallel lines.\n",
    "- However, the objective founction becomes, $\\argmin_{w, w_0} \\frac{||w||}{2}$ due to the inverse relationship.\n",
    "\n",
    "### Why not just minimize $||w||$?\n",
    "Minimizing $||w||$ alone does not gurantee a good separation. The hyperplane could collapse onto the support vectors, leading to poor performance on unseen data.\n",
    "\n",
    "### Constraints\n",
    "- To prevent the hyperplane from collapsing and to ensure a good separation, additional constraints are required (introduced in Hard Margin SVM, a specific MMC implementation).\n",
    "- These constraints typically penalize misclassifications of support vectors.\n",
    "\n",
    "### Summary\n",
    "The Maximum Margin Classifier focuses on maximizing the margin between the hyperplane and the closest data points (support vectors) to achieve good separation between classes and potentially better generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Margin Classifier\n",
    "### Labeling in SVM\n",
    "SVM uses labels +1 and -1 for positive and negative classes respectively. This is just a convention and doesn't affect the underlying concepts.\n",
    "\n",
    "### Constraints for hard margin\n",
    "- A hard margin classifier enforces stricter constraints compared to Maximum Margin Classifier (MMC).\n",
    "- It requires no points to be misclassified and lie between the hyperplane and the support vectors.\n",
    "\n",
    "### Formalizing the constraints\n",
    "- Two hyperplanes are defines, $\\pi+$ (above the decision boundary) and $\\pi-$ (below the decision boundary).\n",
    "- Every green point should satisfy $(w^T . x_i) * y_i >= 1$ (where $y_i$ is +1 for green points). This ensures they lie above $\\pi+$.\n",
    "- Every red point should satisfy $(w^T . x_i) * y_i <= 1$ (where $y_i$ is -1 for red points). This ensures they lie below $\\pi-$.\n",
    "\n",
    "### Functional margin\n",
    "- The functional margin is defined as the margin multiplied by the label ($y_i$).\n",
    "- This ensures both green points (positive label) and red points (negative label) have a positive functional margin.\n",
    "\n",
    "### Verifying correct classification\n",
    "- To check for misclassification, a point is multiplied with its label and is plugged into the equation.\n",
    "- A correctly classified point (either green or red) will always result in a positive value after this multiplication.\n",
    "\n",
    "### Challenges of hard margin\n",
    "- The strict constraint of no misclassification can be difficuly to achieve in practive, especially for datasets that are not perfectly linearly separable.\n",
    "- If the data is not linearly separable, a Hard Margin Classifier might not be able to find a valid hyperplane that satisfies the constraint for all points.\n",
    "\n",
    "### Why \"hard\" margin?\n",
    "- This approach is called as Hard Margin Classifier because it enforces a strict \"all or nothing\" rule. There is no room for even a single misclassification.\n",
    "- This makes it ideal only for scenarios with perfectly linearly separable data points, which can be rare in real-world datasets.\n",
    "\n",
    "### Limitations and Soft Margin Classifiers\n",
    "- Due to its rigid constraints, Hard Margin Classifiers can be impractical for real-world problems.\n",
    "- Soft Margin Classifiers addresses this by allowing a certain degree of misclassification. This allows them to handle non-lineraly separable data while still aiming for a good margin.\n",
    "\n",
    "### Summary\n",
    "Hard Margin Classifier is a powerful concept but has limitations due to its strict requirement of perfect separation. While it serves as a theoretical foundation for understanding maximum margin classification, Soft Margin Classifiers offer a more practical solution for mst real-world tasks where data might not be perfectly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Margin Classifier\n",
    "### Limitations of Hard Margin Classifiers\n",
    "- Hard Margin Classifiers enforce a strict constraints of no misclassifications.\n",
    "- This can be unrealistic for real-world datasets that might not be perfectly linearly separable.\n",
    "\n",
    "### Soft margins\n",
    "- Soft Margin Classifiers address this limitation by allowing a certain degree of misclassification.\n",
    "- This allows for more flexibility in handling non-ideal data while still aiming for a good separation between classes.\n",
    "\n",
    "### Error calculation\n",
    "- In soft margin classification, an error term ($E_i$) is defined for each data point.\n",
    "- This error represents the deviation of a point from its ideal distance to the margin (typically 1 unit).\n",
    "- This error is calculated as, $E_i = 1 - Z_i$ , where $Z_i$ is the output of $(w^T . x_i) + w_0$ for the point.\n",
    "\n",
    "### Constraint relaxation\n",
    "- Unlike Hard Margin Classifiers where, $Z_i * y_i >= 1$ must always hold, Soft Margin Classifiers introduce a slack variable ($\\xi_i$) (pronounced, xi).\n",
    "- The constraint becomes $Z_i * y_i >= 1 - E_i$ (with $\\xi_i >= 0$). This allows points to violate the margin by a certain amount ($E_i$) as long as the slack variable ($\\xi_i$) is non-negative.\n",
    "\n",
    "### Loss function with soft margin\n",
    "The objective function in Soft Margin Classifiers aims to minimize two things,\n",
    "- The norm of the weight vector (w) - similar to Hard Margins (encourages a large margin).\n",
    "- The total error caused by misclassifications (sum of $E_i$).\n",
    "\n",
    "### Ignoring irrelevant errors\n",
    "- The points that are correctly classfied are not penalized even if they fall within the margin ($E_i < 0$).\n",
    "- The slack variable ($\\xi_i$) ensures a loss of zero for such points (since $\\xi_i$ is non-negative).\n",
    "\n",
    "### Trade-off parameter ($C$)\n",
    "- The degree to which misclassifications are tolerated is controlled by a hyperparameter called $C$.\n",
    "- A higher $C$ emphasizes a larger margin, potentially leading to stricter classification but might be less robust to noise.\n",
    "- A lower $C$ allows for more misclassifications but might result in a smaller margin and potentially lower performance on unseen data.\n",
    "\n",
    "### Summary\n",
    "Soft Margin Classifiers offer a more practical approach for real-world data by allowing some degree of misclassification. They introduce a trade-off between maximizing the margin and minimizing errors, controlled by the hyperparameter $C$. This flexibility makes them widely used in various machine learning tasks, particularly when dealing with non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition Of Hinge Loss\n",
    "### Scenario\n",
    "Imagine a line ($\\pi$) representing the decision boundary in an SVM with positive margins ($\\pi+$) and negative margins ($\\i-$). Say that there are 4 data points ($x_1$, $x_2$, $x_3$, $x_4$) labeled positive ($y_i = +1$).\n",
    "\n",
    "### Points and their classification\n",
    "- $x_1$: Lies above $\\pi+$ (correctly classified).\n",
    "- $x_2$: Lies on $\\pi+$ (correctly classified on the margin).\n",
    "- $x_3$: Lies on $\\pi$ (correctly classifies on the decision boundary).\n",
    "- $x_4$: Lies on $\\pi-$ (misclassified on the negative margin).\n",
    "\n",
    "### Hinge loss and errors\n",
    "- Hinge loss penalizes points that fall within the margin or are misclassified. The further a point deviates from the correct side of the margin, the higher the loss.\n",
    "- Errors ($E_i$) are calculated as, $E_i = 1 - Z_i$ , where $Z_i$ is the output of $(w^T . x_i) + w_0$ for the point.\n",
    "\n",
    "### Observations\n",
    "- $x_1$ is correctly classified ($Z_i > 1$), so its error is $E_i = 0$ and hinge loss is 0 (no penatly).\n",
    "- $x_2$ is also correctly classified ($Z_i = 1$), so its error is $E_i = 0$ and hinge loss is 0 (no penatly).\n",
    "- $x_3$ lies on the decision boundary ($Z_i = 0$), leading to $E_i = 1$ and a hinge loss of 1 (maximum penalty for points on the boundary).\n",
    "- $X_4$ is misclassified ($Z_i < 0$), resulting in positive error ($E_i$) and a hinge loss that increases as $Z_i$ becomes more negative.\n",
    "\n",
    "### Hinge loss function\n",
    "Based on the observations, the observation, the hinge loss function can be simplified as,\n",
    "- $\\text{Hinge Loss}(Z_i) = \\max(0, (1 - Z_i))$.\n",
    "- This function ensures no penalty for correctly classified points ($Z_i >= 1$) and increasing penalties for points deviating from the correct side of the margin.\n",
    "\n",
    "### Comparison with Logistic Regression\n",
    "- Logistic Regression uses Log-Loss, which assigns probabilities (0 to 1) to classes. It penalizes the model based on the difference between predicted and actual probabilities.\n",
    "- SVM uses hinge loss, which works with labels (-1, +1) and focuses on maximizing the margin. It penalizes points that violate the margin or are misclassified.\n",
    "\n",
    "### Summary\n",
    "Hinge loss is a core concept in SVMs. It encourages a larger margin by heavily penalizing points that fall within the margin or are misclassified. This focus on the margin helps SVMs achieve good separation between classes, especially for data that might not be perfectly linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primal And Dual Formulations Of SVM\n",
    "### Primal form of SVM\n",
    "- Equation: $\\min(w, b) \\frac{||w||}{2} + \\frac{C}{N} \\sum_{i = 1}^{n} E_i \\text{, such that } (w.T * x_{query} + b) * y_i >= 1 - E_i \\text{, for all } i = 1 to n$.\n",
    "- The target is to minimize a function that has 2 parts,\n",
    "    - Regularization term: $\\frac{||w||}{2}$ penalizes large values of the weight vector w. This helps to prevent overfitting by keeping the model complexity under control.\n",
    "    - Hinge loss term: $\\frac{C}{N} \\sum_{i = 1}^{n} E_i$ measures the total error made by the model on the training data. $E_i$ represents the slack variable for data points $i$, indicating the amount by which the model's prediction deviates from the true label for that point. $C$ is a regularization parameter that controls the trade-off between maximizing the margin and allowing for some misclassification.\n",
    "- The constraints: $(w^T * x_i + b) * y_i >= 1 - E_i$ ensures that for all data points $i$,\n",
    "    - The decision boundary separates correctly classified points by a margin of atleast 1.\n",
    "    - The slack variable ($E_i$) allows for some misclassification by permitting violations of the margin with a penalty in the loss term.\n",
    "- Solving the primal form can be computationally expensive, especially for large datasets. This is because it involves optimizing a non-smooth function (hinge loss) with respect to $w$ and $b$.\n",
    "\n",
    "### Dual form of SVM\n",
    "- Equation: $\\max(\\alpha_i) (\\sum_{i = 1}^{n}\\alpha_i) - (\\frac{1}{2} \\sum_{i = 1}^{n} \\sum_{j = 1}^{n}\\alpha_i * \\alpha_j * y_i * y_j * x_i^T . x_j), \\text{ such that } 0 <= α_i <= C, \\text{ and } \\sum_{i = 1}^{n} \\alpha_i * y_i = 0$.\n",
    "- The dual form of the SVM provides an alternative way to solve the same optimization problem. It utilizes a concept called Lagrange duality to introduce Lagrange multipliers ($\\alpha_i$) and convert the problem into maximizing a function with respect to these multipliers. This new formulation often leads to more efficient optimization process.\n",
    "- Breakdown of dual form:\n",
    "    - The dual form involves maximizing a function based on the Lagrange multipliers ($\\alpha_i$).\n",
    "    - The dual form avoids explicit calculations of $w$ and $w_0$ and instead focuses on the $\\alpha_i$ which are associated with each data point.\n",
    "    - The dual form also includes constraints on the $\\alpha_i$ to ensure they fall within a specific range and satisfy a specific condition.\n",
    "\n",
    "### Dual form and support vectors (SVs)\n",
    "The key connection between dual form and SVs lies in the Lagrange multipliers ($\\alpha_i$). These multipliers play a crucial role in identifying the most important data points for defining the decision boundary.\n",
    "- Non-zero $\\alpha_i$: Data points with $\\alpha_i$ greater than 0 are considered SVs. These are points that lie closest to the decision boundary or violate the margin in the primal form. They are the most informative points for the model and significantly influence the classification.\n",
    "- Zero $\\alpha_i$: Data point with $\\alpha_i$ equal to zero are irrelevant for the decision boundary in the dual form. They either lie far away from the margin or are correctly classified with a large margin. These points contribute minimally to the model's prediction.\n",
    "\n",
    "### Misclassification and the soft margin\n",
    "The primal form allows for some misclassifications through the slack variables $E_i$ and the $C$ parameter.This is known as Soft Margin SVM. By introducing these slack variables, the model can tolerate some errors on the training data in exchange for a potentially better generalization to unseen data. The $C$ parameter controls the severity of the penalty for misclassifications. A higher $C$ enforces a larger margin and reduces misclassification, while a lower $C$ allows for more misclassification on the training data but might lead to better generalization.\n",
    "\n",
    "### Prediction in the dual form\n",
    "For a new data point ($x_{query}$), the prediction is made using a weighted sum of the features of the support vectors ($x_i$). The weights are determined by the corresponding Lagrange multipliers ($\\alpha_i$) and the labels ($y_i$) of the support vectors. Only the support vectors contribute to the prediction since their $\\alpha_i$ values are non-zero.\n",
    "\n",
    "### Summary\n",
    "The dual form of SVM offers a computationally efficient way to solve the SVM optimization problem. It identifies a subset of data points (support vectors) that are most crucial for defining the decision boundary. By allowing for some misclassifications with the soft margin approach, the model can achieve better generalization on unseen data. In practice, during prediction, only the features and labels of the support vectors and the corresponding Lagrange multipliers are needed, making the model memory-efficient.\n",
    "\n",
    "### Additional notes\n",
    "The primal form is useful for interpreting the model's decision boundary. However, the dual form is often preferred for solving optimization problems, especially when dealing with large datasets or high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "The concept of kernels in SVMs hinges on the idea of replacing the dot product with a more general similarity measure.\n",
    "\n",
    "### Dot product and similarity\n",
    "The dot product of two vectors ($x_i^T * x_j$) captures their linear similarity. A higher dot product indicates greater alognment in the same direction.\n",
    "\n",
    "### Limitations of linear SVMs\n",
    "- Linear SVMs can only create linear decision boundaries, which are essentially straight lines or hyperplanes in higher dimensions.\n",
    "- Real-world data often exhibits non-linear relationships that cannot be separated by a single straight line.\n",
    "\n",
    "### Feature transformation for non-linearity\n",
    "- One approach to handle non-linear data is to manually transform the features into a higher-dimensional space where linear separation becomes possible.\n",
    "- For example, you could create new features based on polynomial combinations of the original features like, $x_1^2$, $x_1 * x_2$, etc.\n",
    "- However, this approach can be computationally expensive and requires careful selection of the right transformations.\n",
    "\n",
    "### The kernel trick\n",
    "- Kernels provide a way to implicitly perform feature transformation without explicitly calculating the new features.\n",
    "- The kernel function $K(x_i, x_j)$ takes two data points ($x_i$ and $x_j$) as input and outputs a similarity measure.\n",
    "- This similarity measure is equivalent to the dot product that would be calculated in the higher-dimensional space after the feature transformation.\n",
    "\n",
    "### Benefits of kernels\n",
    "- Kernels allow SVMs to learn non-linear decision boundaries without explicitly computing the high-dimensional feature space.\n",
    "- This can significantly improve the model's performance on non-linearly separable data.\n",
    "- Some kernels, like the Radial Basis Function (RBF) kernel, have fewer hyperparameters to tune compared to polynomial kernels.\n",
    "\n",
    "### Popular kernels\n",
    "- Polynomial kernel: This kernel raises the dot product of the original features to a power (e.g., ($(x_i^T . x_j + c)^m$)). It effectively creates new features based on polynomial combinations of the original features. The degree $m$ is a hyperparameter that needs to be tuned.\n",
    "- Radial Basis Function (RBF) kernel: This kernel uses a Gaussian function to measure similarity based on the distance between data points in the original space. The spread of the Gaussian is controlled by a hyperparameter $\\sigma$. Unlike the polynomial kernel, it does not require choosing a specific feature transformation dimension.\n",
    "\n",
    "### Trade-offs\n",
    "- Choosing the right kernel and its hyperparameters is crucial for optimal performance.\n",
    "- Kernels can increase computational cost during training compared to the linear SVM due to the kernel function evaluations.\n",
    "\n",
    "### Prediction with kernels\n",
    "- During prediction, the model only needs to compute the kernel function between the query point and the support vectors.\n",
    "- This can be more efficient than calculating distances in the original space (like KNN) for large datasets, especially when the number of support vectors is small.\n",
    "- In essence, kernels offer a powerful way to equip SVMs with the ability to learn non-linear decision boundaries while maintaining computational efficiency during prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Kernel Transformation\n",
    "Linear SVMs can only create linear decision boundaries, which are insufficient for handling non-linear data.\n",
    "\n",
    "### Manual feature transformation\n",
    "- One approach is to manually transform the original features into a higher-dimensional space where linear separation becomes possible.\n",
    "- This involves creating new features based on polynomial combinations of the original features e.g., ($x_1^2$), ($x_1 * x_2$), etc.\n",
    "\n",
    "### The polynomial kernel\n",
    "The polynomial kernel offers an alternative to manual feature transformation. It implicitly performs the transformation without explicitly calculating the new features.\n",
    "\n",
    "### How does it work?\n",
    "1. The kernel function: The polynomial kernel takes two data points ($x_i$ and $x_j$) as input and computes their similarity using the formula,\n",
    "    - $K(x_i, x_j) = ((x_i^T * x_j) + c )^m$.\n",
    "    - Here, $m$ is the degree of the polynomial (hyperparameter) and $c$ is the constant (often set to 1).\n",
    "2. Implicit transformation: Raising the dot product to a power creates features that capture interactions between the original features. For example, a quadratic kernel ($m$ = 2) considers features like, ($x_1^2$), ($x_2^2$) and ($x_1 * x_2$).\n",
    "3. Equivalent to higher-dimensional dot product: The kernl function essentially calculates the dot product between the data points in the higher-dimensional feature space created by the polynomial transformation.\n",
    "\n",
    "### Example with quadratic kernel\n",
    "Consider 2 data points $x_1$ and $x_2$ with 2 features each. Applying the quadratic kernel,\n",
    "- $K(x_1, x_2) = (1 + (x_{11} * x_{21}) + (x_{12} * x_{22}))^2$.\n",
    "- This expands to a combination of terms representing interations between original features, $1 + (x_{11}^2 + x_{12}^2) + (x_{21}^2 + x_{22}^2) + (2 * x_{11} * x_{21}) + (2 * x_{11} * x_{21} * x_{12} * x_{22})$.\n",
    "\n",
    "### Benefits\n",
    "Polynomial kernels allow SVMs to learn non-linear decision boundaries without explicitly computing the high-dimensional space.\n",
    "\n",
    "### Drawbacks\n",
    "- Choosing the degree $m$ is crucial. A high degree can lead to overfitting due to the explosion of the features.\n",
    "- The computational cost can increase with the degree of the polynomial.\n",
    "\n",
    "### Conclusion\n",
    "The polynomial kernel is a powerful tool for handling non-linear data in SVMs. However, careful selection of the degree and awareness of potential overfitting are essential for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basis Function (RBF)\n",
    "Polynomial kernels offer a way to capture non-linearity, but they require choosing the degree of the polynomial, which can be challenging. The RBF kernel provides an alternative.\n",
    "\n",
    "### The RBF kernel function\n",
    "The RBF kernel measures similarity between data points using a Gaussian function,\n",
    "- $k_{RBF}(x1, x2) = e^{\\frac{-||x1 - x2||^2}{2*(σ^2}}$.\n",
    "- Here, $||x_1 - x_2||$ is the Euclidean distance between the data poins, and $\\sigma$ is a hyperparameter that controls the spread of the Gaussian.\n",
    "\n",
    "### Similarity based on distance\n",
    "- The RBF kernel assigns higher similarity scores to closer data points (smaller distance) due to the higher value of the Gaussian function.\n",
    "- Conversely, points farther apart receive lower similarity scores.\n",
    "- Points very close together will have a similarity score close to 1.\n",
    "\n",
    "### Benefits of RBF kernel\n",
    "- Unlike polynomial kernels, RBF kernels don't require defining a specific feature soace dimension. This eliminates the need to choose a polynomial degree.\n",
    "- Compared to polynomial kernels with their degree parameter, RBF kernels often require tuning only one hyperparameter ($\\sigma$).\n",
    "\n",
    "### Understanding the $\\sigma$ parameter\n",
    "- A smaller $\\sigma$ value creates a steeper Gaussian curve, making similarity highly dependent on being very close in the original space.\n",
    "- A larger $\\sigma$ value leads to a smoother, flatter Gaussian curve, where points farther apart can still have some degree of similarity.\n",
    "\n",
    "### Connection to KNN\n",
    "The concept behind the $\\sigma$ parameter in the RBF kernel is similar to the k parameter in K-Nearest Neighbors (KNN). Both influence how localized the decision boundary is based on the nearest neighbors.\n",
    "\n",
    "### RBF kernel and prediction efficiency\n",
    "- During prediction, SVMs with RBF kernels only need to compute the kernel function for the query point and the support vectors.\n",
    "- This can be more efficient than calculating distances to all data points, especially for large datasets, as the number of support vectors is typically much smaller than the entire dataset.\n",
    "- In essence, the RBF kernel offers a powerful way to capture non-linearity in SVMs without explicitly transforming features. It relies on a single hyperparameter ($\\sigma$) to control the influence of nearby data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters In SVM\n",
    "Support Vector Machines (SVMs) are powerful algorithms for classification tasks. However, their performance heavily relies on tuning certain parameters called hyperparameters.\n",
    "\n",
    "### Kernel\n",
    "- Function: The kernel function defines how the data is transformed before applying the linear decision boundary.\n",
    "- Common choices:\n",
    "    - Linear: Suitable for linearly separable data.\n",
    "    - Polynomial: Can handle non-linear data by transforming it into higher dimesions.\n",
    "    - RBF (Radial Basis Function): A popular choice for non-linear data, offering good flexibility.\n",
    "- Impact: Choosing the right kernel significantly affects the model's ability to learn complex relationships in the data.\n",
    "\n",
    "### Regularization parameter ($C$)\n",
    "- Function: Controls the trade-off between maximizing the margin and allowing for misclassifications.\n",
    "- Higher $C$: Enforces a larger margin, potentially leading to stricter classification but might be less robust to noise or outliers.\n",
    "- Lower $C$: Allows for more misclassification but might result in smaller margin and potentially lower performance on unseen data.\n",
    "- Impact: Tuning C is crucial to achieve a balance between accuracy and generalization.\n",
    "\n",
    "### Gamma (for RBF kernel)\n",
    "- Function: Controls the influence of a single data point on the overall decision boundary (only relevant for the RBF kernel).\n",
    "- Higher Gamma: Leads to a more complex decision boundary, potentially fitting the training data well but at the risk of overfitting.\n",
    "- Lower Gamma: Results in a smoother decision boundary, potentially leading to better generalization but might underfit complex data.\n",
    "- Impact: Finding the right gamma value helps the model capture the appropriate level of detail in the data.\n",
    "\n",
    "### Epsilon ($epsilon$)\n",
    "- Function: Introducted in Soft Margin SVMs, it defines the tolerance for misclassifications.\n",
    "- Higher Epsilon: Allows for more errors, potentially useful for noisy data.\n",
    "- Lower Epsilon: Less tolerant of errors, aiming for stricter classification.\n",
    "- Impact: Epsilon helps control the flexibility of the model and the number of misclassifications allowed during training.\n",
    "\n",
    "### Tuning hyperparameters\n",
    "There's no single \"best\" value for these hyperparameters. Finding the optimal combination often involves techniques like grid search or random search, which involve trying different combinations and evaluating their performance on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics To Evaluate SVM\n",
    "The choice of metrics to evaluate an SVM model depends on the specific problem that the model is being used to address, and the characteristics of the data.\n",
    "\n",
    "### Accuracy\n",
    "- This is the most basic metric, representing the proportion of correct predictions made by the model. It's calculated as the number of correctly classified samples divided by the total number of samples.\n",
    "- Advantage: Easy to understand and interpret.\n",
    "- Disadvantage: Can be misleading in imbalanced datasets where the majority class might dominate.\n",
    "\n",
    "### Precision\n",
    "- Precision measures the proportion of positive predictions that are actually correct. It tells how good the model is at identifying actual positives from the data it predicts as positive.\n",
    "- Advantage: Useful when dealing with imbalanced datasets and the cost of false positives is high.\n",
    "- Disadvantage: Does not consider the number of true negatives, which can be important depending on the problem.\n",
    "\n",
    "### Recall\n",
    "- Recall measures the proportion of actual positive cases that are correctly identified by the model. It tells how good the model is at finding all the relevant positive cases.\n",
    "- Advantage: Useful when the cost of missing true positive is high (e.g., missing fraudulent transactions).\n",
    "- Disadvantage: Does not consider the number of false positives, which can be a concern for some applications.\n",
    "\n",
    "### F1-Score\n",
    "- The F1 score is the harmonic mean of precision and recall. It provides a balance between the 2 metrics, giving a single score that considers both aspects.\n",
    "- Advantage: Useful for summarizing overall performance when both precision and recall are important.\n",
    "- Disadvantage: Can be lower than both precision and recall if they are significantly different.\n",
    "\n",
    "### ROC AUC\n",
    "- ROC curve plots the True Positive Rate (TPR) and the False Positive Rate (FPR) at different classification thresholds. AUC represents the total area under this curve.\n",
    "- Advantage: ROC AUC is a good measure of model performance that is independent of class imbalance and classification thresholds. It considers all possible classification thresholds.\n",
    "- Disadvantage: Does not directly provide insights into precision or recall at a specific threshold.\n",
    "\n",
    "### Confusion Matrix\n",
    "- This is a table that shows the number of correct and incorrect predictions made by the model for each class.\n",
    "- Advantage: Provides a detailed breakdown of the model's performance on each class, including true positives, true negatives, false positives, and false negatives.\n",
    "- Disadvantage: Can be overwhelming for datasets with many classes.\n",
    "\n",
    "### Choosing the right metric\n",
    "The best metric for the SVM model depends on the specific context. Consider,\n",
    "- Data balance: If the dataset is imbalanced, accuracy might not be a good measure. Consider precision, recall, or F1-score.\n",
    "- Cost of errors: If the cost of misclassifying a particular class is high, prioritize metrics like precision (for reducing flse positives) or recall (for reducing false negatives) depending on the costlier error type.\n",
    "- Overall performance: If a single metric that summarizes the overall performance is needed, then F1 score can be a good choice.\n",
    "- Detailed analysis: Use the confusion matrix for a deeper understanding of the model's strengths and weaknesses for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
