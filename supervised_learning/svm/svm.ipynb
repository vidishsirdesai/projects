{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. It's known for its effectiveness in handling high dimensional data and its ability to perform well even with limited training data.\n",
    "\n",
    "### Classification with SVMs\n",
    "- The core idea is to find an optimal hyperplane that separates the data points of different classes with the maximum margin.\n",
    "- A hyperplane is a decision boundary in n-dimensional space (n = number of features).\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class called the support vectors.\n",
    "- SVMs aim to maximize this margin, which intuitively leads to a better separation between classes and potentially a better generalization to unseen data.\n",
    "\n",
    "### Key components\n",
    "- Support vectors: These are the data points closest to the hyperplane that define the margin. They are crucial for training the model and influence the classification of new data points.\n",
    "- Kernel trick: This technique allows SVMs to handle non-linearly separable data. It essentially transforms the data into a higher-dimensional space where a linear separation might be possible. Common kernels include, linear, polynomial, and radial basis function (RBF).\n",
    "\n",
    "### Advantages of SVMs\n",
    "- Effective in high-dimensional spaces: SVMs can perform well even with a large number of features, making them suitable for complex datasets.\n",
    "- Robust to overfitting: The focus on maximizing the margin can help reduce overfitting, especially when dealing with limited training data.\n",
    "- Interpretability: In some cases, the decision boundary learned by the SVM can be visualized and interpreted, providing insights into the model's behavior.\n",
    "\n",
    "### Disadvantages of SVMs\n",
    "- Can be computationally expensive: Training SVMs can be slower than some other algorithms, especially for large datasets.\n",
    "- Parameter tuning: Choosing the right kernel and its hyperparameters is crucial for optimal performance and can involve experimentation.\n",
    "- Not ideal for very high-dimensional data: While SVMs can handle high dimensions, extremely high dimensionality can still pose challenges.\n",
    "\n",
    "### Applications of SVMs\n",
    "- Text classification (spam detection, sentiment analysis).\n",
    "- Image classification (object detection, handwriting recognition).\n",
    "- Bioinformatic data analysis (gene expression analysis).\n",
    "- Anomaly detection (fraud detection, system intrusion detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah nt think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     0  U dun say so early hor... U c already then say...   \n",
       "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4          nah nt think goes usf lives around though  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_processed.csv\", encoding = \"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Algorithm\n",
    "### 1. Data representation\n",
    "- Each data point is represented as a vector of features ($x_i$) with a corresponding class label ($y_i$).\n",
    "- For example, if classifying emails as spam or ham, features might include frequencies, and class labels would be 1 (spam) or 0 (ham).\n",
    "\n",
    "### 2. Hyperplane\n",
    "- The goal is to find a hyperplane (a decision boundary) in the feature space that separates the data points of different classes with the maximum margin.\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class, called support vectors.\n",
    "\n",
    "### 3. Support vectors\n",
    "- These are the most critical training instances that define the margin.\n",
    "- They are typically the data points closest to the hyperplane on either side, one for each class.\n",
    "- The intuition is that these points have the most influence on the classification of new data points.\n",
    "\n",
    "### 4. Maximizing the margin\n",
    "- The SVM algorithm aims to maximize the margin between the hyperplane and the support vectors.\n",
    "- A larger margin intuitively leads to a better separation between classes and potentially better generalization to unseen data.\n",
    "\n",
    "### 5. Kernel trick (for non-linear data)\n",
    "- In some cases, the data might not be linearly separable in the original feature space.\n",
    "- The kernel trick addresses this by transforming the data into a higher-dimensional space where a linear separation might be possible.\n",
    "- Common kernel functions include,\n",
    "    - Linear kernel (for already linearly separable data).\n",
    "    - Polynomial kernel (transforms data to a higher-dimensional polynomial space).\n",
    "    - Radial Basis Function (RBF) kernel (projects data into a high-dimensional space using a Radial Basis Function).\n",
    "\n",
    "### 6. Classification of new data points\n",
    "Once the SVM is trained (hyperplane and support vectors identified), a new data point is classified by,\n",
    "- Transforming the data points into the same feature space as the training data (if using a kernel).\n",
    "- Calculating the distaance from the new point to the hyperplane.\n",
    "- Assigning the class label based on which side of the hyperplane the new point falls on.\n",
    "\n",
    "### Mathematical formulation (simplified)\n",
    "The decision for an SVM with a linear kernel can be expressed as, $f(x) = w^T * x + w_0$. Where,\n",
    "- $w$ = Weight vector (normal to the hyperplane).\n",
    "- $w_0$ = Bias term.\n",
    "- $x$ = New data point.\n",
    "\n",
    "The goal is to find $w$ and $w_0$ that maximize the margin while correctly classifying all training points. This involves solving a constrained optimization proble,\n",
    "\n",
    "### Learning algorithms\n",
    "Several algorithms are used to train SVMs, including, Sequential Minimal Optimization (SMO), a popular algorithm that efficiently solves the optimization problem for finding the optimal hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Margin Classifier\n",
    "### Problem\n",
    "The task is to classify data points (positive - green, negative - red) into separate classes using a hyperplace (decision boundary).\n",
    "\n",
    "### Challenge\n",
    "Choosing the \"best\" hyperplane among many possible ones.\n",
    "\n",
    "### Traditional approach\n",
    "Previously, algorithms might have considered all data points and evaluated different hyperplanes based on some criteria.\n",
    "\n",
    "### Maximum Margin Classifier (MMC)\n",
    "This approach focuses on maximizing the margin between the hyperplane and the closest data points (support vectors) from each class.\n",
    "\n",
    "### Key points\n",
    "1. Margin: The distance between the hyperplane and the closest data points (one from each class) on either side.\n",
    "2. Support vectors: These are the data points closest to the hyperplane and define the margin.\n",
    "3. Intuition: A larger margin intuitively leads to better separation between classes and potentially better generalization to unseen data.\n",
    "\n",
    "### Why support vectors?\n",
    "Instead of considering all data points, MMC only focuses on support vectors because,\n",
    "- They have the most influence on the classification of new data points.\n",
    "- Maximizing the margin around them ensures a good separation between classes for most other points as well.\n",
    "\n",
    "### Finding the best hyperplane\n",
    "1. Consider 2 hyperplanes ($\\pi_1$ and $\\pi_2$) and their corresponding support vectors.\n",
    "2. Create parallel lines ($\\pi+$ and $\\pi-$) on either side of each hyperplane, touching the support vectors.\n",
    "3. Calculate the distance between these parallel lines ($d_1$ and $d_2$) for each hyperplane.\n",
    "4. The hyperplane with the larger distance (larger margin) between its parallel lines is considered better (higher $d$ is better).\n",
    "\n",
    "### Formalization and optimization\n",
    "- The equation of a hyperplane can be expressed as, $w^T * x + w_0 = 0$. Where, $w$ = weight vector, and $w_0$ = bias term.\n",
    "- The margin has to be maximized, which can be classified as $\\frac{2 * k}{||w||}$. Where, k = distance between the hyperplane and its parallel lines.\n",
    "- However, the objective founction becomes, $\\argmin_{w, w_0} \\frac{||w||}{2}$ due to the inverse relationship.\n",
    "\n",
    "### Why not just minimize $||w||$?\n",
    "Minimizing $||w||$ alone does not gurantee a good separation. The hyperplane could collapse onto the support vectors, leading to poor performance on unseen data.\n",
    "\n",
    "### Constraints\n",
    "- To prevent the hyperplane from collapsing and to ensure a good separation, additional constraints are required (introduced in Hard Margin SVM, a specific MMC implementation).\n",
    "- These constraints typically penalize misclassifications of support vectors.\n",
    "\n",
    "### Summary\n",
    "The Maximum Margin Classifier focuses on maximizing the margin between the hyperplane and the closest data points (support vectors) to achieve good separation between classes and potentially better generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Margin Classifier\n",
    "### Labeling in SVM\n",
    "SVM uses labels +1 and -1 for positive and negative classes respectively. This is just a convention and doesn't affect the underlying concepts.\n",
    "\n",
    "### Constraints for hard margin\n",
    "- A hard margin classifier enforces stricter constraints compared to Maximum Margin Classifier (MMC).\n",
    "- It requires no points to be misclassified and lie between the hyperplane and the support vectors.\n",
    "\n",
    "### Formalizing the constraints\n",
    "- Two hyperplanes are defines, $\\pi+$ (above the decision boundary) and $\\pi-$ (below the decision boundary).\n",
    "- Every green point should satisfy $(w^T . x_i) * y_i >= 1$ (where $y_i$ is +1 for green points). This ensures they lie above $\\pi+$.\n",
    "- Every red point should satisfy $(w^T . x_i) * y_i <= 1$ (where $y_i$ is -1 for red points). This ensures they lie below $\\pi-$.\n",
    "\n",
    "### Functional margin\n",
    "- The functional margin is defined as the margin multiplied by the label ($y_i$).\n",
    "- This ensures both green points (positive label) and red points (negative label) have a positive functional margin.\n",
    "\n",
    "### Verifying correct classification\n",
    "- To check for misclassification, a point is multiplied with its label and is plugged into the equation.\n",
    "- A correctly classified point (either green or red) will always result in a positive value after this multiplication.\n",
    "\n",
    "### Challenges of hard margin\n",
    "- The strict constraint of no misclassification can be difficuly to achieve in practive, especially for datasets that are not perfectly linearly separable.\n",
    "- If the data is not linearly separable, a Hard Margin Classifier might not be able to find a valid hyperplane that satisfies the constraint for all points.\n",
    "\n",
    "### Why \"hard\" margin?\n",
    "- This approach is called as Hard Margin Classifier because it enforces a strict \"all or nothing\" rule. There is no room for even a single misclassification.\n",
    "- This makes it ideal only for scenarios with perfectly linearly separable data points, which can be rare in real-world datasets.\n",
    "\n",
    "### Limitations and Soft Margin Classifiers\n",
    "- Due to its rigid constraints, Hard Margin Classifiers can be impractical for real-world problems.\n",
    "- Soft Margin Classifiers addresses this by allowing a certain degree of misclassification. This allows them to handle non-lineraly separable data while still aiming for a good margin.\n",
    "\n",
    "### Summary\n",
    "Hard Margin Classifier is a powerful concept but has limitations due to its strict requirement of perfect separation. While it serves as a theoretical foundation for understanding maximum margin classification, Soft Margin Classifiers offer a more practical solution for mst real-world tasks where data might not be perfectly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Margin Classifier\n",
    "### Limitations of Hard Margin Classifiers\n",
    "- Hard Margin Classifiers enforce a strict constraints of no misclassifications.\n",
    "- This can be unrealistic for real-world datasets that might not be perfectly linearly separable.\n",
    "\n",
    "### Soft margins\n",
    "- Soft Margin Classifiers address this limitation by allowing a certain degree of misclassification.\n",
    "- This allows for more flexibility in handling non-ideal data while still aiming for a good separation between classes.\n",
    "\n",
    "### Error calculation\n",
    "- In soft margin classification, an error term ($E_i$) is defined for each data point.\n",
    "- This error represents the deviation of a point from its ideal distance to the margin (typically 1 unit).\n",
    "- This error is calculated as, $E_i = 1 - Z_i$ , where $Z_i$ is the output of $(w^T . x_i) + w_0$ for the point.\n",
    "\n",
    "### Constraint relaxation\n",
    "- Unlike Hard Margin Classifiers where, $Z_i * y_i >= 1$ must always hold, Soft Margin Classifiers introduce a slack variable ($\\xi_i$) (pronounced, xi).\n",
    "- The constraint becomes $Z_i * y_i >= 1 - E_i$ (with $\\xi_i >= 0$). This allows points to violate the margin by a certain amount ($E_i$) as long as the slack variable ($\\xi_i$) is non-negative.\n",
    "\n",
    "### Loss function with soft margin\n",
    "The objective function in Soft Margin Classifiers aims to minimize two things,\n",
    "- The norm of the weight vector (w) - similar to Hard Margins (encourages a large margin).\n",
    "- The total error caused by misclassifications (sum of $E_i$).\n",
    "\n",
    "### Ignoring irrelevant errors\n",
    "- The points that are correctly classfied are not penalized even if they fall within the margin ($E_i < 0$).\n",
    "- The slack variable ($\\xi_i$) ensures a loss of zero for such points (since $\\xi_i$ is non-negative).\n",
    "\n",
    "### Trade-off parameter ($C$)\n",
    "- The degree to which misclassifications are tolerated is controlled by a hyperparameter called $C$.\n",
    "- A higher $C$ emphasizes a larger margin, potentially leading to stricter classification but might be less robust to noise.\n",
    "- A lower $C$ allows for more misclassifications but might result in a smaller margin and potentially lower performance on unseen data.\n",
    "\n",
    "### Summary\n",
    "Soft Margin Classifiers offer a more practical approach for real-world data by allowing some degree of misclassification. They introduce a trade-off between maximizing the margin and minimizing errors, controlled by the hyperparameter $C$. This flexibility makes them widely used in various machine learning tasks, particularly when dealing with non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
