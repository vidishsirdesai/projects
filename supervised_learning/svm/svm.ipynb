{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. It's known for its effectiveness in handling high dimensional data and its ability to perform well even with limited training data.\n",
    "\n",
    "### Classification with SVMs\n",
    "- The core idea is to find an optimal hyperplane that separates the data points of different classes with the maximum margin.\n",
    "- A hyperplane is a decision boundary in n-dimensional space (n = number of features).\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class called the support vectors.\n",
    "- SVMs aim to maximize this margin, which intuitively leads to a better separation between classes and potentially a better generalization to unseen data.\n",
    "\n",
    "### Key components\n",
    "- Support vectors: These are the data points closest to the hyperplane that define the margin. They are crucial for training the model and influence the classification of new data points.\n",
    "- Kernel trick: This technique allows SVMs to handle non-linearly separable data. It essentially transforms the data into a higher-dimensional space where a linear separation might be possible. Common kernels include, linear, polynomial, and radial basis function (RBF).\n",
    "\n",
    "### Advantages of SVMs\n",
    "- Effective in high-dimensional spaces: SVMs can perform well even with a large number of features, making them suitable for complex datasets.\n",
    "- Robust to overfitting: The focus on maximizing the margin can help reduce overfitting, especially when dealing with limited training data.\n",
    "- Interpretability: In some cases, the decision boundary learned by the SVM can be visualized and interpreted, providing insights into the model's behavior.\n",
    "\n",
    "### Disadvantages of SVMs\n",
    "- Can be computationally expensive: Training SVMs can be slower than some other algorithms, especially for large datasets.\n",
    "- Parameter tuning: Choosing the right kernel and its hyperparameters is crucial for optimal performance and can involve experimentation.\n",
    "- Not ideal for very high-dimensional data: While SVMs can handle high dimensions, extremely high dimensionality can still pose challenges.\n",
    "\n",
    "### Applications of SVMs\n",
    "- Text classification (spam detection, sentiment analysis).\n",
    "- Image classification (object detection, handwriting recognition).\n",
    "- Bioinformatic data analysis (gene expression analysis).\n",
    "- Anomaly detection (fraud detection, system intrusion detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah nt think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message  \\\n",
       "0     0  Go until jurong point, crazy.. Available only ...   \n",
       "1     0                      Ok lar... Joking wif u oni...   \n",
       "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3     0  U dun say so early hor... U c already then say...   \n",
       "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  go jurong point crazy available bugis n great ...  \n",
       "1                            ok lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4          nah nt think goes usf lives around though  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_processed.csv\", encoding = \"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Algorithm\n",
    "### 1. Data representation\n",
    "- Each data point is represented as a vector of features ($x_i$) with a corresponding class label ($y_i$).\n",
    "- For example, if classifying emails as spam or ham, features might include frequencies, and class labels would be 1 (spam) or 0 (ham).\n",
    "\n",
    "### 2. Hyperplane\n",
    "- The goal is to find a hyperplane (a decision boundary) in the feature space that separates the data points of different classes with the maximum margin.\n",
    "- The margin is the distance between the hyperplane and the closest data points from each class, called support vectors.\n",
    "\n",
    "### 3. Support vectors\n",
    "- These are the most critical training instances that define the margin.\n",
    "- They are typically the data points closest to the hyperplane on either side, one for each class.\n",
    "- The intuition is that these points have the most influence on the classification of new data points.\n",
    "\n",
    "### 4. Maximizing the margin\n",
    "- The SVM algorithm aims to maximize the margin between the hyperplane and the support vectors.\n",
    "- A larger margin intuitively leads to a better separation between classes and potentially better generalization to unseen data.\n",
    "\n",
    "### 5. Kernel trick (for non-linear data)\n",
    "- In some cases, the data might not be linearly separable in the original feature space.\n",
    "- The kernel trick addresses this by transforming the data into a higher-dimensional space where a linear separation might be possible.\n",
    "- Common kernel functions include,\n",
    "    - Linear kernel (for already linearly separable data).\n",
    "    - Polynomial kernel (transforms data to a higher-dimensional polynomial space).\n",
    "    - Radial Basis Function (RBF) kernel (projects data into a high-dimensional space using a Radial Basis Function).\n",
    "\n",
    "### 6. Classification of new data points\n",
    "Once the SVM is trained (hyperplane and support vectors identified), a new data point is classified by,\n",
    "- Transforming the data points into the same feature space as the training data (if using a kernel).\n",
    "- Calculating the distaance from the new point to the hyperplane.\n",
    "- Assigning the class label based on which side of the hyperplane the new point falls on.\n",
    "\n",
    "### Mathematical formulation (simplified)\n",
    "The decision for an SVM with a linear kernel can be expressed as, $f(x) = w^T * x + w_0$. Where,\n",
    "- $w$ = Weight vector (normal to the hyperplane).\n",
    "- $w_0$ = Bias term.\n",
    "- $x$ = New data point.\n",
    "\n",
    "The goal is to find $w$ and $w_0$ that maximize the margin while correctly classifying all training points. This involves solving a constrained optimization proble,\n",
    "\n",
    "### Learning algorithms\n",
    "Several algorithms are used to train SVMs, including, Sequential Minimal Optimization (SMO), a popular algorithm that efficiently solves the optimization problem for finding the optimal hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
