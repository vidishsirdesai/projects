{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "While dealing with a multi-class classification problem, logistic regression OvR can be applied only if the classes are linearly separable. hence polynomial regression should be used while dealing with a non-linear separation boundary, but the problem with this is that, feature engineering is difficult, as in, it is hard to guess which features are to be used for model building, what other relavant features can be created from existing features, etc.\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm widely used popularly for classification tasks, and regression tasks as well. It works by classifying a data point based on the labels of its nearest neighbors in the training data.\n",
    "\n",
    "KNN assumes that similar data points tend to have similar labels. To classify a new data point, KNN finds the K closest data points (neighbors) in the training set based on distance metric (Euclidean, Manhattan, Cosine, etc). The majority vote (for classification), or average value (for regression) of these K neighbors is used to predict the label or value of the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Algorithm\n",
    "1. Data preparation:\n",
    "    - The data is prepared by ensuring that it is in a suitable format for distance calculation (usually numerical features).\n",
    "    - Feature scaling might be necessary if features have different ranges.\n",
    "2. Distance calculation: A distance metric is chosen to calculate the distance between the new data point, and each data point in the training set.\n",
    "3. Find the nearest neighbors: Based on the calculated distances, the K closest data points (neighbors) to the new data point are identified. The value of K is a hyperparameter that needs to be tuned.\n",
    "4. Prediction:\n",
    "    - For classification: The most frequent class label among the K nearest neighbors is assigned to new data point (majority vote).\n",
    "    - For regression: The average value of the target variable for the K nearest neighbors is used to predict the value for the new data point.\n",
    "\n",
    "Alternatively,\n",
    "1. For an unknown data point $x_i$, the distance between it and every training data point in the dataset. The choice of the distance metric depends on the number of dimensions.\n",
    "2. After calculating the distances for all the training points, they are sorted in ascending order (lowest to highest) based on the distance from the new data point $x_i$. This ensures that the data points closest to $x_i$ are at the beginning of the sorted list.\n",
    "3. The value of K, a hyperparameter, determines the number of nearest neighbors to consider for prediction. The K hyperparameter is curcial, and can be tuned based on the specific problem, and dataset. After sorting, the top K data points are chosen as the nearest neighbors for the new data point.\n",
    "4. The value of K chosen should be such that it should not be a multiple of the number of classes. This is done to prevent a scenario where the unknown data point is at equidistant from 2 sets of data points. But in case where, after hyprparameter tuning, the best value IS a number that is a multiple of the number of classes, then in that case, either pick random between the classes, or pick the one that is the closest. Meaning, sum up the distances of all the points present in a particular class, do the same for all the classes. Now pick the class which has the lowest sum of distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling in Distance Bases Algorithms\n",
    "- For algorithms which work on distance, it is important to make sure that all the features are at the same scale.\n",
    "- This applies to any ML algorithm where distance is used. If scaling is not done correctly, the feature that is at a higher scale will become dominant and affect the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features in KNN\n",
    "- KNN works really well when features are of numerical data type. If features are something for which the distance cannot be calculated for, KNN cannot be used.\n",
    "- Categorical values, if present, are encoded. For example, it the features are $f_1$, $f_2$, $f_3$, and $f_4$, and $f_4$ is the blood group, and when OHE is performed over this, it will result in 8 more dimensions. Meaning there are now 11 dimensions in total in the dataset. KNN is dependent on the number of dimensions, and it becomes slower with increase in dimensions.\n",
    "- One way to solve this is to target encode, or for very high dimensional categorical values, choose another algorithm.\n",
    "- The methodology used to encode the categorical features can also be considered another type of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
