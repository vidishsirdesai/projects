{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Decision trees are a powerful, and popular supervised machine learning technique used for both classification, and regression tasks. They are widely used for interpretability, ease of implementation, and ability to handle various data types.\n",
    "\n",
    "A decision tree resembeles a flowchart like structure with 3 main components,\n",
    "1. Internal nodes: These represent questions, or test applied to a specific feature of the data.\n",
    "2. Branches: These represent the outcome of the test at an internal node, leading to different parts of the tree.\n",
    "3. Leaf nodes: These represent the final decision or prediction made by the tree.\n",
    "\n",
    "Advantages of decision trees,\n",
    "- Interpretability: Decision trees are easy to understand and interpret. The branches can easily be followed to understand the logic behind the predictions.\n",
    "- No feature scaling: Decision trees can handle both numerical and categorical features without the need for explicit feature scaling, which can be a challenge in other algorithms.\n",
    "- Robust to outliers: Decision trees are relatively robust to outliers in the data compared to some other models.\n",
    "- Can handle missing values: Techniques like splitting based on the presence or absence of a value can be used to handle missing data.\n",
    "\n",
    "Disadvantages of decision trees,\n",
    "- Prone to overfitting: If allowed to grow too deep, decision trees can become overly complex and start fitting the training data too closely, leading to poor performance on unseen data (overfitting). Techniques like pruning or setting a maximum depth can help mitigate this.\n",
    "- High variance: Decision trees can be sensitive to small changes in the data, leading to potentially high variance in the model's predictions. Techniques like bagging or random forests can help reduce variance.\n",
    "- Feature importance: While interpretable, understanding the exact contribution of each feature to the final decision can be challenging in complex trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm\n",
    "1. Start with the entire dataset at the root node.\n",
    "2. Choose the best splitting feature: This involves finding the feature that best separates the data into distinct groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
