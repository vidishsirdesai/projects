{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Naive Bayes is family of classification algorithms based on Bayes' theorem. It is a popular choice for various classification tasks due to its simplicity, efficiency, and interpretability.\n",
    "\n",
    "### Core principle\n",
    "Naive Bayes classifiers work under the assumption fo conditional independence between features (predictors) given the class label (target variable). In simpler terms, it assumes that knowing the value of one feature does not influence the probability of another feature's value, as long as the class label is already known. While this assumption is not always hold true in reality, it often works well in practive for many classification problems.\n",
    "\n",
    "### Classification process\n",
    "- Training: The model learns from the labeled dataset where each data point has features and a corresponding class label.\n",
    "- Prediction: For a new unseen data point, the model calculates the probability of it belonging to each class. It achieves this by,\n",
    "    1. Using Bayes' theorem to compute the posterior probability (probability of a class given the features).\n",
    "    2. Assuming conditional independence between features, which simplifies the calculations.\n",
    "    3. Multiplying the probabilities of each feature value given the class and multiplying by the prior probability of the class itself (learned from the training data).\n",
    "- Assigning class label: The class with the highest posterior probability is assigned as the predicted class for the new data point.\n",
    "\n",
    "### Example\n",
    "Imagine emails are being classified as spam or not spam based on features like наличиe слова \"деньги\" (presence of the word \"money\") and наличие восклицательных знаков (presence of exclamation marks). Naive Bayes would assume that the presence of \"money\" doesn't influence the presence of exclamation marks (and vice versa) given the email class (spam or not spam).\n",
    "\n",
    "### Advantages of Naive Bayes\n",
    "- Simplicity and efficiency: Naive Bayes is easy to understand and implement, making it a good choice for beginners. It's also computationally efficient for training and prediction.\n",
    "- Interpretability: The model allows to understand how each feature contributes to the classification by examining the feature probabilities for each class.\n",
    "- Performance: Naive Bayes can perform well for various classification tasks, especially when dealing with high-dimensional data (many features).\n",
    "\n",
    "### Disadvantage of Naive Bayes\n",
    "- Conditional independence assumption: The assumption of conditional independence between features might not always be valid, which can lead to suboptimal performance in some cases.\n",
    "- Sensitivity to features: Naive Bayes can be sensitive to irrelevant features or features with many unique values. Feature selection or preprocessing techniques might be necessary for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Bayes Theorem\n",
    "$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$\n",
    "\n",
    "Where,\n",
    "- $P(A|B)$ = Posterior. The probability of A being true, given B is true.\n",
    "- $P(B|A)$ = Likelihood. The probability of B being true, given A is true.\n",
    "- $P(A)$ = Prior. The probability of A being true. This is knowledge.\n",
    "- $P(B)$ = Marginalization. The probability of B being true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "### 1. Data preprocessing\n",
    "- Text cleaning:\n",
    "    - Remove punctuation, stop words (common words with little meaning), and potentially numbers depending on the task.\n",
    "    - Convert text to lowercase for consistency.\n",
    "    - Consider stemming or lemmatization (reducing words to base form) for improved accuracy (optional).\n",
    "- Feature representation: Represent each document (sentence) as a feature vector.\n",
    "- Common approaches:\n",
    "    - Bag-of-Words (BoW): Each word occurrence is a feature (0 or 1 indicating absence or presence).(Bernoulli NB).\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF): Weights words based on their importance within the document and rarity across the corpus. (Multinomial NB)\n",
    "\n",
    "### 2. Model training\n",
    "- Calculate class priors: Estimate the probability ($P(y = c)$) for each class ($c$) based on frequency in the training data.\n",
    "- Calculate conditional probabilities:\n",
    "    - Estimate the probability of each feature (word) appearing given a specific class ($P(w_i|y = c)$)\n",
    "    - Use techniques like Laplace Smoothing to avoid zero probabilities for unseen words (especially for multinomial NB).\n",
    "\n",
    "### 3. Classification of new sentence\n",
    "- Calculate posterior probability:\n",
    "    - Use the equation: $P(y = c | \\text{sentence}) ≈ \\Pi(P(w_i | y = c)) * P(y = c)$.\n",
    "    - Multiply the probabilities of each word ($w_i$) appearing in the sentence given its class ($c$).\n",
    "    - Multiply by the prior probability of class ($c$).\n",
    "- Class prediction: Assign the sentence to the class with the highest posterior probability ($P(y = c | sentence)$).\n",
    "\n",
    "### Key points\n",
    "- Naive Bayes assumes independence between words in a document given the class label (simplification).\n",
    "- This assumption might not always hold true but offers computational efficiency and can be surprisingly effective.\n",
    "- Multinomial NB can capture word frequency information but requires managing the feature space size.\n",
    "- Laplace Smoothing helps address zero probabilities and improves model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier With Naive Bayes, And Bag-of-Words\n",
    "### Objective\n",
    "Create a binary text classifier to distinguish spam email from legitimate emails (ham).\n",
    "\n",
    "### Challenges\n",
    "- Text data cannot be fed into ML models.\n",
    "- The text information has to be converted into features suitable for the model.\n",
    "\n",
    "### Solution\n",
    "- Feature extraction using Bag-of-Words: This technique represents documents as a collection of words, ignoring grammar and word order.\n",
    "    - All the unique words from the entire email dataset are extracted.\n",
    "    - Each email is the represented by a feature vector where each element indicated the frequency (count) of a particular word in that email.\n",
    "\n",
    "### Classification using Naive Bayes\n",
    "- Naive Bayes is well suited for text classification tasks.\n",
    "- It assumes the independence of features (words) in a document, which might not be strictly true but often works well in practice for text data.\n",
    "- The model calculates the probability of an email being spam or ham based on the presence and frequency of words associated with each category.\n",
    "\n",
    "### Example\n",
    "- Emails:\n",
    "    - \"Can you please look at the task ...?\" (ham)\n",
    "    - \"Hi! I am a Nigerian Prince\" (spam)\n",
    "- Extracted Bag-of-Words: \"Can\", \"you\", \"please\", \"look\", \"at\", \"the\", \"task\", \"...\", \"Hi\", \"!\", \"am\", \"a\", “Nigerian\", \"Prince\".\n",
    "- Feature vectors:\n",
    "    - Ham: (1 count of \"Can\", 1 count of \"you\", ..., 0 for \"Nigerian\", 0 for “Prince\").\n",
    "    - Spam: (0 for \"Can\", 0 for \"you\", ..., 1 count of “Nigerian\", count of \"Prince\").\n",
    "\n",
    "Naive Bayes uses these feature vectors and their corresponding labels (spam/ ham) to learn the probability distribution of words for each category. During prediction for a new email, the model calculates the probabilities of the email being spam and ham based on the word frequencies and classifies it accordingly.\n",
    "\n",
    "### Summary\n",
    "- Bag-of-Words transforms text data into numerical features for analysis.\n",
    "- Naive Bayes leverages these features to classify emails as spam or ham based on word probabilities learned from the training data. This approach provides a simple and effective way to build a spam classifier.\n",
    "\n",
    "### Note\n",
    "Real world spam classification can be more complex and might involve additional techniques like stemming or lemmatization (reducing words to their root form), n-grams (considering sequences of words), feature weighting based on importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Bag-of-Words (BoW)\n",
    "Bag-of-Words (BoW) is a fundamental technique used in Natural Language Processing (NLP) for representing text data. It focuses on the occurrences of words within a document, ignoring grammar or word order.\n",
    "\n",
    "### Core idea\n",
    "- Imagine a bag filled with words, where each word appears as many times as it occurs in the document.\n",
    "- The order of context in which the words appear is not considered.\n",
    "\n",
    "### Creating a Bag-of-Words representation\n",
    "1. Preprocessing: Text cleaning steps like removing punctuation, stop words (that is, common words like \"a\", \"an\", \"the\"), and converting text to lowercase are often performed.\n",
    "2. Tokenization: The text is split into individual words (tokens).\n",
    "3. Vocabulary creation: A list of unique words encountered across all documents in the corpus (collection of documents) is created. This is called the vocabulary.\n",
    "4. Feature vector representation: Each document is represented by a feature vector. This vector has the same length as the vocabulary.\n",
    "\n",
    "- Each element in the vector corresponds to a word in the vocabulary.\n",
    "- The value at each element represents the number of times that particular word appears in the document (its frequency)/\n",
    "\n",
    "### Example\n",
    "Consider 2 documents,\n",
    "- Document 1: \"The quick brown fox jumps over the laxy dog.\".\n",
    "- Document 2: \"The dog is lazy. The fox is quick.\".\n",
    "\n",
    "After preprocessing and tokenization, the result would be,\n",
    "- Vocabulary: [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"is\"].\n",
    "\n",
    "The feature vectors for these documents could be,\n",
    "- Document 1: [2, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "- Document 2: [2, 1, 0, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "### Applications\n",
    "Bag-of-Words is a simple and effective way to represent text data for various NLP tasks, including,\n",
    "- Document classification (e.g., spam detection, sentiment analysis).\n",
    "- Information retrieval (e.g., document search).\n",
    "- Topic modeling (identifying groups of related words).\n",
    "\n",
    "### Limitations\n",
    "- BoW ignores word order and context, which can be crucial for understanding the meaning of a sentence.\n",
    "- Words with similar meanings (synonyms) are treated differently.\n",
    "- The effectiveness of BoW depends heavily on the quality of the preprocessing steps.\n",
    "\n",
    "### Alternatives\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency) is a popular extension of BoW that incorporates the importance of words within a document and across the corpus.\n",
    "- Word embeddings, like word2vec and GloVe, capture semantic relationships between words and provide a more nuanced representation of text data.\n",
    "- Overall, Bag-of-Words is a foundational technique in NLP, offering a simple and efficient way to represent text data for various tasks. However, it's important to be aware of its limitations and consider alternative approaches depending on the specific application.\n",
    "\n",
    "### Further reading\n",
    "https://www.scaler.com/topics/nlp/text-representation-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization And Feature Reduction In Spam Classification\n",
    "### Text to vectors\n",
    "- For machine learning models to work, text data needs to be converted into numerical features for processing.\n",
    "- BoW is a common technique for text vectorization.\n",
    "- BoW represents documents as vectors where each element corresponds to a unique word in the vocabulary.\n",
    "- The value in each element represents the frequency (count) of that word in the document.\n",
    "\n",
    "### Example\n",
    "Consider the sentence: \"Can you please look at the task...?\".\n",
    "- Vocabulary: [\"Can\", \"you\", \"please\", \"look\", \"at\", \"the\", \"task\", “...\"].\n",
    "- BoW vector: [1, 1, 1, 1, 1, 1, 1, 1] (Assuming each word appears once).\n",
    "\n",
    "### Challenges with high dimensionality\n",
    "- With large corpus, the vocabulary size (the number of unique words) can become very large. \n",
    "- This leads to high dimensional feature vectors (potentially tens of thousands of features).\n",
    "- High dimensionality can pose problems for machine learning models,\n",
    "    - Increased computational cost for training and prediction.\n",
    "    - Potential for overfitting, where the model memorizes training data instead of learning the general patterns.\n",
    "\n",
    "### Feature reduction techniques\n",
    "- Text cleaning: Preprocessing steps like removing the stop words (common words like \"the\", \"a\", \"an\") and punctuation can significantly reduce the vocabulary size.\n",
    "- Dimensionality reduction techniques:\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF): This method weights words based on their importance within a document and across the corpus. Words that are frequent in a document but rare overall (like \"Nigeria\" for spam) receive higher weights, leading to more informative features.\n",
    "    - Principal Component Analysis (PCA): This technique projects data points onto a lower-dimensional space while capturing most of the variance in the data. This reduces the number of features while preserving the most important information.\n",
    "\n",
    "### Summary\n",
    "- BoW provides a basic way to convert text into numerical features.\n",
    "- High dimensionality due to large vocabulary size can hinder model performance.\n",
    "- Text cleaning and dimensionality reduction techniques like TF-IDF and PCA help manage feature space and improve model effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning For Text Classification\n",
    "1. Convert sentences in words. This technically is called tokenization.\n",
    "2. Convert all the text to lowercase. How will this help? This will remove duplicates like, the, THE, The, etc.\n",
    "3. Remove non-alphabetical features. What does this mean? e.g., comma (,), full-stop (.), etc. These along with numbers can be removed. Removing numbers is not a hard rule, they can be left as it is in the text.\n",
    "4. Remove stopwords. Meaning, words such as, the, how, where, etc., can be removed. Stopwords are words that do not add a lot of value to the classification.\n",
    "\n",
    "NOTE: All of this text processing is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Intuition For Naive Bayes\n",
    "### Objective\n",
    "Classify a new text message (sentence) as spam (class 1) or ham (class 0) based on the words it contains.\n",
    "\n",
    "### Mathematical formulation\n",
    "The posterior probability has to be calculated,\n",
    "\n",
    "$P(y = c | w_1, w_2, ..., w_n)$ = Probability of the sentence belonging to class $c$ (spam or ham) given the set of words ($w_1$ to $w_n$).\n",
    "\n",
    "Using Bayes' theorem,\n",
    "\n",
    "$P(A | B) = \\frac{P(B | A) * P(A)}{P(B)}$\n",
    "\n",
    "Where,\n",
    "- A = Class ($c$ = 0 for ham, and $c$ = 1 for spam).\n",
    "- B = Set of words ($w_1$, $w_2$, ..., $w_n$).\n",
    "\n",
    "### Challenges\n",
    "1. Calculating likelihood ($P(B | A)$):\n",
    "    - The probability of all the words appearing together given the class is needed (e.g., $P(w_1, w_2, ..., w_n | y = 1)$ for spam).\n",
    "    - Directly calculating this joint probability is difficult due to the \"curse of dimensionality\" - the probability becomes extremely small as the number of words increases.\n",
    "2. Naive assumption: Naive Bayes addresses this by assuming independence between words in a document given the class label ($c$). This means,\n",
    "    - $P(w_1, w_2, ..., w_n | y = c) ≈ P(w_1 | y = c) * P(w_2 | y = c, w_1) * ... * P(w_n | y = c, w_1, w_2, ..., w_{n - 1})$.\n",
    "    - We estimate the probability of each word individually given the class ($c$).\n",
    "\n",
    "### Impact of the assumption\n",
    "- This simplification makes the calculation of likelihood tractable.\n",
    "- However, the independence assumption might not always hold true in natural language, where word order and context can influence meaning.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes offers a computationally efficient approach to text classification by,\n",
    "- Formulating the problem using Bayes' theorem and conditional probabilities.\n",
    "- Making the simplifying assumption of word independence given the class label.\n",
    "- Despite the assumption, Naive Bayes can be surprisingly effective for many text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Assumption in Naive Bayes\n",
    "### The core assumption\n",
    "Naive Bayes in text classification assumes independence between words in a document given the class label (spam or ham). This means,\n",
    "- $P(w_1, w_2, ..., w_n | y = c) ≈ P(w_1 | y = c) * P(w_2 | y = c) * ... * P(w_n | y = c)$.\n",
    "- We estimate the probability of each word individually given the class, ignoring the influence of other words in the sentence.\n",
    "\n",
    "### Impact of the assumption\n",
    "- Simplification: This assumption makes calculating the likelihood (probability of words given the class) tractable, avoiding the \"curse of dimensionality\" issue.\n",
    "- Limitation: The assumption might not always hold true. Words can be related, and their presence can influence the probability of others (e.g., \"happy\" and \"new\" appearing together more frequently).\n",
    "\n",
    "### Example\n",
    "Consider $P(w_2 | y = 1, w_1)$. Naively, it becomes $P(w_2 | y = 1)$, ignoring the presence of $w_1$. In reality, the probability of \"new\" might depend on \"happy\" being present.\n",
    "\n",
    "### Benefits of the assumption\n",
    "- Computational efficiency: Easier to calculate individual word probabilities than complex joint probabilities.\n",
    "- Surprisingly effective: Despite the simplification, Naive Bayes can achieve good performance in many text classification tasks.\n",
    "\n",
    "### Justification for the assumption\n",
    "- While word dependencies exist, their overall impact might average out across a large corpus.\n",
    "- The simplicity of the model can sometimes compensate for the imperfect assumption.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes takes a pragmatic approach. It acknowledges that word independence isn't entirely true but leverages the assumption for computational efficiency and achieves reasonable performance in many real-world scenarios. This trade-off between simplicity and accuracy is what makes Naive Bayes a popular choice for text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Naive Bayes For Text Classification\n",
    "### Objective\n",
    "Classify a sentence as spam (class 1) or ham (class 0) based on the words it contains.\n",
    "\n",
    "### Key equation\n",
    "The posterior probability has to be found: $P(y = c | \\text{sentence})$, which is the probability of the sentence belonging to class $c$ (spam or ham) given the words in the sentence.\n",
    "\n",
    "### Naive Bayes approach\n",
    "1. Leverages Bayes' theorem: $P(y = c | sentence) = \\frac{P(sentence | y = c) * P(y = c)}{P(\\text{sentence})}$.\n",
    "2. Naive assumption: Assumes independence between words in the sentence given the class label ($c$). This simplifies the calculation of $P(\\text{sentence} | y = c)$.\n",
    "3. Simplified equation (for spam class, $c$ = 1): $P(y = 1 | \\text{sentence}) ≈ \\Pi(P(w_i | y = 1)) * P(y = 1)$. Where,\n",
    "    - $\\Pi$ (product symbol) = Multiplies the probabilities of each word ($w_i$) appearing in the sentence given its spam ($y$ = 1).\n",
    "    - $P(y = 1)$ = Prior probability of a message being spam.\n",
    "\n",
    "### Classification\n",
    "- Calculate $P(y = 1 | \\text{sentence})$ and $P(y = 0 | \\text{sentence})$ using the same approach for both spam and ham classes.\n",
    "- The sentence is classified into the class with the higher posterior probability.\n",
    "\n",
    "### Why doesn't the denominator matter?\n",
    "- The denominator, $P(\\text{sentence})$ cancels out when comparing $P(y = 1 | \\text{sentence})$ and $P(y = 0 | \\text{sentence})$ because it is the same for both calculations.\n",
    "- Only the class with the higher probability is considered, so the constant denominator does not affect the final decision.\n",
    "\n",
    "### Naive assumption trade-off\n",
    "- The assumption simplifies calculations but might not always hold true (words can be related).\n",
    "- Despite the simplification, Naive Bayes can be surprisingly effective for many text classification tasks.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes offers a simple yet powerful approach for text classification. By leveraging Bayes' theorem and making a simplifying assumption, it efficiently estimates the probability of a sentence belonging to a class based on word probabilities. While the independence assumption is not perfect, it often provides good results in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations Of Naive Bayes\n",
    "While Naive Bayes offers a powerful approach, it has some limitations.\n",
    "\n",
    "### Limited text understanding\n",
    "- It analyzes words independently, ignoring their meaning or relationships within the sentence.\n",
    "- New words encountered during prediction (not in the training vocabulary) can lead to issues.\n",
    "\n",
    "### Assumption of order independence\n",
    "- The model does not consider the word order, which can affect meaning.\n",
    "- Sentences like \"good movie\" and \"movie bad\" might be treated similarly.\n",
    "\n",
    "### Frequency insensitivity\n",
    "The model treats a word appearing once or multiple times the same in the bag-of-words representation. Information about word frequency is lost.\n",
    "\n",
    "### Zero probability problem\n",
    "- If a word from a new sentence is absent from the vocabulary, its probability becomes 0.\n",
    "- This can lead to the entire equation for that class becoming 0, making classification impossible.\n",
    "\n",
    "### Handling out-of-vocabulary (OOV) words\n",
    "- Simple approach: Assume the word is not present at all (probability = 0).\n",
    "- $P(\\text{unknown word} | y = 1)$: Assign a uniform probability (often 1) to unseen words.\n",
    "- Laplace Smoothing: A more sophisticated technique that adds a small value (e.g., 1) to the count of each word estimating probabilities. This avoids zero probabilities and provides smoother estimates.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes offers a trade-off between simplicity and accuracy. While it has limitations in understanding complex text relationships, it can be effective for many classification tasks. Techniques like Laplace help address the zero probability problem and improve robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Smoothing For Naive Bayes\n",
    "### Problem\n",
    "Naive Bayes calculates the probability of words ($w_j$) appearing in a class (e.g., spam). If a word is absent from the training data for a specific class, its probability becomes 0. This can lead to,\n",
    "1. Zero probability problem: The entire equation for that class becomes 0, making classification impossible.\n",
    "2. Mathematical issues: Multiplication by 0 can cause problems in calculations.\n",
    "\n",
    "### Solution: Laplace Smoothing\n",
    "This technique adds a small value ($\\alpha$) to the count of each word when estimating probabilities. The formula for Laplace Smoothing with Naive Bayes is, $P(w_j | y = 1) = (\\frac{\\text{Count}(w_j, y = 1) + \\alpha}{Total number of words in class 1 + \\alpha * c})$. Where,\n",
    "- $\\alpha$ = Hyperparameter controlling smoothing (typically a small value like 1).\n",
    "- $c$ = Number of possible values for $w_j$ (in this case, 0 or 1).\n",
    "\n",
    "### Advantages\n",
    "- Non-zero probabilities: Ensures all words have a non-zero probability, even if unseen in training data.\n",
    "- Robustness: Prevents the model from breaking down due to zero probabilities.\n",
    "- Smoother estimates: Reduces the impact of sparse data, leading to more stable and reliable probability estimates.\n",
    "\n",
    "### Example\n",
    "- $\\alpha$ = 1.\n",
    "- Word \"important\" not present in spam emails ($\\text{Count}(\\text{important}, y = 1) = 0$).\n",
    "\n",
    "### Without Smoothing\n",
    "$\\frac{P(\\text{important} | y = 1)}{\\text{Total spam emails} = 0}$ (classification impossible).\n",
    "\n",
    "### With Smoothing\n",
    "$P(\\text{important} | y = 1) = \\frac{0 + 1}{\\text{Total spam emails} + 1 * 2} = \\frac{1}{\\text{Total spam emails} + 2}$ (provides a valid probability).\n",
    "\n",
    "Laplace Smoothing is a simple yet effective technique that improves the robustness and reliability of Naive Bayes for text classification by avoiding zero probabilities and providing smoother probability estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli V. Multinomial Naive Bayes\n",
    "### Feature representation\n",
    "The key difference between Bernoulli and Multinomial Naive Bayes lies in how they handle features (word occurrences) in text classification.\n",
    "\n",
    "### Bernoulli Naive Bayes (Bernoulli NB)\n",
    "- Suitable for features with only 2 possible values (typically 0 or 1).\n",
    "- Example: \"good\" can be either present (1) or absent (0) in a document.\n",
    "\n",
    "### Multinomial Naive Bayes (Multinomial NB)\n",
    "- Applicable for features with multiple distince values (k, where k > 2).\n",
    "- Example: \"good\" can appear 0 times, 1 time, 2 times, and so on (represented by different values based on frequency).\n",
    "\n",
    "### Impact on features\n",
    "- Bernoulli NB:\n",
    "    - Simpler model with fewer features (0 or 1 for each word).\n",
    "    - Might miss information about word frequency.\n",
    "- Multinomial NB:\n",
    "    - More complex model with increased features for each word (representing frequency).\n",
    "    - Captures word frequency information but leads to a larger feature space.\n",
    "\n",
    "### Feature engineering considerations\n",
    "While Multinomial NB can capture frequency, the increase in features can be problematic. Techniques like,\n",
    "- Minimum frequency threshold: Ignore words appearing less than a certain number of times.\n",
    "- Maximum frequency threshold: Cap the maximum value for frequent words (e.g., \"the\").\n",
    "\n",
    "These techniques help manage the feature space size in Multinomial NB.\n",
    "\n",
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
