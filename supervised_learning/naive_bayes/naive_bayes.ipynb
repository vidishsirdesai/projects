{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Naive Bayes is family of classification algorithms based on Bayes' theorem. It is a popular choice for various classification tasks due to its simplicity, efficiency, and interpretability.\n",
    "\n",
    "### Core principle\n",
    "Naive Bayes classifiers work under the assumption fo conditional independence between features (predictors) given the class label (target variable). In simpler terms, it assumes that knowing the value of one feature does not influence the probability of another feature's value, as long as the class label is already known. While this assumption is not always hold true in reality, it often works well in practive for many classification problems.\n",
    "\n",
    "### Classification process\n",
    "- Training: The model learns from the labeled dataset where each data point has features and a corresponding class label.\n",
    "- Prediction: For a new unseen data point, the model calculates the probability of it belonging to each class. It achieves this by,\n",
    "    1. Using Bayes' theorem to compute the posterior probability (probability of a class given the features).\n",
    "    2. Assuming conditional independence between features, which simplifies the calculations.\n",
    "    3. Multiplying the probabilities of each feature value given the class and multiplying by the prior probability of the class itself (learned from the training data).\n",
    "- Assigning class label: The class with the highest posterior probability is assigned as the predicted class for the new data point.\n",
    "\n",
    "### Example\n",
    "Imagine emails are being classified as spam or not spam based on features like наличиe слова \"деньги\" (presence of the word \"money\") and наличие восклицательных знаков (presence of exclamation marks). Naive Bayes would assume that the presence of \"money\" doesn't influence the presence of exclamation marks (and vice versa) given the email class (spam or not spam).\n",
    "\n",
    "### Advantages of Naive Bayes\n",
    "- Simplicity and efficiency: Naive Bayes is easy to understand and implement, making it a good choice for beginners. It's also computationally efficient for training and prediction.\n",
    "- Interpretability: The model allows to understand how each feature contributes to the classification by examining the feature probabilities for each class.\n",
    "- Performance: Naive Bayes can perform well for various classification tasks, especially when dealing with high-dimensional data (many features).\n",
    "\n",
    "### Disadvantage of Naive Bayes\n",
    "- Conditional independence assumption: The assumption of conditional independence between features might not always be valid, which can lead to suboptimal performance in some cases.\n",
    "- Sensitivity to features: Naive Bayes can be sensitive to irrelevant features or features with many unique values. Feature selection or preprocessing techniques might be necessary for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "sns.set_theme(style = \"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_clean.csv\", encoding = \"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Bayes Theorem\n",
    "$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$\n",
    "\n",
    "Where,\n",
    "- $P(A|B)$ = Posterior. The probability of A being true, given B is true.\n",
    "- $P(B|A)$ = Likelihood. The probability of B being true, given A is true.\n",
    "- $P(A)$ = Prior. The probability of A being true. This is knowledge.\n",
    "- $P(B)$ = Marginalization. The probability of B being true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "### 1. Data preprocessing\n",
    "- Text cleaning:\n",
    "    - Remove punctuation, stop words (common words with little meaning), and potentially numbers depending on the task.\n",
    "    - Convert text to lowercase for consistency.\n",
    "    - Consider stemming or lemmatization (reducing words to base form) for improved accuracy (optional).\n",
    "- Feature representation: Represent each document (sentence) as a feature vector.\n",
    "- Common approaches:\n",
    "    - Bag-of-Words (BoW): Each word occurrence is a feature (0 or 1 indicating absence or presence).(Bernoulli NB).\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF): Weights words based on their importance within the document and rarity across the corpus. (Multinomial NB)\n",
    "\n",
    "### 2. Model training\n",
    "- Calculate class priors: Estimate the probability ($P(y = c)$) for each class ($c$) based on frequency in the training data.\n",
    "- Calculate conditional probabilities:\n",
    "    - Estimate the probability of each feature (word) appearing given a specific class ($P(w_i|y = c)$)\n",
    "    - Use techniques like Laplace Smoothing to avoid zero probabilities for unseen words (especially for multinomial NB).\n",
    "\n",
    "### 3. Classification of new sentence\n",
    "- Calculate posterior probability:\n",
    "    - Use the equation: $P(y = c | \\text{sentence}) ≈ \\Pi(P(w_i | y = c)) * P(y = c)$.\n",
    "    - Multiply the probabilities of each word ($w_i$) appearing in the sentence given its class ($c$).\n",
    "    - Multiply by the prior probability of class ($c$).\n",
    "- Class prediction: Assign the sentence to the class with the highest posterior probability ($P(y = c | sentence)$).\n",
    "\n",
    "### Key points\n",
    "- Naive Bayes assumes independence between words in a document given the class label (simplification).\n",
    "- This assumption might not always hold true but offers computational efficiency and can be surprisingly effective.\n",
    "- Multinomial NB can capture word frequency information but requires managing the feature space size.\n",
    "- Laplace Smoothing helps address zero probabilities and improves model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier With Naive Bayes, And Bag-of-Words\n",
    "### Objective\n",
    "Create a binary text classifier to distinguish spam email from legitimate emails (ham).\n",
    "\n",
    "### Challenges\n",
    "- Text data cannot be fed into ML models.\n",
    "- The text information has to be converted into features suitable for the model.\n",
    "\n",
    "### Solution\n",
    "- Feature extraction using Bag-of-Words: This technique represents documents as a collection of words, ignoring grammar and word order.\n",
    "    - All the unique words from the entire email dataset are extracted.\n",
    "    - Each email is the represented by a feature vector where each element indicated the frequency (count) of a particular word in that email.\n",
    "\n",
    "### Classification using Naive Bayes\n",
    "- Naive Bayes is well suited for text classification tasks.\n",
    "- It assumes the independence of features (words) in a document, which might not be strictly true but often works well in practice for text data.\n",
    "- The model calculates the probability of an email being spam or ham based on the presence and frequency of words associated with each category.\n",
    "\n",
    "### Example\n",
    "- Emails:\n",
    "    - \"Can you please look at the task ...?\" (ham)\n",
    "    - \"Hi! I am a Nigerian Prince\" (spam)\n",
    "- Extracted Bag-of-Words: \"Can\", \"you\", \"please\", \"look\", \"at\", \"the\", \"task\", \"...\", \"Hi\", \"!\", \"am\", \"a\", “Nigerian\", \"Prince\".\n",
    "- Feature vectors:\n",
    "    - Ham: (1 count of \"Can\", 1 count of \"you\", ..., 0 for \"Nigerian\", 0 for “Prince\").\n",
    "    - Spam: (0 for \"Can\", 0 for \"you\", ..., 1 count of “Nigerian\", count of \"Prince\").\n",
    "\n",
    "Naive Bayes uses these feature vectors and their corresponding labels (spam/ ham) to learn the probability distribution of words for each category. During prediction for a new email, the model calculates the probabilities of the email being spam and ham based on the word frequencies and classifies it accordingly.\n",
    "\n",
    "### Summary\n",
    "- Bag-of-Words transforms text data into numerical features for analysis.\n",
    "- Naive Bayes leverages these features to classify emails as spam or ham based on word probabilities learned from the training data. This approach provides a simple and effective way to build a spam classifier.\n",
    "\n",
    "### Note\n",
    "Real world spam classification can be more complex and might involve additional techniques like stemming or lemmatization (reducing words to their root form), n-grams (considering sequences of words), feature weighting based on importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Bag-of-Words (BoW)\n",
    "Bag-of-Words (BoW) is a fundamental technique used in Natural Language Processing (NLP) for representing text data. It focuses on the occurrences of words within a document, ignoring grammar or word order.\n",
    "\n",
    "### Core idea\n",
    "- Imagine a bag filled with words, where each word appears as many times as it occurs in the document.\n",
    "- The order of context in which the words appear is not considered.\n",
    "\n",
    "### Creating a Bag-of-Words representation\n",
    "1. Preprocessing: Text cleaning steps like removing punctuation, stop words (that is, common words like \"a\", \"an\", \"the\"), and converting text to lowercase are often performed.\n",
    "2. Tokenization: The text is split into individual words (tokens).\n",
    "3. Vocabulary creation: A list of unique words encountered across all documents in the corpus (collection of documents) is created. This is called the vocabulary.\n",
    "4. Feature vector representation: Each document is represented by a feature vector. This vector has the same length as the vocabulary.\n",
    "\n",
    "- Each element in the vector corresponds to a word in the vocabulary.\n",
    "- The value at each element represents the number of times that particular word appears in the document (its frequency)/\n",
    "\n",
    "### Example\n",
    "Consider 2 documents,\n",
    "- Document 1: \"The quick brown fox jumps over the laxy dog.\".\n",
    "- Document 2: \"The dog is lazy. The fox is quick.\".\n",
    "\n",
    "After preprocessing and tokenization, the result would be,\n",
    "- Vocabulary: [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"is\"].\n",
    "\n",
    "The feature vectors for these documents could be,\n",
    "- Document 1: [2, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "- Document 2: [2, 1, 0, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "### Applications\n",
    "Bag-of-Words is a simple and effective way to represent text data for various NLP tasks, including,\n",
    "- Document classification (e.g., spam detection, sentiment analysis).\n",
    "- Information retrieval (e.g., document search).\n",
    "- Topic modeling (identifying groups of related words).\n",
    "\n",
    "### Limitations\n",
    "- BoW ignores word order and context, which can be crucial for understanding the meaning of a sentence.\n",
    "- Words with similar meanings (synonyms) are treated differently.\n",
    "- The effectiveness of BoW depends heavily on the quality of the preprocessing steps.\n",
    "\n",
    "### Alternatives\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency) is a popular extension of BoW that incorporates the importance of words within a document and across the corpus.\n",
    "- Word embeddings, like word2vec and GloVe, capture semantic relationships between words and provide a more nuanced representation of text data.\n",
    "- Overall, Bag-of-Words is a foundational technique in NLP, offering a simple and efficient way to represent text data for various tasks. However, it's important to be aware of its limitations and consider alternative approaches depending on the specific application.\n",
    "\n",
    "### Further reading\n",
    "https://www.scaler.com/topics/nlp/text-representation-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization And Feature Reduction In Spam Classification\n",
    "### Text to vectors\n",
    "- For machine learning models to work, text data needs to be converted into numerical features for processing.\n",
    "- BoW is a common technique for text vectorization.\n",
    "- BoW represents documents as vectors where each element corresponds to a unique word in the vocabulary.\n",
    "- The value in each element represents the frequency (count) of that word in the document.\n",
    "\n",
    "### Example\n",
    "Consider the sentence: \"Can you please look at the task...?\".\n",
    "- Vocabulary: [\"Can\", \"you\", \"please\", \"look\", \"at\", \"the\", \"task\", “...\"].\n",
    "- BoW vector: [1, 1, 1, 1, 1, 1, 1, 1] (Assuming each word appears once).\n",
    "\n",
    "### Challenges with high dimensionality\n",
    "- With large corpus, the vocabulary size (the number of unique words) can become very large. \n",
    "- This leads to high dimensional feature vectors (potentially tens of thousands of features).\n",
    "- High dimensionality can pose problems for machine learning models,\n",
    "    - Increased computational cost for training and prediction.\n",
    "    - Potential for overfitting, where the model memorizes training data instead of learning the general patterns.\n",
    "\n",
    "### Feature reduction techniques\n",
    "- Text cleaning: Preprocessing steps like removing the stop words (common words like \"the\", \"a\", \"an\") and punctuation can significantly reduce the vocabulary size.\n",
    "- Dimensionality reduction techniques:\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF): This method weights words based on their importance within a document and across the corpus. Words that are frequent in a document but rare overall (like \"Nigeria\" for spam) receive higher weights, leading to more informative features.\n",
    "    - Principal Component Analysis (PCA): This technique projects data points onto a lower-dimensional space while capturing most of the variance in the data. This reduces the number of features while preserving the most important information.\n",
    "\n",
    "### Summary\n",
    "- BoW provides a basic way to convert text into numerical features.\n",
    "- High dimensionality due to large vocabulary size can hinder model performance.\n",
    "- Text cleaning and dimensionality reduction techniques like TF-IDF and PCA help manage feature space and improve model effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning For Text Classification\n",
    "1. Convert sentences in words. This technically is called tokenization.\n",
    "2. Convert all the text to lowercase. How will this help? This will remove duplicates like, the, THE, The, etc.\n",
    "3. Remove non-alphabetical features. What does this mean? e.g., comma (,), full-stop (.), etc. These along with numbers can be removed. Removing numbers is not a hard rule, they can be left as it is in the text.\n",
    "4. Remove stopwords. Meaning, words such as, the, how, where, etc., can be removed. Stopwords are words that do not add a lot of value to the classification.\n",
    "\n",
    "NOTE: All of this text processing is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical Intuition For Naive Bayes\n",
    "### Objective\n",
    "Classify a new text message (sentence) as spam (class 1) or ham (class 0) based on the words it contains.\n",
    "\n",
    "### Mathematical formulation\n",
    "The posterior probability has to be calculated,\n",
    "\n",
    "$P(y = c | w_1, w_2, ..., w_n)$ = Probability of the sentence belonging to class $c$ (spam or ham) given the set of words ($w_1$ to $w_n$).\n",
    "\n",
    "Using Bayes' theorem,\n",
    "\n",
    "$P(A | B) = \\frac{P(B | A) * P(A)}{P(B)}$\n",
    "\n",
    "Where,\n",
    "- A = Class ($c$ = 0 for ham, and $c$ = 1 for spam).\n",
    "- B = Set of words ($w_1$, $w_2$, ..., $w_n$).\n",
    "\n",
    "### Challenges\n",
    "1. Calculating likelihood ($P(B | A)$):\n",
    "    - The probability of all the words appearing together given the class is needed (e.g., $P(w_1, w_2, ..., w_n | y = 1)$ for spam).\n",
    "    - Directly calculating this joint probability is difficult due to the \"curse of dimensionality\" - the probability becomes extremely small as the number of words increases.\n",
    "2. Naive assumption: Naive Bayes addresses this by assuming independence between words in a document given the class label ($c$). This means,\n",
    "    - $P(w_1, w_2, ..., w_n | y = c) ≈ P(w_1 | y = c) * P(w_2 | y = c, w_1) * ... * P(w_n | y = c, w_1, w_2, ..., w_{n - 1})$.\n",
    "    - We estimate the probability of each word individually given the class ($c$).\n",
    "\n",
    "### Impact of the assumption\n",
    "- This simplification makes the calculation of likelihood tractable.\n",
    "- However, the independence assumption might not always hold true in natural language, where word order and context can influence meaning.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes offers a computationally efficient approach to text classification by,\n",
    "- Formulating the problem using Bayes' theorem and conditional probabilities.\n",
    "- Making the simplifying assumption of word independence given the class label.\n",
    "- Despite the assumption, Naive Bayes can be surprisingly effective for many text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Assumption in Naive Bayes\n",
    "### The core assumption\n",
    "Naive Bayes in text classification assumes independence between words in a document given the class label (spam or ham). This means,\n",
    "- $P(w_1, w_2, ..., w_n | y = c) ≈ P(w_1 | y = c) * P(w_2 | y = c) * ... * P(w_n | y = c)$.\n",
    "- We estimate the probability of each word individually given the class, ignoring the influence of other words in the sentence.\n",
    "\n",
    "### Impact of the assumption\n",
    "- Simplification: This assumption makes calculating the likelihood (probability of words given the class) tractable, avoiding the \"curse of dimensionality\" issue.\n",
    "- Limitation: The assumption might not always hold true. Words can be related, and their presence can influence the probability of others (e.g., \"happy\" and \"new\" appearing together more frequently).\n",
    "\n",
    "### Example\n",
    "Consider $P(w_2 | y = 1, w_1)$. Naively, it becomes $P(w_2 | y = 1)$, ignoring the presence of $w_1$. In reality, the probability of \"new\" might depend on \"happy\" being present.\n",
    "\n",
    "### Benefits of the assumption\n",
    "- Computational efficiency: Easier to calculate individual word probabilities than complex joint probabilities.\n",
    "- Surprisingly effective: Despite the simplification, Naive Bayes can achieve good performance in many text classification tasks.\n",
    "\n",
    "### Justification for the assumption\n",
    "- While word dependencies exist, their overall impact might average out across a large corpus.\n",
    "- The simplicity of the model can sometimes compensate for the imperfect assumption.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes takes a pragmatic approach. It acknowledges that word independence isn't entirely true but leverages the assumption for computational efficiency and achieves reasonable performance in many real-world scenarios. This trade-off between simplicity and accuracy is what makes Naive Bayes a popular choice for text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Of Naive Bayes For Text Classification\n",
    "### Objective\n",
    "Classify a sentence as spam (class 1) or ham (class 0) based on the words it contains.\n",
    "\n",
    "### Key equation\n",
    "The posterior probability has to be found: $P(y = c | \\text{sentence})$, which is the probability of the sentence belonging to class $c$ (spam or ham) given the words in the sentence.\n",
    "\n",
    "### Naive Bayes approach\n",
    "1. Leverages Bayes' theorem: $P(y = c | sentence) = \\frac{P(sentence | y = c) * P(y = c)}{P(\\text{sentence})}$.\n",
    "2. Naive assumption: Assumes independence between words in the sentence given the class label ($c$). This simplifies the calculation of $P(\\text{sentence} | y = c)$.\n",
    "3. Simplified equation (for spam class, $c$ = 1): $P(y = 1 | \\text{sentence}) ≈ \\Pi(P(w_i | y = 1)) * P(y = 1)$. Where,\n",
    "    - $\\Pi$ (product symbol) = Multiplies the probabilities of each word ($w_i$) appearing in the sentence given its spam ($y$ = 1).\n",
    "    - $P(y = 1)$ = Prior probability of a message being spam.\n",
    "\n",
    "### Classification\n",
    "- Calculate $P(y = 1 | \\text{sentence})$ and $P(y = 0 | \\text{sentence})$ using the same approach for both spam and ham classes.\n",
    "- The sentence is classified into the class with the higher posterior probability.\n",
    "\n",
    "### Why doesn't the denominator matter?\n",
    "- The denominator, $P(\\text{sentence})$ cancels out when comparing $P(y = 1 | \\text{sentence})$ and $P(y = 0 | \\text{sentence})$ because it is the same for both calculations.\n",
    "- Only the class with the higher probability is considered, so the constant denominator does not affect the final decision.\n",
    "\n",
    "### Naive assumption trade-off\n",
    "- The assumption simplifies calculations but might not always hold true (words can be related).\n",
    "- Despite the simplification, Naive Bayes can be surprisingly effective for many text classification tasks.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes offers a simple yet powerful approach for text classification. By leveraging Bayes' theorem and making a simplifying assumption, it efficiently estimates the probability of a sentence belonging to a class based on word probabilities. While the independence assumption is not perfect, it often provides good results in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations Of Naive Bayes\n",
    "While Naive Bayes offers a powerful approach, it has some limitations.\n",
    "\n",
    "### Limited text understanding\n",
    "- It analyzes words independently, ignoring their meaning or relationships within the sentence.\n",
    "- New words encountered during prediction (not in the training vocabulary) can lead to issues.\n",
    "\n",
    "### Assumption of order independence\n",
    "- The model does not consider the word order, which can affect meaning.\n",
    "- Sentences like \"good movie\" and \"movie bad\" might be treated similarly.\n",
    "\n",
    "### Frequency insensitivity\n",
    "The model treats a word appearing once or multiple times the same in the bag-of-words representation. Information about word frequency is lost.\n",
    "\n",
    "### Zero probability problem\n",
    "- If a word from a new sentence is absent from the vocabulary, its probability becomes 0.\n",
    "- This can lead to the entire equation for that class becoming 0, making classification impossible.\n",
    "\n",
    "### Handling out-of-vocabulary (OOV) words\n",
    "- Simple approach: Assume the word is not present at all (probability = 0).\n",
    "- $P(\\text{unknown word} | y = 1)$: Assign a uniform probability (often 1) to unseen words.\n",
    "- Laplace Smoothing: A more sophisticated technique that adds a small value (e.g., 1) to the count of each word estimating probabilities. This avoids zero probabilities and provides smoother estimates.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes offers a trade-off between simplicity and accuracy. While it has limitations in understanding complex text relationships, it can be effective for many classification tasks. Techniques like Laplace help address the zero probability problem and improve robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Smoothing For Naive Bayes\n",
    "### Problem\n",
    "Naive Bayes calculates the probability of words ($w_j$) appearing in a class (e.g., spam). If a word is absent from the training data for a specific class, its probability becomes 0. This can lead to,\n",
    "1. Zero probability problem: The entire equation for that class becomes 0, making classification impossible.\n",
    "2. Mathematical issues: Multiplication by 0 can cause problems in calculations.\n",
    "\n",
    "### Solution: Laplace Smoothing\n",
    "This technique adds a small value ($\\alpha$) to the count of each word when estimating probabilities. The formula for Laplace Smoothing with Naive Bayes is, $P(w_j | y = 1) = (\\frac{\\text{Count}(w_j, y = 1) + \\alpha}{Total number of words in class 1 + \\alpha * c})$. Where,\n",
    "- $\\alpha$ = Hyperparameter controlling smoothing (typically a small value like 1).\n",
    "- $c$ = Number of possible values for $w_j$ (in this case, 0 or 1).\n",
    "\n",
    "### Advantages\n",
    "- Non-zero probabilities: Ensures all words have a non-zero probability, even if unseen in training data.\n",
    "- Robustness: Prevents the model from breaking down due to zero probabilities.\n",
    "- Smoother estimates: Reduces the impact of sparse data, leading to more stable and reliable probability estimates.\n",
    "\n",
    "### Example\n",
    "- $\\alpha$ = 1.\n",
    "- Word \"important\" not present in spam emails ($\\text{Count}(\\text{important}, y = 1) = 0$).\n",
    "\n",
    "### Without Smoothing\n",
    "$\\frac{P(\\text{important} | y = 1)}{\\text{Total spam emails} = 0}$ (classification impossible).\n",
    "\n",
    "### With Smoothing\n",
    "$P(\\text{important} | y = 1) = \\frac{0 + 1}{\\text{Total spam emails} + 1 * 2} = \\frac{1}{\\text{Total spam emails} + 2}$ (provides a valid probability).\n",
    "\n",
    "Laplace Smoothing is a simple yet effective technique that improves the robustness and reliability of Naive Bayes for text classification by avoiding zero probabilities and providing smoother probability estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli V. Multinomial Naive Bayes\n",
    "### Feature representation\n",
    "The key difference between Bernoulli and Multinomial Naive Bayes lies in how they handle features (word occurrences) in text classification.\n",
    "\n",
    "### Bernoulli Naive Bayes (Bernoulli NB)\n",
    "- Suitable for features with only 2 possible values (typically 0 or 1).\n",
    "- Example: \"good\" can be either present (1) or absent (0) in a document.\n",
    "\n",
    "### Multinomial Naive Bayes (Multinomial NB)\n",
    "- Applicable for features with multiple distince values (k, where k > 2).\n",
    "- Example: \"good\" can appear 0 times, 1 time, 2 times, and so on (represented by different values based on frequency).\n",
    "\n",
    "### Impact on features\n",
    "- Bernoulli NB:\n",
    "    - Simpler model with fewer features (0 or 1 for each word).\n",
    "    - Might miss information about word frequency.\n",
    "- Multinomial NB:\n",
    "    - More complex model with increased features for each word (representing frequency).\n",
    "    - Captures word frequency information but leads to a larger feature space.\n",
    "\n",
    "### Feature engineering considerations\n",
    "While Multinomial NB can capture frequency, the increase in features can be problematic. Techniques like,\n",
    "- Minimum frequency threshold: Ignore words appearing less than a certain number of times.\n",
    "- Maximum frequency threshold: Cap the maximum value for frequent words (e.g., \"the\").\n",
    "\n",
    "These techniques help manage the feature space size in Multinomial NB.\n",
    "\n",
    "### Summary\n",
    "- Choose Bernoulli NB for binary features (presence or absence).\n",
    "- Choose Multinomial NB for features with multiple features (frequency).\n",
    "- Be mindful of feature explosion in Multinomial NB and consider feature engineering techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification\n",
    "Naive Bayes can effectively used for multi-class classification problems (more than 2 classes).\n",
    "\n",
    "### Key idea\n",
    "- The core idea concept from binary classification is extended.\n",
    "- The posterior probability ($P(y = c | \\text{sentence})$) is calculated for each possible class ($c$).\n",
    "- The class with the highest posterior probability is chosen for the sentence.\n",
    "\n",
    "### Mathematical formulation\n",
    "Similar to the binary case, the same basic equation can be used but for multiple cases,\n",
    "\n",
    "$P(y = c | \\text{sentence}) ≈ \\Pi{(P(w_i | y = c)) * P(y = c)}$. Where,\n",
    "- c iterates over all possible classes (0, 1, 2, etc).\n",
    "- $P(w_i | y = c)$ is the probability of word $w_i$ appearing, given class $c$.\n",
    "- $P(y = c)$ is the prior probability of class $c$ (overall frequency of class $c$).\n",
    "\n",
    "### Classification\n",
    "1. Calculate the posterior probability for each class using the equation above.\n",
    "2. Assign the sentence to the class with the highest posterior probability.\n",
    "\n",
    "### Example (Multinomial NB)\n",
    "Consider a scenario with 3 classes, spam ($c$ = 0), important ($c$ = 1), and advertisment ($c$ = 2). $P(y = 0 | \\text{sentence})$, $P(y = 1 | \\text{sentence})$, and $P(y = 2 | \\text{sentence})$ are calculated for a new sentence. The class with the highest probability wins.\n",
    "\n",
    "### Summary\n",
    "Naive Bayes handles multi-class classification efficiently by calculating posterior probabilities for each class and assigning the sentence to the class with the highest probability. This approach extends the core concepts from binary classification to handle more complex scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation Of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok lar... Joking wif u oni...'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"message\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vidishsirdesai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vidishsirdesai/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vidishsirdesai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing text\n",
    "text_sample = \"Hello, world!\"\n",
    "\n",
    "# tokenization\n",
    "nltk.word_tokenize(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sample.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: lowercase\n",
    "lowercase_text_sample = [i.lower() for i in nltk.word_tokenize(text_sample)]\n",
    "lowercase_text_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123@gmail.com --> 123gmailcom\n"
     ]
    }
   ],
   "source": [
    "# step 2: remove non-alpha tokens\n",
    "import re\n",
    "\n",
    "text_sample = [\"123@gmail.com\"]\n",
    "\n",
    "for word in text_sample:\n",
    "    # replace everything that is not (^) alphanumeric or whitespace with \"\"\n",
    "    c_word = re.sub(r\"[^\\w\\s]\", \"\", word)\n",
    "    if c_word not in nltk.corpus.stopwords.words(\"english\"):\n",
    "        print(word, \"-->\", c_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok lar joking wif u oni'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# packages for text processing\n",
    "# import re, nltk, ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"punkt_tab\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# from nltk import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# a simple text processing function\n",
    "def clean_tokenized_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Performs basic cleaning of tokenized sentence.\n",
    "    \"\"\"\n",
    "    # an empty string to store the processes sentence\n",
    "    cleaned_sentence = \"\"\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        # convert to lowercase\n",
    "        cleaned_word = word.lower()\n",
    "        # remove punctuations by substitution\n",
    "        cleaned_word = re.sub(r\"[^\\w\\s]\", \"\", cleaned_word)\n",
    "\n",
    "        # remove stopwords\n",
    "        if cleaned_word != \"\" and cleaned_word not in nltk.corpus.stopwords.words(\"english\"):\n",
    "            # append the processed words to new list\n",
    "            cleaned_sentence = cleaned_sentence + \" \" + cleaned_word\n",
    "        \n",
    "    return (cleaned_sentence.strip())\n",
    "\n",
    "sentence = \"Ok lar... The Joking wif u oni...\"\n",
    "clean_tokenized_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>message</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>ham</td>\n",
       "      <td>We not leaving yet. Ok lor then we go elsewher...</td>\n",
       "      <td>leaving yet ok lor go elsewhere n eat u thk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2862</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok that would b lovely, if u r sure. Think abo...</td>\n",
       "      <td>ok would b lovely u r sure think wot u want dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>ham</td>\n",
       "      <td>Aight, lemme know what's up</td>\n",
       "      <td>aight lem know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dear :-/ why you mood off. I cant drive so i b...</td>\n",
       "      <td>dear mood cant drive brother drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thats cool! I am a gentleman and will treat yo...</td>\n",
       "      <td>thats cool gentleman treat dignity respect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3840</th>\n",
       "      <td>ham</td>\n",
       "      <td>Howz pain.it will come down today.do as i said...</td>\n",
       "      <td>howz painit come todaydo said ystrdayice medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>ham</td>\n",
       "      <td>You are everywhere dirt, on the floor, the win...</td>\n",
       "      <td>everywhere dirt floor windows even shirt somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>ham</td>\n",
       "      <td>Oh really? perform, write a paper, go to a mov...</td>\n",
       "      <td>oh really perform write paper go movie home mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy at the car shop who was flirting with ...</td>\n",
       "      <td>guy car shop flirting got phone number paperwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>ham</td>\n",
       "      <td>Dont know supports ass and srt i thnk. I think...</td>\n",
       "      <td>dont know supports ass srt thnk think ps3 play...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type                                            message  \\\n",
       "1917  ham  We not leaving yet. Ok lor then we go elsewher...   \n",
       "2862  ham  Ok that would b lovely, if u r sure. Think abo...   \n",
       "3032  ham                        Aight, lemme know what's up   \n",
       "4395  ham  Dear :-/ why you mood off. I cant drive so i b...   \n",
       "1395  ham  Thats cool! I am a gentleman and will treat yo...   \n",
       "3840  ham  Howz pain.it will come down today.do as i said...   \n",
       "154   ham  You are everywhere dirt, on the floor, the win...   \n",
       "3921  ham  Oh really? perform, write a paper, go to a mov...   \n",
       "4800  ham  The guy at the car shop who was flirting with ...   \n",
       "1871  ham  Dont know supports ass and srt i thnk. I think...   \n",
       "\n",
       "                                        cleaned_message  \n",
       "1917        leaving yet ok lor go elsewhere n eat u thk  \n",
       "2862  ok would b lovely u r sure think wot u want dr...  \n",
       "3032                                     aight lem know  \n",
       "4395                 dear mood cant drive brother drive  \n",
       "1395         thats cool gentleman treat dignity respect  \n",
       "3840  howz painit come todaydo said ystrdayice medicine  \n",
       "154   everywhere dirt floor windows even shirt somet...  \n",
       "3921  oh really perform write paper go movie home mi...  \n",
       "4800  guy car shop flirting got phone number paperwo...  \n",
       "1871  dont know supports ass srt thnk think ps3 play...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the text cleaning function to the dataset\n",
    "df[\"cleaned_message\"] = df[\"message\"].apply(clean_tokenized_sentence)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_message\"] = df[\"cleaned_message\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   type             5572 non-null   object\n",
      " 1   cleaned_message  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# dropping the \"message\" column\n",
    "df.drop(columns = [\"message\"], inplace = True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0</td>\n",
       "      <td>aight close still around alex place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>0</td>\n",
       "      <td>awesome minute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>1</td>\n",
       "      <td>text82228 get ringtones logos games wwwtxt8222...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>1</td>\n",
       "      <td>adult 18 content video shortly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0</td>\n",
       "      <td>yo chad gymnastics class wan na take site says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216</th>\n",
       "      <td>0</td>\n",
       "      <td>office around 4 pm going hospital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>0</td>\n",
       "      <td>playng 9 doors game gt racing phone lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>0</td>\n",
       "      <td>get home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>0</td>\n",
       "      <td>thank much skyped wit kz sura didnt get pleasu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0</td>\n",
       "      <td>hello handsome finding job lazy working toward...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                    cleaned_message\n",
       "696      0                aight close still around alex place\n",
       "4449     0                                     awesome minute\n",
       "2547     1  text82228 get ringtones logos games wwwtxt8222...\n",
       "2861     1                     adult 18 content video shortly\n",
       "963      0  yo chad gymnastics class wan na take site says...\n",
       "4216     0                  office around 4 pm going hospital\n",
       "2678     0            playng 9 doors game gt racing phone lol\n",
       "2148     0                                           get home\n",
       "1261     0  thank much skyped wit kz sura didnt get pleasu...\n",
       "185      0  hello handsome finding job lazy working toward..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the target column \"type\"\n",
    "def encode_type(i):\n",
    "    encode = {\n",
    "        \"spam\": 1,\n",
    "        \"ham\": 0\n",
    "    }\n",
    "    return encode[i]\n",
    "\n",
    "df[\"type\"] = df[\"type\"].apply(encode_type)\n",
    "# df[\"type\"] = df[\"type\"].apply({\"spam\": 1, \"ham\": 0})\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4179,), (1393,), (4179,), (1393,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"cleaned_message\"], df[\"type\"], test_size = 0.25, random_state = 42)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4179, 7612), (1393, 7612))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting to bag of words and then to features\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "# converts to BoW and then to features\n",
    "x_test = vectorizer.transform(x_test)\n",
    "# size of the bag\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'call': 1571,\n",
       " 'tell': 6544,\n",
       " 'headache': 3293,\n",
       " 'want': 7144,\n",
       " 'use': 6986,\n",
       " 'hour': 3441,\n",
       " 'sick': 5952,\n",
       " 'time': 6682,\n",
       " 'never': 4618,\n",
       " 'try': 6836,\n",
       " 'alone': 912,\n",
       " 'take': 6480,\n",
       " 'weight': 7208,\n",
       " 'tear': 6530,\n",
       " 'comes': 1879,\n",
       " 'ur': 6971,\n",
       " 'heart': 3303,\n",
       " 'falls': 2712,\n",
       " 'eyes': 2688,\n",
       " 'always': 925,\n",
       " 'remember': 5511,\n",
       " 'stupid': 6341,\n",
       " 'friend': 2954,\n",
       " 'share': 5876,\n",
       " 'bslvyl': 1498,\n",
       " 'raji': 5384,\n",
       " 'pls': 5098,\n",
       " 'favour': 2744,\n",
       " 'convey': 1963,\n",
       " 'birthday': 1334,\n",
       " 'wishes': 7293,\n",
       " 'nimya': 4645,\n",
       " 'today': 6717,\n",
       " 'iï½ï½ï½m': 3697,\n",
       " 'prob': 5260,\n",
       " 'hi': 3351,\n",
       " 'kate': 3819,\n",
       " 'lovely': 4122,\n",
       " 'see': 5798,\n",
       " 'tonight': 6748,\n",
       " 'ill': 3539,\n",
       " 'phone': 5029,\n",
       " 'tomorrow': 6737,\n",
       " 'got': 3141,\n",
       " 'sing': 5979,\n",
       " 'guy': 3218,\n",
       " 'gave': 3029,\n",
       " 'card': 1624,\n",
       " 'xxx': 7459,\n",
       " 'usual': 6998,\n",
       " 'iam': 3498,\n",
       " 'fine': 2804,\n",
       " 'happy': 3268,\n",
       " 'amp': 938,\n",
       " 'well': 7217,\n",
       " 'nope': 4683,\n",
       " 'watching': 7165,\n",
       " 'tv': 6859,\n",
       " 'home': 3396,\n",
       " 'going': 3112,\n",
       " 'bored': 1407,\n",
       " 'ps': 5309,\n",
       " 'grown': 3195,\n",
       " 'right': 5606,\n",
       " 'wat': 7160,\n",
       " 'tht': 6663,\n",
       " 'incident': 3566,\n",
       " 'awake': 1142,\n",
       " 'snow': 6076,\n",
       " 'shes': 5889,\n",
       " 'good': 3122,\n",
       " 'wondering': 7330,\n",
       " 'wont': 7332,\n",
       " 'say': 5743,\n",
       " 'smiling': 6053,\n",
       " 'coping': 1972,\n",
       " 'long': 4085,\n",
       " 'distance': 2310,\n",
       " 'intrepid': 3632,\n",
       " 'duo': 2443,\n",
       " 'great': 3176,\n",
       " 'soon': 6119,\n",
       " 'gobi': 3102,\n",
       " 'arts': 1055,\n",
       " 'college': 1868,\n",
       " 'credits': 2035,\n",
       " 'topped': 6764,\n",
       " 'http': 3461,\n",
       " 'wwwbubbletextcom': 7399,\n",
       " 'renewal': 5526,\n",
       " 'pin': 5057,\n",
       " 'tgxxrz': 6587,\n",
       " 'thought': 6645,\n",
       " 'put': 5336,\n",
       " 'back': 1173,\n",
       " 'box': 1422,\n",
       " 'wnt': 7320,\n",
       " 'buy': 1542,\n",
       " 'bmw': 1383,\n",
       " 'car': 1623,\n",
       " 'urgently': 6976,\n",
       " 'vry': 7098,\n",
       " 'urgentbut': 6975,\n",
       " 'hv': 3492,\n",
       " 'shortage': 5922,\n",
       " 'lt': 4141,\n",
       " 'gt': 3201,\n",
       " 'lacsthere': 3903,\n",
       " 'source': 6145,\n",
       " 'arng': 1040,\n",
       " 'dis': 2295,\n",
       " 'amt': 942,\n",
       " 'lacs': 3902,\n",
       " 'thats': 6602,\n",
       " 'mobile': 4427,\n",
       " 'added': 805,\n",
       " 'contact': 1949,\n",
       " 'list': 4039,\n",
       " 'wwwfullonsmscom': 7409,\n",
       " 'place': 5068,\n",
       " 'send': 5823,\n",
       " 'free': 2929,\n",
       " 'sms': 6061,\n",
       " 'people': 4990,\n",
       " 'visit': 7073,\n",
       " 'fullonsmscom': 2986,\n",
       " 'easy': 2467,\n",
       " 'ah': 864,\n",
       " 'sen': 5822,\n",
       " 'selected': 5810,\n",
       " 'means': 4298,\n",
       " 'know': 3876,\n",
       " 'lacking': 3901,\n",
       " 'particular': 4941,\n",
       " 'dramastorm': 2391,\n",
       " 'details': 2236,\n",
       " 'part': 4939,\n",
       " 'worried': 7349,\n",
       " 'kkhow': 3868,\n",
       " 'business': 1534,\n",
       " 'probably': 5261,\n",
       " 'earlier': 2454,\n",
       " 'station': 6258,\n",
       " 'think': 6627,\n",
       " 'sorry': 6128,\n",
       " 'meeting': 4313,\n",
       " 'later': 3937,\n",
       " 'forwarded': 2911,\n",
       " '21870000': 347,\n",
       " 'mailbox': 4208,\n",
       " 'messaging': 4348,\n",
       " 'alert': 897,\n",
       " '40': 464,\n",
       " 'matches': 4267,\n",
       " 'please': 5090,\n",
       " '09056242159': 156,\n",
       " 'retrieve': 5585,\n",
       " 'messages': 4346,\n",
       " 'cc100pmin': 1673,\n",
       " 'another': 964,\n",
       " 'job': 3749,\n",
       " 'one': 4803,\n",
       " 'hospital': 3431,\n",
       " 'data': 2128,\n",
       " 'analysis': 946,\n",
       " 'something': 6102,\n",
       " 'starts': 6251,\n",
       " 'monday': 4452,\n",
       " 'sure': 6416,\n",
       " 'thesis': 6618,\n",
       " 'finished': 2810,\n",
       " 'entry': 2567,\n",
       " 'gr8prizes': 3155,\n",
       " 'wkly': 7311,\n",
       " 'comp': 1891,\n",
       " 'chance': 1701,\n",
       " 'win': 7277,\n",
       " 'latest': 3938,\n",
       " 'nokia': 4671,\n",
       " '8800': 686,\n",
       " 'psp': 5311,\n",
       " 'ï½250': 7578,\n",
       " 'cash': 1651,\n",
       " 'every': 2616,\n",
       " 'wktxt': 7314,\n",
       " '80878': 637,\n",
       " 'httpwwwgr8prizescom': 3463,\n",
       " '08715705022': 113,\n",
       " 'youcarlos': 7518,\n",
       " 'isare': 3658,\n",
       " 'vibrate': 7044,\n",
       " 'acting': 791,\n",
       " 'might': 4363,\n",
       " 'hear': 3301,\n",
       " 'texts': 6585,\n",
       " 'smoking': 6059,\n",
       " 'wylie': 7442,\n",
       " 'smokes': 6057,\n",
       " 'much': 4512,\n",
       " 'justify': 3794,\n",
       " 'ruining': 5668,\n",
       " 'shit': 5903,\n",
       " 'new': 4619,\n",
       " 'video': 7048,\n",
       " 'phone750': 5030,\n",
       " 'anytime': 985,\n",
       " 'network': 4614,\n",
       " 'mins': 4380,\n",
       " '150': 282,\n",
       " 'text': 6575,\n",
       " 'five': 2829,\n",
       " 'pounds': 5181,\n",
       " 'per': 4992,\n",
       " 'week': 7199,\n",
       " '08000776320': 40,\n",
       " 'reply': 5541,\n",
       " 'delivery': 2198,\n",
       " 'sounds': 6142,\n",
       " 'kthen': 3888,\n",
       " 'special': 6159,\n",
       " 'number': 4723,\n",
       " 'live': 4048,\n",
       " '11': 250,\n",
       " 'oh': 4777,\n",
       " 'fuck': 2976,\n",
       " 'juswoke': 3795,\n",
       " 'bed': 1261,\n",
       " 'boatin': 1386,\n",
       " 'docks': 2326,\n",
       " 'slept': 6026,\n",
       " 'wid': 7262,\n",
       " '25': 360,\n",
       " 'year': 7483,\n",
       " 'old': 4795,\n",
       " 'spinout': 6184,\n",
       " 'giv': 3082,\n",
       " 'da': 2101,\n",
       " 'gossip': 3140,\n",
       " 'l8r': 3893,\n",
       " 'knock': 3874,\n",
       " 'txt': 6869,\n",
       " 'whose': 7257,\n",
       " '80082': 627,\n",
       " 'enter': 2557,\n",
       " 'weekly': 7203,\n",
       " 'draw': 2394,\n",
       " 'gift': 3070,\n",
       " 'voucher': 7093,\n",
       " 'store': 6299,\n",
       " 'yr': 7535,\n",
       " 'choice': 1780,\n",
       " 'cs': 2051,\n",
       " 'wwwtklscom': 7433,\n",
       " 'age16': 852,\n",
       " 'stoptxtstopï½150week': 6298,\n",
       " 'tessy': 6569,\n",
       " 'favor': 2742,\n",
       " 'dnt': 2322,\n",
       " 'forget': 2892,\n",
       " 'shijas': 5893,\n",
       " 'dice': 2254,\n",
       " 'art': 1053,\n",
       " 'class': 1807,\n",
       " 'thru': 6660,\n",
       " 'thanks': 6593,\n",
       " 'though': 6644,\n",
       " 'idea': 3517,\n",
       " 'come': 1877,\n",
       " 'persons': 5012,\n",
       " 'story': 6304,\n",
       " 'yeah': 7482,\n",
       " 'totes': 6777,\n",
       " 'wan': 7140,\n",
       " 'na': 4546,\n",
       " 'calling': 1590,\n",
       " 'lot': 4105,\n",
       " 'times': 6684,\n",
       " 'lil': 4015,\n",
       " 'busyi': 1537,\n",
       " 'noon': 4680,\n",
       " 'nt': 4714,\n",
       " 'paying': 4971,\n",
       " 'attention': 1105,\n",
       " 'morning': 4472,\n",
       " 'love': 4119,\n",
       " 'go': 3097,\n",
       " 'sleep': 6019,\n",
       " 'wish': 7292,\n",
       " 'day': 2136,\n",
       " 'full': 2985,\n",
       " 'feeling': 2757,\n",
       " 'better': 1302,\n",
       " 'opportunity': 4830,\n",
       " 'last': 3931,\n",
       " 'babe': 1164,\n",
       " 'kiss': 3862,\n",
       " 'drive': 2408,\n",
       " 'lor': 4099,\n",
       " 'huh': 3472,\n",
       " 'slow': 6038,\n",
       " 'tot': 6774,\n",
       " 'reach': 5418,\n",
       " 'ago': 860,\n",
       " 'liao': 3992,\n",
       " 'days': 2137,\n",
       " 'leh': 3976,\n",
       " 'dun': 2440,\n",
       " 'cut': 2090,\n",
       " 'short': 5921,\n",
       " 'like': 4009,\n",
       " 'failed': 2700,\n",
       " 'quite': 5360,\n",
       " 'sad': 5691,\n",
       " 'shopping': 5917,\n",
       " 'raining': 5380,\n",
       " 'mah': 4204,\n",
       " 'hard': 3269,\n",
       " 'leave': 3965,\n",
       " 'orchard': 4844,\n",
       " 'blame': 1348,\n",
       " 'life': 4000,\n",
       " 'give': 3083,\n",
       " 'happiness': 3267,\n",
       " 'bad': 1176,\n",
       " 'experience': 2669,\n",
       " 'essential': 2596,\n",
       " 'gods': 3105,\n",
       " 'blessings': 1359,\n",
       " 'ok': 4781,\n",
       " 'april': 1017,\n",
       " 'cant': 1615,\n",
       " 'wait': 7118,\n",
       " 'daddy': 2104,\n",
       " 'make': 4216,\n",
       " 'scream': 5770,\n",
       " 'pleasure': 5093,\n",
       " 'slap': 6017,\n",
       " 'ass': 1074,\n",
       " 'dick': 2255,\n",
       " 'yes': 7494,\n",
       " 'reg': 5490,\n",
       " 'ciao': 1793,\n",
       " '2nd': 396,\n",
       " 'tried': 6820,\n",
       " 'ï½1450': 7563,\n",
       " 'prize': 5257,\n",
       " 'claim': 1801,\n",
       " '09053750005': 155,\n",
       " 'b4': 1156,\n",
       " '310303': 427,\n",
       " 'csstop': 2056,\n",
       " '08718725756': 124,\n",
       " '140ppm': 279,\n",
       " 'told': 6729,\n",
       " 'gautham': 3027,\n",
       " 'someonone': 6097,\n",
       " 'trying': 6838,\n",
       " 'via': 7042,\n",
       " 'dating': 2132,\n",
       " 'service': 5842,\n",
       " 'find': 2802,\n",
       " 'could': 1993,\n",
       " 'landline': 3915,\n",
       " '09064015307': 191,\n",
       " 'box334sk38ch': 1429,\n",
       " 'lazy': 3952,\n",
       " 'type': 6880,\n",
       " 'forgot': 2896,\n",
       " 'ï½_': 7602,\n",
       " 'lect': 3968,\n",
       " 'saw': 5742,\n",
       " 'pouch': 5178,\n",
       " 'nice': 4631,\n",
       " 'man': 4224,\n",
       " 'bday': 1246,\n",
       " 'wife': 7265,\n",
       " 'didnt': 2258,\n",
       " 'parents': 4930,\n",
       " 'kids': 3850,\n",
       " 'went': 7222,\n",
       " 'work': 7340,\n",
       " 'even': 2610,\n",
       " 'colleagues': 1862,\n",
       " 'dearly': 2157,\n",
       " 'drop': 2414,\n",
       " 'tank': 6497,\n",
       " 'sunshine': 6398,\n",
       " 'hols': 3394,\n",
       " 'med': 4305,\n",
       " 'holiday': 3391,\n",
       " 'stamped': 6234,\n",
       " 'self': 5812,\n",
       " 'address': 809,\n",
       " 'envelope': 2571,\n",
       " 'drinks': 2406,\n",
       " 'us': 6983,\n",
       " 'uk': 6893,\n",
       " 'po': 5109,\n",
       " '113': 252,\n",
       " 'bray': 1449,\n",
       " 'wicklow': 7261,\n",
       " 'eire': 2508,\n",
       " 'quiz': 5363,\n",
       " 'saturday': 5735,\n",
       " 'unsub': 6945,\n",
       " 'stop': 6292,\n",
       " 'whats': 7240,\n",
       " 'indeed': 3580,\n",
       " 'way': 7172,\n",
       " 'either': 2509,\n",
       " 'bye': 1554,\n",
       " 'harish': 3274,\n",
       " 'rent': 5528,\n",
       " 'transfred': 6801,\n",
       " 'acnt': 785,\n",
       " 'urgent': 6974,\n",
       " '800': 623,\n",
       " 'flights': 2846,\n",
       " 'europe': 2606,\n",
       " 'away': 1145,\n",
       " '10th': 249,\n",
       " 'sept': 5836,\n",
       " '09050000555': 146,\n",
       " 'ba128nnfwfly150ppm': 1161,\n",
       " 'hide': 3353,\n",
       " 'anythiing': 982,\n",
       " 'keeping': 3827,\n",
       " 'met': 4352,\n",
       " 'stranger': 6310,\n",
       " 'choose': 1782,\n",
       " 'world': 7347,\n",
       " 'stands': 6239,\n",
       " 'friendship': 2958,\n",
       " 'ends': 2537,\n",
       " 'lets': 3989,\n",
       " 'friends': 2956,\n",
       " 'forever': 2889,\n",
       " 'gud': 3205,\n",
       " 'nitz': 4654,\n",
       " 'welcome': 7215,\n",
       " 'ukmobiledate': 6894,\n",
       " 'msg': 4498,\n",
       " 'giving': 3085,\n",
       " '08719839835': 140,\n",
       " 'future': 2998,\n",
       " 'mgs': 4355,\n",
       " 'billed': 1322,\n",
       " '150p': 283,\n",
       " 'daily': 2106,\n",
       " 'cancel': 1609,\n",
       " '89123': 700,\n",
       " 'hanks': 3256,\n",
       " 'lotsly': 4108,\n",
       " 'awarded': 1144,\n",
       " 'sipix': 5985,\n",
       " 'digital': 2274,\n",
       " 'camera': 1602,\n",
       " '09061221061': 174,\n",
       " 'within': 7300,\n",
       " '28days': 372,\n",
       " 'box177': 1425,\n",
       " 'm221bp': 4173,\n",
       " '2yr': 414,\n",
       " 'warranty': 7154,\n",
       " '150ppm': 295,\n",
       " '16': 304,\n",
       " 'pï½399': 5342,\n",
       " 'wlcome': 7315,\n",
       " 'wonder': 7328,\n",
       " 'eaten': 2469,\n",
       " 'lion': 4032,\n",
       " 'nothing': 4699,\n",
       " 'yup': 7543,\n",
       " 'lol': 4078,\n",
       " 'awesome': 1146,\n",
       " 'click': 1820,\n",
       " 'delete': 2191,\n",
       " 'current': 2080,\n",
       " 'food': 2877,\n",
       " 'also': 919,\n",
       " 'wipro': 7288,\n",
       " 'get': 3058,\n",
       " 'thing': 6625,\n",
       " 'years': 7484,\n",
       " 'gentle': 3051,\n",
       " 'princess': 5247,\n",
       " 'sweet': 6443,\n",
       " 'poem': 5127,\n",
       " 'dear': 2153,\n",
       " 'near': 4584,\n",
       " 'dont': 2361,\n",
       " 'fear': 2747,\n",
       " 'cheer': 1741,\n",
       " 'ni8': 4628,\n",
       " 'yay': 7479,\n",
       " 'girls': 3080,\n",
       " 'esplanade': 2594,\n",
       " 'else': 2520,\n",
       " 'hope': 3417,\n",
       " 'hmmmbut': 3380,\n",
       " 'texted': 6580,\n",
       " 'showered': 5937,\n",
       " 'erything': 2586,\n",
       " 'enjoy': 2548,\n",
       " 'playing': 5086,\n",
       " 'football': 2882,\n",
       " 'basketball': 1218,\n",
       " 'anything': 984,\n",
       " 'outdoors': 4867,\n",
       " 'double': 2374,\n",
       " 'txts': 6876,\n",
       " '6months': 585,\n",
       " 'bluetooth': 1378,\n",
       " 'orange': 4841,\n",
       " 'available': 1128,\n",
       " 'sony': 6116,\n",
       " 'motorola': 4481,\n",
       " 'phones': 5032,\n",
       " 'mobileupd8': 4431,\n",
       " '08000839402': 41,\n",
       " 'call2optoutn9dx': 1579,\n",
       " 'evening': 2611,\n",
       " 'roger': 5630,\n",
       " 'wahala': 7114,\n",
       " 'need': 4594,\n",
       " 'accidentally': 769,\n",
       " 'brought': 1489,\n",
       " 'em': 2522,\n",
       " 'aight': 875,\n",
       " 'comin': 1882,\n",
       " 'hey': 3346,\n",
       " 'tmr': 6709,\n",
       " 'maybe': 4289,\n",
       " 'meet': 4310,\n",
       " 'yck': 7480,\n",
       " 'lip': 4036,\n",
       " 'synced': 6461,\n",
       " 'shangela': 5871,\n",
       " 'needs': 4599,\n",
       " 'dat': 2127,\n",
       " 'slowly': 6040,\n",
       " 'vomit': 7088,\n",
       " 'lotr': 4106,\n",
       " 'sis': 5989,\n",
       " 'aft': 843,\n",
       " 'dinner': 2283,\n",
       " 'nite': 4649,\n",
       " 'finally': 2799,\n",
       " 'completed': 1902,\n",
       " 'course': 2003,\n",
       " 'whenwhere': 7245,\n",
       " 'pick': 5041,\n",
       " 'possible': 5166,\n",
       " 'dint': 2286,\n",
       " 'happened': 3261,\n",
       " 'switch': 6453,\n",
       " 'cell': 1684,\n",
       " 'whole': 7254,\n",
       " 'isnt': 3665,\n",
       " 'care': 1628,\n",
       " 'alrite': 917,\n",
       " 'hunny': 3480,\n",
       " 'wot': 7357,\n",
       " '2nite': 399,\n",
       " 'end': 2534,\n",
       " 'goin': 3110,\n",
       " 'town': 6784,\n",
       " 'jus': 3793,\n",
       " 'pub': 5315,\n",
       " 'instead': 3618,\n",
       " 'chillin': 1770,\n",
       " 'mo': 4423,\n",
       " 'bedroom': 1263,\n",
       " 'jen': 3734,\n",
       " 'wif': 7264,\n",
       " 'family': 2716,\n",
       " 'booking': 1399,\n",
       " 'tour': 6782,\n",
       " 'package': 4895,\n",
       " 'darren': 2124,\n",
       " 'saying': 5746,\n",
       " 'ge': 3040,\n",
       " 'den': 2203,\n",
       " 'cos': 1982,\n",
       " 'xy': 7466,\n",
       " 'feel': 2755,\n",
       " 'awkward': 1147,\n",
       " 'lunch': 4155,\n",
       " 'ten': 6555,\n",
       " 'shower': 5936,\n",
       " 'plan': 5073,\n",
       " 'coming': 1883,\n",
       " 'brum': 1496,\n",
       " 'putting': 5339,\n",
       " '6times': 590,\n",
       " 'may': 4287,\n",
       " 'dreams': 2398,\n",
       " 'true': 6830,\n",
       " 'update': 6955,\n",
       " 'weed': 7197,\n",
       " 'dealer': 2149,\n",
       " 'carlos': 1640,\n",
       " 'freedom': 2931,\n",
       " 'lunsford': 4156,\n",
       " 'im': 3542,\n",
       " 'realy': 5438,\n",
       " 'soz': 6148,\n",
       " 'imat': 3545,\n",
       " 'mums': 4523,\n",
       " '2moro': 388,\n",
       " 'ya': 7468,\n",
       " 'came': 1601,\n",
       " 'restocked': 5576,\n",
       " 'badrith': 1179,\n",
       " 'chennai': 1748,\n",
       " 'surely': 6417,\n",
       " 'competition': 1897,\n",
       " 'thk': 6636,\n",
       " 'oso': 4856,\n",
       " 'boring': 1409,\n",
       " 'uniform': 6926,\n",
       " 'ask': 1065,\n",
       " 'macho': 4189,\n",
       " 'budget': 1510,\n",
       " 'bb': 1231,\n",
       " 'bold': 1389,\n",
       " 'dollars': 2353,\n",
       " 'tuesday': 6851,\n",
       " 'freemsg': 2936,\n",
       " 'buffy': 1513,\n",
       " 'satisfy': 5732,\n",
       " 'men': 4331,\n",
       " 'randy': 5396,\n",
       " 'pix': 5064,\n",
       " 'qlynnbv': 5348,\n",
       " 'help08700621170150p': 3326,\n",
       " 'priest': 5243,\n",
       " 'joined': 3756,\n",
       " 'league': 3959,\n",
       " 'keep': 3826,\n",
       " 'touch': 6778,\n",
       " 'mean': 4296,\n",
       " 'deal': 2148,\n",
       " 'personal': 5008,\n",
       " 'cost': 1984,\n",
       " 'gain': 3004,\n",
       " 'rights': 5608,\n",
       " 'wifedont': 7266,\n",
       " 'demand': 2202,\n",
       " 'iti': 3676,\n",
       " 'husband': 3489,\n",
       " 'toolets': 6757,\n",
       " 'grumpy': 3199,\n",
       " 'mom': 4447,\n",
       " 'lying': 4168,\n",
       " 'play': 5081,\n",
       " 'jokes': 3762,\n",
       " 'oops': 4819,\n",
       " 'done': 2358,\n",
       " 'studyn': 6334,\n",
       " 'library': 3995,\n",
       " 'problem': 5262,\n",
       " 'baby': 1168,\n",
       " 'talk': 6488,\n",
       " 'called': 1584,\n",
       " 'left': 3970,\n",
       " 'message': 4343,\n",
       " 'yo': 7512,\n",
       " 'dude': 2433,\n",
       " 'guess': 3208,\n",
       " 'arrested': 1048,\n",
       " 'ha': 3225,\n",
       " 'watch': 7161,\n",
       " 'already': 915,\n",
       " 'haf': 3228,\n",
       " 'smth': 6066,\n",
       " 'mind': 4372,\n",
       " 'bomb': 1393,\n",
       " 'date': 2129,\n",
       " 'wanted': 7146,\n",
       " 'elaine': 2513,\n",
       " 'confirmed': 1931,\n",
       " 'kk': 3867,\n",
       " 'sister': 5990,\n",
       " 'talking': 6491,\n",
       " 'damn': 2107,\n",
       " 'poor': 5148,\n",
       " 'zac': 7545,\n",
       " 'stand': 6236,\n",
       " 'knows': 3879,\n",
       " 'wants': 7148,\n",
       " 'questions': 5356,\n",
       " 'aathi': 742,\n",
       " 'hello': 3319,\n",
       " 'request': 5549,\n",
       " 'rs5': 5657,\n",
       " 'transfered': 6799,\n",
       " 'derek': 2220,\n",
       " 'thank': 6592,\n",
       " 'wonderful': 7329,\n",
       " 'says': 5747,\n",
       " 'money': 4453,\n",
       " 'definitely': 2184,\n",
       " 'buying': 1545,\n",
       " 'significance': 5961,\n",
       " 'weekends': 7202,\n",
       " 'shows': 5941,\n",
       " 'ï½900': 7601,\n",
       " 'guaranteed': 3203,\n",
       " '09061701939': 177,\n",
       " 'code': 1853,\n",
       " 's89': 5684,\n",
       " 'valid': 7013,\n",
       " '12hrs': 270,\n",
       " 'meant': 4299,\n",
       " 'opposed': 4832,\n",
       " 'drunken': 2423,\n",
       " 'night': 4638,\n",
       " 'unless': 6936,\n",
       " 'book': 1397,\n",
       " 'kinda': 3857,\n",
       " 'joke': 3760,\n",
       " 'thet': 6619,\n",
       " 'really': 5437,\n",
       " 'looking': 4092,\n",
       " 'skinny': 6008,\n",
       " 'white': 7252,\n",
       " 'line': 4023,\n",
       " 'casting': 1658,\n",
       " 'look': 4089,\n",
       " 'pattern': 4965,\n",
       " 'recently': 5457,\n",
       " 'crap': 2020,\n",
       " 'prakesh': 5200,\n",
       " 'said': 5698,\n",
       " 'minute': 4383,\n",
       " 'sed': 5797,\n",
       " 'sexy': 5856,\n",
       " 'mood': 4467,\n",
       " 'minuts': 4385,\n",
       " 'latr': 3940,\n",
       " 'cake': 1563,\n",
       " 'speed': 6169,\n",
       " 'speedchat': 6170,\n",
       " '80155': 630,\n",
       " 'swap': 6437,\n",
       " 'chatter': 1726,\n",
       " 'chat80155': 1724,\n",
       " 'pobox36504w45wq': 5117,\n",
       " '150pmsg': 288,\n",
       " 'rcd': 5412,\n",
       " 'hoping': 3424,\n",
       " 'langport': 3920,\n",
       " 'still': 6280,\n",
       " 'office': 4766,\n",
       " 'site': 5993,\n",
       " 'simulate': 5976,\n",
       " 'test': 6570,\n",
       " 'gives': 3084,\n",
       " 'tough': 6780,\n",
       " 'readiness': 5426,\n",
       " 'likely': 4011,\n",
       " 'mittelschmertz': 4409,\n",
       " 'google': 3135,\n",
       " 'paracetamol': 4925,\n",
       " 'worry': 7351,\n",
       " 'dunno': 2442,\n",
       " 'lei': 3977,\n",
       " 'shd': 5882,\n",
       " 'driving': 2411,\n",
       " 'sch': 5755,\n",
       " 'hr': 3456,\n",
       " 'oni': 4806,\n",
       " 'confuses': 1934,\n",
       " 'things': 6626,\n",
       " 'doesnt': 2334,\n",
       " 'wrong': 7378,\n",
       " 'sort': 6130,\n",
       " 'invited': 3637,\n",
       " 'tho': 6641,\n",
       " 'nvm': 4733,\n",
       " 'wear': 7180,\n",
       " 'sport': 6206,\n",
       " 'shoes': 5912,\n",
       " 'anyway': 986,\n",
       " 'late': 3934,\n",
       " 'spending': 6177,\n",
       " 'brother': 1486,\n",
       " 'next': 4626,\n",
       " 'ready': 5428,\n",
       " 'spoiled': 6196,\n",
       " 'escape': 2589,\n",
       " 'theatre': 6609,\n",
       " 'kavalan': 3821,\n",
       " 'minutes': 4384,\n",
       " 'consensus': 1942,\n",
       " 'cool': 1969,\n",
       " 'let': 3988,\n",
       " 'kicks': 3847,\n",
       " 'around': 1043,\n",
       " 'waiti': 7120,\n",
       " 'min': 4369,\n",
       " 'typical': 6882,\n",
       " 'doctors': 2329,\n",
       " 'reminds': 5520,\n",
       " '2godid': 384,\n",
       " 'little': 4047,\n",
       " 'lounge': 4115,\n",
       " 'liked': 4010,\n",
       " 'photos': 5035,\n",
       " 'pity': 5063,\n",
       " 'suggestions': 6380,\n",
       " 'yet': 7504,\n",
       " 'easiest': 2462,\n",
       " 'barcelona': 1204,\n",
       " 'ru': 5661,\n",
       " 'house': 3444,\n",
       " 'looks': 4093,\n",
       " 'scrumptious': 5776,\n",
       " 'eat': 2468,\n",
       " 'ï½ï½ï½': 7610,\n",
       " 'entertaining': 2560,\n",
       " 'getting': 3064,\n",
       " 'hugh': 3470,\n",
       " 'laurie': 3945,\n",
       " 'stick': 6278,\n",
       " 'especially': 2593,\n",
       " 'apart': 989,\n",
       " 'yesterday': 7503,\n",
       " 'happen': 3259,\n",
       " 'silent': 5966,\n",
       " 'tensed': 6558,\n",
       " 'prabha': 5192,\n",
       " 'soryda': 6136,\n",
       " 'frm': 2961,\n",
       " 'sory': 6135,\n",
       " 'noe': 4666,\n",
       " 'la': 3894,\n",
       " 'wana': 7142,\n",
       " 'pei': 4984,\n",
       " 'bf': 1308,\n",
       " 'rite': 5618,\n",
       " 'round': 5646,\n",
       " 'til': 6679,\n",
       " 'ish': 3660,\n",
       " 'apologetic': 994,\n",
       " 'fallen': 2710,\n",
       " 'actin': 790,\n",
       " 'spoilt': 6197,\n",
       " 'child': 1762,\n",
       " 'caught': 1666,\n",
       " 'till': 6680,\n",
       " 'wo': 7321,\n",
       " 'badly': 1178,\n",
       " 'cheers': 1743,\n",
       " 'sir': 5987,\n",
       " 'receive': 5453,\n",
       " 'account': 777,\n",
       " '1hr': 318,\n",
       " 'delay': 2189,\n",
       " 'kare': 3816,\n",
       " 'month': 4462,\n",
       " 'upto': 6968,\n",
       " '50': 521,\n",
       " 'calls': 1593,\n",
       " 'standard': 6237,\n",
       " 'charge': 1712,\n",
       " 'activate': 794,\n",
       " '9061100010': 716,\n",
       " 'wire3net': 7289,\n",
       " '1st4terms': 322,\n",
       " 'pobox84': 5120,\n",
       " 'm26': 4175,\n",
       " '3uz': 460,\n",
       " 'ï½150': 7564,\n",
       " 'mobcudb': 4426,\n",
       " 'gim': 3074,\n",
       " 'caveboy': 1670,\n",
       " 'garage': 3018,\n",
       " 'keys': 3838,\n",
       " 'bookshelf': 1402,\n",
       " 'swhrt': 6448,\n",
       " 'dey': 2245,\n",
       " '2daylove': 378,\n",
       " 'misstake': 4401,\n",
       " 'mad': 4192,\n",
       " 'found': 2914,\n",
       " 'diff': 2268,\n",
       " 'farm': 2727,\n",
       " 'shop': 5915,\n",
       " 'cheese': 1745,\n",
       " 'gon': 3118,\n",
       " 'rip': 5616,\n",
       " 'uterus': 7000,\n",
       " 'rimac': 5610,\n",
       " 'access': 765,\n",
       " 'somewhere': 6108,\n",
       " 'beneath': 1292,\n",
       " 'pale': 4908,\n",
       " 'moon': 4468,\n",
       " 'light': 4006,\n",
       " 'someone': 6096,\n",
       " 'goodnite': 3130,\n",
       " 'meh': 4316,\n",
       " 'join': 3755,\n",
       " 'bus': 1531,\n",
       " 'ï½ï½': 7604,\n",
       " 'ericsson': 2577,\n",
       " 'oredi': 4847,\n",
       " 'lar': 3925,\n",
       " 'asked': 1067,\n",
       " 'search': 5781,\n",
       " 'login': 4071,\n",
       " 'todays': 6719,\n",
       " 'voda': 7082,\n",
       " 'numbers': 4726,\n",
       " 'ending': 2536,\n",
       " '5226': 528,\n",
       " '350': 434,\n",
       " 'award': 1143,\n",
       " 'hava': 3284,\n",
       " 'match': 4265,\n",
       " '08712300220': 87,\n",
       " 'quoting': 5368,\n",
       " '1131': 253,\n",
       " 'rates': 5402,\n",
       " 'app': 998,\n",
       " 'posh': 5159,\n",
       " 'birds': 1331,\n",
       " 'chaps': 1709,\n",
       " 'user': 6989,\n",
       " 'trial': 6819,\n",
       " 'prods': 5274,\n",
       " 'champneys': 1700,\n",
       " 'dob': 2323,\n",
       " 'asap': 1060,\n",
       " 'ta': 6466,\n",
       " 'toopray': 6759,\n",
       " 'meremove': 4339,\n",
       " 'teeth': 6538,\n",
       " 'painful': 4904,\n",
       " 'maintaining': 4214,\n",
       " 'stuff': 6335,\n",
       " 'remb': 5510,\n",
       " 'sat': 5727,\n",
       " 'infernal': 3591,\n",
       " 'affairs': 834,\n",
       " 'mayb': 4288,\n",
       " 'worrying': 7352,\n",
       " 'appt': 1015,\n",
       " 'shame': 5869,\n",
       " 'missed': 4396,\n",
       " 'quizzes': 5366,\n",
       " 'popcorn': 5151,\n",
       " 'hair': 3233,\n",
       " 'dad': 2103,\n",
       " 'boo': 1396,\n",
       " 'kerala': 3833,\n",
       " 'daysso': 2139,\n",
       " 'prepared': 5220,\n",
       " 'finalise': 2798,\n",
       " 'travel': 6806,\n",
       " 'visitneed': 7075,\n",
       " 'finish': 2808,\n",
       " 'works': 7346,\n",
       " '09066350750': 201,\n",
       " 'complimentary': 1907,\n",
       " 'ibiza': 3502,\n",
       " '10000': 236,\n",
       " 'await': 1140,\n",
       " 'collection': 1866,\n",
       " 'sae': 5692,\n",
       " '434': 477,\n",
       " 'sk3': 6003,\n",
       " '8wp': 714,\n",
       " 'ppm': 5189,\n",
       " '18': 308,\n",
       " 'understand': 6915,\n",
       " 'wine': 7281,\n",
       " 'slurp': 6042,\n",
       " 'takes': 6483,\n",
       " 'atyour': 1108,\n",
       " 'dead': 2147,\n",
       " 'midnight': 4361,\n",
       " 'earliest': 2456,\n",
       " 'chat': 1723,\n",
       " 'ax': 1150,\n",
       " 'figure': 2783,\n",
       " 'trust': 6833,\n",
       " 'math': 4270,\n",
       " 'promise': 5287,\n",
       " '447801259231': 481,\n",
       " 'secret': 5788,\n",
       " 'admirer': 814,\n",
       " 'ufind': 6886,\n",
       " 'reveal': 5591,\n",
       " 'thinks': 6631,\n",
       " 'specialcall': 6160,\n",
       " '09058094597': 164,\n",
       " 'cause': 1667,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;BernoulliNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.BernoulliNB.html\">?<span>Documentation for BernoulliNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>BernoulliNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bernoulli_nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712849964106246\n",
      "0.8837209302325582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multinomial_nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791816223977028\n",
      "0.9214092140921409\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters In Naive Bayes\n",
    "While Naive Bayes is often praised for its simplicity, it does have a few hyperparameters that can be tuned to improve performance.\n",
    "\n",
    "### Smoothing parameter ($\\alpha$)\n",
    "- Used in Laplace Smoothing to address the zero probability problem.\n",
    "- A small value (e.g., 1) is typically used to add a small amount of weight to each feature count, preventing zeros. Tuning this parameter can help balance model robustness (avoiding zero probabilities) and overfitting.\n",
    "\n",
    "### Feature selection\n",
    "- Naive Bayes assumes independence between features, which might not always be true.\n",
    "- Selecting a relevant subset of features that contribute most to classification can improve both bias and variance.\n",
    "- Techniques like chi-square tests, information gain, or feature importance analysis can help identify these features.\n",
    "\n",
    "### Model selection\n",
    "There are several variants of Naive Bayes, each suited for different data types,\n",
    "- Bernoulli NB: Suitable for binary features (presence or absence).\n",
    "- Multinomial NB: Suitable for features with multiple distinct values (frequency).\n",
    "- Gaussian NB: Suitable for continuous features (assumes a Gaussian distrinution).\n",
    "\n",
    "Choosing the appropriate variant based on the data's characteristics can significantly impact the performance.\n",
    "\n",
    "### Class priors\n",
    "- Naive Bayes uses class priors ($P(y = c)$) to represent the probability of each class appearing in the data.\n",
    "- By default, these are estimated based on the class frequency in the training data (uniform for balanced datasets).\n",
    "- In some cases, domain knowledge about the prior probabilities of differnt classes might be available.\n",
    "- Setting informative priors based on this knowledge can potentially improve classifcation accuracy.\n",
    "\n",
    "### Tuning techniques\n",
    "- Techniques like `GridSearchCV` or `RandomSearchCV` can be used to explore differnt hyperparameter combinations and identify the best performing configuration for the specific dataset.\n",
    "- Cross-validation is crucial to evaluate the model's performance on unseen data and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics To Evaluate Naive Bayes\n",
    "### Classification accuracy\n",
    "- The most basic metric, representing the proportion of correctly classified instances.\n",
    "- $\\text{Accuracy} = \\frac{Total number of correct predictions}{\\text{Total number of predictions}} = \\frac{TP + TN}{TP + FP + TN + FN}$.\n",
    "\n",
    "### Precision\n",
    "- Measures the proportion of positive predictions that were actually correct.\n",
    "- $\\text{Precision} = \\frac{TP}{TP + FP}$.\n",
    "- Useful for understanding how many of the model's positive predictions were relevant.\n",
    "\n",
    "### Recall\n",
    "- Measures the proportions of actual positive cases that were correctly classified.\n",
    "- $\\text{Recall} = \\frac{TP}{TP + FN}$.\n",
    "- Useful for understanding how many relevant cases the model captured.\n",
    "\n",
    "### F1-score\n",
    "- Harmonic mean of precision and recall, combining both metrics into a single score.\n",
    "- $\\text{F1-score} = \\frac{2 * \\text{Precision} * \\text{Recall}}{\\text{Precision} + \\text{Recall}}$.\n",
    "- Provides a balanced view of both precision and recall.\n",
    "\n",
    "### Confusion matrix\n",
    "- A visual representation of the model's performance on a classification task.\n",
    "- Rows represent actual classes, and columns represent predicted classes.\n",
    "- Values represent the number of instances in each category (True Positives, False Positives, True Negatives, False Negatives).\n",
    "\n",
    "### ROC curve (Receiving Operating Characteristics curve)\n",
    "- Plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at different classification thresholds.\n",
    "- A good model will have an ROC curve that stays close to the top-left corner, indicating high TPR and low FPR.\n",
    "\n",
    "### Choosing the right metric\n",
    "The choice of metric depends on the specific problem context.\n",
    "- Accuracy: A good starting point but can be misleading in imbalanced datasets.\n",
    "- Precision: Important when the cost of false positives is high (e.g., spam filtering).\n",
    "- Recall: Important when missing relevant cases is critical (e.g., medical diagnosis).\n",
    "- F1-score: A balanced view, useful when both precision and recall matter.\n",
    "\n",
    "### Additional considerations\n",
    "- Cross-validation: Evaluate the model's performance on unseen data using techniques like k-fold cross-validation.\n",
    "- Error analysis: Analyze misclassified instances to understand potential weaknesses of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-Off In Naive Bayes\n",
    "Naive Bayes inherently introduces some bias due to its core assumption of independence between features. This means it assumes that the presence of one feature does not affect the presence or absence of any other feature given the class label. While this assumption can be a good starting point, it may not always hold true for real world data, leading to underfitting for complex relationships.\n",
    "\n",
    "### Factors affecting the trade-off in Naive Bayes\n",
    "- Number of features: \n",
    "    - Too few features: Might lead to underfitting due to limited information for classification.\n",
    "    - Too many features: Can increase variance, especially if some features are irrelevant or redundant. Feature selection can help address this.\n",
    "- Smoothing techniques: Laplace Smoothing or other techniques help prevent zero probabilities for unseen words, reducing variance and improving model robustness.\n",
    "- Choice of model variant: Different variations of Naive Bayes (e.g., Gaussian Naive Bayes v. Multinomial Naive Bayes) are suited for different data types and can impact bias-variance. Choosing the appropriate variant can help mitigate some bias.\n",
    "\n",
    "### Strategies to improve the trade-off\n",
    "- Feature selection: Select the most relevant features that contribute significantly to classification. Techniques like information gain, chi-square tests, or feature importance analysis can help identify these features.\n",
    "- Smoothing techniques: As mentioned earlier, smoothing techniques like Laplace Smoothing can prevent zero probabilities and reduce variance.\n",
    "- Model selection and regularization: Explore variations of Naive Bayes that might handle specific data characteristics better (e.g., Gaussian Naive Bayes for continuous features). Regularization techniques can be used to penalize overly complex models and reduce variance.\n",
    "- Hyperparameter tuning: Tuning hyperparameters (e.g., smoothing parameter in Laplace smoothing) can help find the sweet spot between bias and variance. Techniques like `GridSearchCV` can be used for this.\n",
    "\n",
    "### Finding the optimal balance\n",
    "The goal is to find the sweet spot between bias and variance. The strategies mentioned above can help achieve this. Evaluating the model's performance on unseen data (e.g., using cross-validation) is crucial to ensure it generalizes well and avoids overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting And Overfitting In Naive Bayes\n",
    "### Underfitting\n",
    "- Symptoms:\n",
    "    - Poor performance on both, training and testing data.\n",
    "    - The model fails to capture the underlying patterns in the data.\n",
    "- Causes in Naive Bayes:\n",
    "    - Limited features: The model might not have enough features to adequately represent the data complexity.\n",
    "    - Overly strong independence assumption: The assumption of independence between features might be too strict, leading the model to miss important relationships.\n",
    "- Solutions:\n",
    "    - Feature engineering: Extract more features that capture relevant information from the data.\n",
    "    - Relaxing independence assumption: Explore alternative Naive Bayes variants (e.g., Kernel Naive Bayes) that allows for some feature dependencies.\n",
    "\n",
    "### Overfitting\n",
    "- Symptoms:\n",
    "    - High performance on the training data but poor performance on unseen data.\n",
    "    - The model memorized specific details of the training data that don't generalize well.\n",
    "- Causes in Naive Bayes:\n",
    "    - Too many features: Using irrelevant or noisy features can increase the model's complexity and lead to overfitting.\n",
    "    - Laplace Smoothing with high $\\alpha$: A high smoothing parameter might smooth out the data too much, leading to overfitting.\n",
    "- Solutions:\n",
    "    - Feature selection: Select only the most relevant features that contribute significantly to classification.\n",
    "    - Regularization: Techniques like L1 or L2 regularization can penalize overly complex models and reduce overfitting.\n",
    "    - Hyperparameter tuning: Optimize the smoothing parameter ($\\alpha$) in Laplace smoothing to avoid smoothing.\n",
    "    - Cross-validation: Evaluate the model on unseen data using techniques like K-Fold Cross Validation to ensure it generalizes well.\n",
    "\n",
    "### Strategies to prevent underfitting and overfitting\n",
    "- Feature engineering: Explore creating new features or transforming existing features to better represent the data.\n",
    "- Model selection: Consider different Naive Bayes variants or other classification algorithms based on the data characteristics.\n",
    "- Hyperparameter tuning: Tune hyperparameters like smoothing parameters and regularization coefficients to find the optimal balance.\n",
    "- Data augmentation (for text data): Techniques like synonym replacement or back translation can increase the training data size and reduce overfitting.\n",
    "- Early stopping: Stop training the model if validation performance starts to decline, preventing it from overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect Of Outliers On Naive Bayes\n",
    "Naive Bayes is susceptible to outliers in the data, which can negatively impact its performance. The following are ways in which outliers can impact the model,\n",
    "1. Zero probability problem:\n",
    "    - Outliers can be words or features rarely seen in the training data.\n",
    "    - Naive Bayes estimates the probability of each feature (word) appearing given a specific class.\n",
    "    - If an unseen word appears in a new sentence (outlier), its probability becomes 0.\n",
    "    - This can lead to the entire equation for that class becoming 0, making classifcation impossible.\n",
    "2. Skewed class distributions:\n",
    "    - Outliers can skew the distribution of features within a class.\n",
    "    - The model learns the class probabilities based on these skewed distributions.\n",
    "    - When encountering new data with outliers, the model might misclassify them due to the learned (inaccurate) class distributions.\n",
    "3. Impact on feature importance:\n",
    "    - Outliers might be assigned high importance based on their unique characteristics.\n",
    "    - This can mislead the model into giving undue weightage to these features during classification.\n",
    "    - The model might prioritize the outlier feature over more relevant but common features, leading to misclassification.\n",
    "\n",
    "### How to mitigate the effects of outliers?\n",
    "1. Data preprocessing:\n",
    "    - Identify and remove outliers using techniques like outlier detection algorithms (statistical methods, z-scores).\n",
    "    - Consider capping extreme values to a certain threshold.\n",
    "2. Feature engineering: Apply techniques like feature scaling or normalization to reduce the influence of outliers on the feature space.\n",
    "3. Robust estimators: Explore alternative Naive Bayes variants, like Lidstone Smoothing, that are less sensitive to outliers when estimating probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects Of Data Imbalance On Naive Bayes\n",
    "### Problem\n",
    "In an imbalanced dataset, one class (the majority class) has significantly more samples than the other classes. Naive Bayes relies on class probabilities for prediction, and with imbalance data, it can,\n",
    "- Overestimate the majority class probability: Due to the skewed distribution, the model might learn a higher prior probability for the majority class. This can lead to the model favoring the majority class during prediction, even for borderline cases.\n",
    "- Underestimate the minority class probability: With fewer examples, the model might struggle to learn accurate probabilities for the minority class. This can result in misclassifications, where minority class instances are incorrectly classified as the majority class.\n",
    "\n",
    "### Impact on performance\n",
    "- Reduced accuracy: The model might achieve high overall accuracy due to correctly classifying most majority class instances. However, its performance on the minority class will be likely poor, leading to a misleading overall picture.\n",
    "- Difficulties in evaluation: Standard metrics like accuracy becomes less informative in imbalanced datasets. Metrics like precision, recall, and F1-score for the minority class become crucial to understand its performance.\n",
    "\n",
    "### Mitigation strategies\n",
    "1. Data preprocessing techniques:\n",
    "    - Oversampling: Duplicate minority class instances to create a more balanced dataset. Techniques like SMOTE can be used for synthetic data generation.\n",
    "    - Undersampling: Randomly remove instances from the majority class to match the size of the minority class. However, this discards potentially valuable data.\n",
    "    - Cost-sensitive learning: Assign higher weights to misclassifications of the minority class during training, forcing the model to pay more attention to these instances.\n",
    "2. Model selection: Consider alternative classification algorithms that are less sensitive to data imbalance, such as Random Forest or Support Vector Machines (SVMs) with appropriate cost functions.\n",
    "3. Ensemble methods: Combine predictions from multiple Naive Bayes models trained on different resampled versions of the data (e.g., using oversampling and undersampling). This can improve overall performance and reduce bias towards the majority class.\n",
    "\n",
    "### Choosing the right approach\n",
    "The best approach to address data imbalance depends on the specific data and the cost of misclassification for each class. Carefully evaluate the impact of different techniques on the model's performance before deploying it in a real-world setting.\n",
    "\n",
    "### Additional considerations\n",
    "- Understanding domain knowledge: The domain knowledge about the relative importance of different classes can be incorporated into the model through the cost-sensitive learning or by prioritizing the minority class during evaluation.\n",
    "- Active learning: Query the user for labels on unlabeled data points, focusing on the majority class to improve the model's knowledge in that area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time And Space Complexity Of Naive Bayes\n",
    "### Time complexity\n",
    "- Training time complexity: $O(n * d)$. Where,\n",
    "    - $n$ = Number of training samples.\n",
    "    - $d$ = Number of features.\n",
    "- Explanation of training time complexity: The algorithm iterates over each training sample and updates the probability counts for each feature-class combination. This process is linear in the number of samples and features, leading to a time complexity of $O(n * d)$.\n",
    "- Testing time complexity: $O(d * c)$. Where,\n",
    "    - $d$ = Number of features.\n",
    "    - $c$ = Number of classes.\n",
    "- Explanation of testing time complexity: For each test instance, the algorithm calculates the probability of each class given the features. This involves iterating over each feature and looking up its probability for each class. This results in a time complexity of $O(d * c)$.\n",
    "\n",
    "### Space complexity\n",
    "- Space complexity: $O(d * c)$. To store the probability of each feature given a class.\n",
    "- Explanation of space complexity: The algorithm stores the probability of each feature given a class. This requires space proportional to the number of features and classes, leading to a space complexity of $O(d * c)$.\n",
    "\n",
    "### Key points\n",
    "- Naive Bayes is known for its simplicity and efficiency, especially in text classification tasks.\n",
    "- The low time and space complexity make it suitable for large datasets.\n",
    "- The assumption of feature independence can be a limitation, but it often works well in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
