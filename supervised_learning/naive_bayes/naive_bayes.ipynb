{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Naive Bayes is family of classification algorithms based on Bayes' theorem. It is a popular choice for various classification tasks due to its simplicity, efficiency, and interpretability.\n",
    "\n",
    "### Core principle\n",
    "Naive Bayes classifiers work under the assumption fo conditional independence between features (predictors) given the class label (target variable). In simpler terms, it assumes that knowing the value of one feature does not influence the probability of another feature's value, as long as the class label is already known. While this assumption is not always hold true in reality, it often works well in practive for many classification problems.\n",
    "\n",
    "### Classification process\n",
    "- Training: The model learns from the labeled dataset where each data point has features and a corresponding class label.\n",
    "- Prediction: For a new unseen data point, the model calculates the probability of it belonging to each class. It achieves this by,\n",
    "    1. Using Bayes' theorem to compute the posterior probability (probability of a class given the features).\n",
    "    2. Assuming conditional independence between features, which simplifies the calculations.\n",
    "    3. Multiplying the probabilities of each feature value given the class and multiplying by the prior probability of the class itself (learned from the training data).\n",
    "- Assigning class label: The class with the highest posterior probability is assigned as the predicted class for the new data point.\n",
    "\n",
    "### Example\n",
    "Imagine emails are being classified as spam or not spam based on features like наличиe слова \"деньги\" (presence of the word \"money\") and наличие восклицательных знаков (presence of exclamation marks). Naive Bayes would assume that the presence of \"money\" doesn't influence the presence of exclamation marks (and vice versa) given the email class (spam or not spam).\n",
    "\n",
    "### Advantages of Naive Bayes\n",
    "- Simplicity and efficiency: Naive Bayes is easy to understand and implement, making it a good choice for beginners. It's also computationally efficient for training and prediction.\n",
    "- Interpretability: The model allows to understand how each feature contributes to the classification by examining the feature probabilities for each class.\n",
    "- Performance: Naive Bayes can perform well for various classification tasks, especially when dealing with high-dimensional data (many features).\n",
    "\n",
    "### Disadvantage of Naive Bayes\n",
    "- Conditional independence assumption: The assumption of conditional independence between features might not always be valid, which can lead to suboptimal performance in some cases.\n",
    "- Sensitivity to features: Naive Bayes can be sensitive to irrelevant features or features with many unique values. Feature selection or preprocessing techniques might be necessary for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Bayes Theorem\n",
    "$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)}$\n",
    "\n",
    "Where,\n",
    "- $P(A|B)$ = Posterior. The probability of A being true, given B is true.\n",
    "- $P(B|A)$ = Likelihood. The probability of B being true, given A is true.\n",
    "- $P(A)$ = Prior. The probability of A being true. This is knowledge.\n",
    "- $P(B)$ = Marginalization. The probability of B being true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "### 1. Data preprocessing\n",
    "- Text cleaning:\n",
    "    - Remove punctuation, stop words (common words with little meaning), and potentially numbers depending on the task.\n",
    "    - Convert text to lowercase for consistency.\n",
    "    - Consider stemming or lemmatization (reducing words to base form) for improved accuracy (optional).\n",
    "- Feature representation: Represent each document (sentence) as a feature vector.\n",
    "- Common approaches:\n",
    "    - Bag-of-Words (BoW): Each word occurrence is a feature (0 or 1 indicating absence or presence).(Bernoulli NB).\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF): Weights words based on their importance within the document and rarity across the corpus. (Multinomial NB)\n",
    "\n",
    "### 2. Model training\n",
    "- Calculate class priors: Estimate the probability ($P(y = c)$) for each class ($c$) based on frequency in the training data.\n",
    "- Calculate conditional probabilities:\n",
    "    - Estimate the probability of each feature (word) appearing given a specific class ($P(w_i|y = c)$)\n",
    "    - Use techniques like Laplace Smoothing to avoid zero probabilities for unseen words (especially for multinomial NB).\n",
    "\n",
    "### 3. Classification of new sentence\n",
    "- Calculate posterior probability:\n",
    "    - Use the equation: $P(y = c | sentence) ≈ \\Pi(P(w_i | y = c)) * P(y = c)$.\n",
    "    - Multiply the probabilities of each word ($w_i$) appearing in the sentence given its class ($c$).\n",
    "    - Multiply by the prior probability of class ($c$).\n",
    "- Class prediction: Assign the sentence to the class with the highest posterior probability ($P(y = c | sentence)$).\n",
    "\n",
    "### Key points\n",
    "- Naive Bayes assumes independence between words in a document given the class label (simplification).\n",
    "- This assumption might not always hold true but offers computational efficiency and can be surprisingly effective.\n",
    "- Multinomial NB can capture word frequency information but requires managing the feature space size.\n",
    "- Laplace Smoothing helps address zero probabilities and improves model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
