{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 08:05:18 WARN Utils: Your hostname, Vidishs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.64 instead (on interface en0)\n",
      "24/11/19 08:05:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 08:05:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.64:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x108f95050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-20</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>62.330002</td>\n",
       "      <td>60.200001</td>\n",
       "      <td>60.840000</td>\n",
       "      <td>53.990601</td>\n",
       "      <td>17369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-21</td>\n",
       "      <td>60.980000</td>\n",
       "      <td>62.790001</td>\n",
       "      <td>60.910000</td>\n",
       "      <td>61.880001</td>\n",
       "      <td>54.913509</td>\n",
       "      <td>12089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>62.439999</td>\n",
       "      <td>63.259998</td>\n",
       "      <td>62.130001</td>\n",
       "      <td>62.689999</td>\n",
       "      <td>55.632324</td>\n",
       "      <td>9197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>62.779999</td>\n",
       "      <td>63.820000</td>\n",
       "      <td>62.549999</td>\n",
       "      <td>63.450001</td>\n",
       "      <td>56.306763</td>\n",
       "      <td>12823400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>63.360001</td>\n",
       "      <td>64.470001</td>\n",
       "      <td>63.259998</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>56.794834</td>\n",
       "      <td>9441200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2016-01-20  61.799999  62.330002  60.200001  60.840000  53.990601  17369100\n",
       "1  2016-01-21  60.980000  62.790001  60.910000  61.880001  54.913509  12089200\n",
       "2  2016-01-22  62.439999  63.259998  62.130001  62.689999  55.632324   9197500\n",
       "3  2016-01-25  62.779999  63.820000  62.549999  63.450001  56.306763  12823400\n",
       "4  2016-01-26  63.360001  64.470001  63.259998  64.000000  56.794834   9441200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pandas = pd.read_csv(\"WMT.csv\")\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1259 entries, 0 to 1258\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       1259 non-null   object \n",
      " 1   Open       1259 non-null   float64\n",
      " 2   High       1259 non-null   float64\n",
      " 3   Low        1259 non-null   float64\n",
      " 4   Close      1259 non-null   float64\n",
      " 5   Adj Close  1259 non-null   float64\n",
      " 6   Volume     1259 non-null   int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 69.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_pandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.date(2016, 1, 20), Open=61.799999, High=62.330002, Low=60.200001, Close=60.84, Adj Close=53.990601, Volume=17369100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"WMT.csv\", inferSchema = True, header = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|  Volume|\n",
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "|2016-01-20|61.799999|62.330002|60.200001|    60.84|53.990601|17369100|\n",
      "|2016-01-21|    60.98|62.790001|    60.91|61.880001|54.913509|12089200|\n",
      "|2016-01-22|62.439999|63.259998|62.130001|62.689999|55.632324| 9197500|\n",
      "|2016-01-25|62.779999|    63.82|62.549999|63.450001|56.306763|12823400|\n",
      "|2016-01-26|63.360001|64.470001|63.259998|     64.0|56.794834| 9441200|\n",
      "|2016-01-27|64.099998|    65.18|63.889999|63.950001|56.750477|10214300|\n",
      "|2016-01-28|64.029999|64.510002|    63.43|64.220001| 56.99007|11278300|\n",
      "|2016-01-29|    64.75|66.529999|64.739998|66.360001|58.889149|16439100|\n",
      "|2016-02-01|65.910004|    67.93|65.889999|     67.5| 59.90081|14728400|\n",
      "|2016-02-02|67.300003|67.839996|66.279999|66.860001|59.332867|13585900|\n",
      "|2016-02-03|67.309998|     67.5|    65.07|66.269997| 58.80928|12315600|\n",
      "|2016-02-04|65.760002|66.550003|65.010002|66.419998| 58.94239|12833400|\n",
      "|2016-02-05|66.860001|67.529999|65.879997|     67.0|  59.4571|14196500|\n",
      "|2016-02-08|     66.5|67.150002|65.160004|66.900002|59.368362|20743600|\n",
      "|2016-02-09|65.489998|66.410004|    64.68|65.809998|58.401058|14642400|\n",
      "|2016-02-10|66.190002|66.589996|65.650002|65.790001| 58.38332| 9709300|\n",
      "|2016-02-11|65.019997|65.760002|64.779999|    65.32|57.966228|11186700|\n",
      "|2016-02-12|65.519997|    66.25|64.870003|    66.18|58.729424| 9695500|\n",
      "|2016-02-16|66.610001|66.800003|     65.5|65.900002|58.480934|11360500|\n",
      "|2016-02-17|66.099998|66.610001|65.809998|66.110001|58.667301|12426700|\n",
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 08:06:15 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|            Open|              High|               Low|             Close|         Adj Close|           Volume|\n",
      "+-------+----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            1259|              1259|              1259|              1259|              1259|             1259|\n",
      "|   mean|96.5083001715647| 97.33101670611597| 95.74480543367744| 96.54861796902313| 92.42759966243042|8509255.043685464|\n",
      "| stddev|23.3274864439079|23.590538442427473|23.019708825126568|23.292869033013766|25.335281463390604|4760478.447370805|\n",
      "|    min|           60.98|         62.330002|         60.200001|             60.84|         53.990601|          2227400|\n",
      "|    max|      153.600006|        153.660004|        151.660004|        152.789993|        152.233536|         56233000|\n",
      "+-------+----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, Open: double, High: double, Low: double, Close: double, Adj Close: double, Volume: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(how = \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|  Volume|\n",
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "|2016-01-20|61.799999|62.330002|60.200001|    60.84|53.990601|17369100|\n",
      "|2016-01-21|    60.98|62.790001|    60.91|61.880001|54.913509|12089200|\n",
      "+----------+---------+---------+---------+---------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Close\"] < 62).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.64:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_obj = df.rdd\n",
    "\n",
    "# check the data type of the RDD\n",
    "type(rdd_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_obj.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_rdd = rdd_obj.repartition(4)\n",
    "new_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = list(range(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 μs, sys: 0 ns, total: 1 μs\n",
      "Wall time: 2.86 μs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "res = filter(lambda i: i % 2, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "RDD_example = sc.parallelize(numbers)\n",
    "print(RDD_example.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 μs, sys: 1 μs, total: 2 μs\n",
      "Wall time: 2.15 μs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "res2 = RDD_example.filter(lambda i: i % 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.6 ms ± 82.1 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "388 μs ± 109 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "data = {'col1': [i for i in range(10**6)],\n",
    "        'col2': [i+1 for i in range(10**6)],\n",
    "        'col3': ['A' if i % 2 == 0 else 'B' for i in range(10**6)]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Perform a simple filter operation\n",
    "pandas_filtered = df[df['col3'] == 'A']\n",
    "spark_filtered = spark_df.filter(spark_df.col3 == 'A')\n",
    "\n",
    "# Measure the time taken by Pandas DataFrame\n",
    "%timeit pandas_filtered = df[df['col3'] == 'A']\n",
    "\n",
    "# Measure the time taken by PySpark DataFrame\n",
    "%timeit spark_filtered = spark_df.filter(spark_df.col3 == 'A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
