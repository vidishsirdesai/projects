# What is Big Data?
Big Data refers to extremely large datasets that are too large or complex to be processed by traditional data processing tools. These datasets are characterized by the following 3 V's,
- Volume: The sheer amount of data is massive.
- Velocity: The data is generated at a high speed, often in real-time.
- Variety: The data comes in various formats, such as structured, unstructured, and semi-structured.

### Examples of Big Data
- Social media data: Posts, comments, likes, and shares generated by users.
- IoT data: Data collected from sensors and devices connected to the internet.
- Scientific data: Data generated from experiments, simulations, and observations.
- Financial data: Market data, transaction data, and customer data.

### Challenges of Big Data
- Storage: Storing large datasets requires specialized storage solutions.
- Processing: Processing Big Data requires powerful computing resources.
- Analysis: Analyzing Big Data can be complex due to its size, variety, and velocity.
- Security: Protecting sensitive data stored in Big Data systems is crucial.

### Technologies used for Big Data
- Hadoop: A distributed computing framework for processing large datasets.
- Spark: A fast and general-purpose data processing engine.
- NoSQL databases: Databases designed to handle large-scale, unstructured data.
- Data lakes: Centralized repositories for storing large volumes of data in its raw format.
- Machine learning: Algorithms used to discover patterns and insights in large datasets.

### Benefits of Big Data
- Improved decision-making: Big Data can provide valuable insights for businesses.
- Increased efficiency: Big Data can help automate processes and reduce costs.
- Enhanced customer experience: Big Data can be used to personalize products and services.
- Innovation: Big Data can drive innovation by creating new products and services.


# What is Apache Spark?
Apache Spark is an open-source analytics engine for large scale data (Big Data) processing. It is designed to be fast, general-purpose, and scalable, making it ideal for a variety of data-intensive workloads.

### Key features
- Speed: Spark is significantly faster than traditional data processing frameworks like Hadoop MapReduce due to its in-memory processing capabilities.
- Scalability: It can handle massive datasets and distributed processing across clusters of machine.
- Versatility: Spark supports a wide range of data processing tasks, including batch processing, real-time streaming, SQL analytics, machine learning, and graph processing.
- Language support: It offers APIs in multiple languages like Python, Scala, Java, and R, making it accessible to broad range of developers.
- Fault tolerance: Spark is designed to be resilient to failures, ensuring that data processing jobs can continue even if individual nodes in the cluster fail.   
- Integration: It integrates well with other popular big data technologies like Hadoop and Kafka.

### Common use cases for Apache Spark
- Data warehousing and analytics: Processing large datasets for reporting and analysis.   
- Machine learning: Training and deploying machine learning models on large datasets.   
- Real-time data processing: Analyzing streaming data for applications like fraud detection and recommendation systems.   
- Graph processing: Analyzing complex relationships between data points in graphs and networks.

### Why can't `pandas` be used to deal with Big Data?
While `pandas` is a powerful tool for data manipulation and analysis, it has limitations when dealing with extremely large datasets or Big Data. The following are some key reasons as to why `pandas` is not the best choice for Big Data,
1. In-memory processing: Pandas is designed to work with data that can fit into the memory of a single machine. For very large datasets, this can become a bottleneck as the data may not fit in memory.
2. Scalability: Pandas struggles to scale to distributed systems, making it difficult to process data across multiple machines.
3. Performance: While Pandas is efficient for smaller datasets, its performance can degrade as the data size increases, especially when performing complex operations.
4. Limited support for distributed computing: Pandas doesn't have built-in support for distributed computing frameworks like Hadoop or Spark, which are essential for processing Big Data.
5. Unoptimized for unstructured or semi-structured data: `pandas` is not optimized for unstructured or semi-structured data.

To address these limitations, it's often more suitable to use specialized Big Data tools like `pyspark` or `dask`.


# Apache Spark v. Pandas
Apache Spark and Pandas are both popular tools for data processing and analysis, but they serve different purposes and have distinct strengths.

### Apache Spark
- Designed for: Large scale, distributed data processing.
- Key features:
    - In-memory processing for speed.
    - Supports SQL, machine learning, graph processing, and streaming.
    - Can be used with various languages like Python, Scala, Java, and R.
    - Spark is built on top of Resilient distributed dataset (RDD) for fault tolerance. 
    - RDDs are immutable.
    - It can read data from multiple file systems (e.g. Hadoop file system, AWS file system, Local, etc,.).
    - It is compatible with other Big Data systems.
- Best suited for:
    - Processing massive datasets across clusters of machines.
    - Real-time data processing.
    - Machine learning tasks on large-scale data.

### Pandas
- Designed for: Data manipulation and analysis in Python.
- Key features:
    - Data structures like Series and DataFrames.
    - Powerful indexing and selection operations.
    - Built-in functions for data cleaning, transformation, and aggregation.
    - Integration with other Python libraries like NumPy and Matplotlib.
    - Pandas uses DataFrame as a building block.
    - DataFrames are mutable.
    - It cannot read data only from the local file system.
    - It is not compatible with other Big Data systems.
- Best suited for:
    - Smaller datasets that can fit in memory.
    - Data exploration and analysis.
    - Data cleaning and preprocessing.

### When to use which
- Spark:
    - When dealing with extremely large datasets that cannot fit in memory of a single machine.
    - When requiring distributed processing across multiple machines.
    - For real-time data processing or machine learning tasks on large-scale data.
- Pandas:
    - When working with smaller datasets that can fit in memory.
    - For data exploration, analysis, and cleaning tasks.
    - When needing to integrate with other Python libraries for visualization or statistical analysis.


# What is `pyspark`?
`pyspark` is a Python framework for Apache Spark. It provides an API that allows you to work with the powerful distributed computing capabilities of Spark using Python syntax. This makes it easier for Python developers to leverage Spark's ability to process large datasets in parallel across a cluster of machines.

### Key features
- DataFrames and RDDs: PySpark provides two primary data structures:
    - DataFrames: These are similar to tables in a relational database, providing a structured way to work with data.
    - Resilient Distributed Datasets (RDDs): These are collections of immutable, distributed data that can be operated on in parallel.
- SQL-like operations: PySpark supports SQL-like operations on DataFrames using the Spark SQL module.
- Machine learning: PySpark includes a machine learning library (MLlib) for building and training various machine learning models.
- Graph processing: PySpark's GraphFrames library allows you to work with graph-structured data.

### Why use `pyspark`?
- Scalability: PySpark can handle large datasets and distributed processing tasks efficiently.
- Ease of use: It provides a Python-friendly API, making it accessible to Python developers.
- Rich ecosystem: PySpark benefits from the extensive Python ecosystem, with numerous libraries and tools available for data analysis, visualization, and more.
- Integration with Spark: PySpark seamlessly integrates with the broader Spark ecosystem, allowing you to leverage other components like Spark Streaming and Spark SQL.


# What is Lazy Evaluation?
Lazy evaluation is a programming concept where expressions are evaluated only when their results are actually needed. This means that an expression's value is not calculated until it is used in a calculation or printed.

### Key characteristics
- Delayed evaluation: Expressions are not evaluated until their results are required.
- Efficiency: Lazy evaluation can improve performance by avoiding unnecessary calculations.
- Infinite data structures: Can be used to represent potentially infinite sequences or data structures without running out of memory.

### Benefits
- Improved performance: Can avoid unnecessary calculations, especially for large datasets or complex expressions.
- Simplified programming: Can make code more concise and easier to reason about.
- Infinite data structures: Allows for the representation of potentially infinite sequences without memory limitations.

### Drawbacks
- Debugging: Can make debugging more challenging, as the evaluation order may not be immediately apparent.
- Side effects: Can introduce unexpected side effects if expressions have side effects that are not evaluated until later.


# How does Apache Spark use Lazy Evaluation?
1. RDD Creation: When you create a Resilient Distributed Dataset (RDD) in Spark, you define a transformation that specifies how to compute the RDD's elements. However, Spark doesn't actually execute this transformation immediately.
2. Transformation Chain: As you apply more transformations to the RDD (e.g., mapping, filtering, reducing), Spark creates a lineage or dependency graph of these transformations. This graph represents a sequence of operations that need to be performed to compute the final RDD.
3. Action Trigger: Only when you perform an action on the RDD (e.g., collecting its elements, counting them, or saving them to disk), Spark will execute the entire lineage of transformations.
4. Optimization: Spark's execution engine will analyze the lineage graph and optimize it for efficient execution. This includes techniques like pipelining, where intermediate results are passed directly between transformations without materializing them on disk.

### Benefits of Lazy Evaluation in Spark
- Efficiency: Avoids unnecessary computations by only evaluating transformations when their results are needed.
- Fault tolerance: If a node in the cluster fails during a computation, Spark can recompute the affected RDD partitions based on the lineage graph.
- Scalability: Allows for efficient processing of large datasets across distributed clusters.
- Flexibility: Enables chaining of transformations and complex data pipelines.


# What is a RDD?
RDD (Resilient Distributed Dataset) is a fundamental data structure in Apache Spark. It represents a collection of immutable, distributed elements that can be operated on in parallel. RDDs are fault-tolerant, meaning that if a node in the cluster fails, Spark can automatically recompute the lost data.

### Key characteristics of RDDs
- Distributed: RDDs are partitioned and distributed across multiple nodes in a cluster.
- Immutable: Once created, RDDs cannot be modified. New RDDs are created through transformations.
- Fault-tolerant: Spark automatically recomputes lost partitions if a node fails.
- Lineage: RDDs maintain a lineage, which is a record of the transformations that were used to create it. This lineage allows Spark to recompute lost data efficiently.

### Operations on RDDs
- Transformations: Create new RDDs from existing ones. Examples include:
    - `map`: Applies a function to each element of the RDD.
    - `filter`: Filters elements based on a predicate.
    - `union`: Combines two RDDs.
    - `join`: Joins two RDDs based on a key.
- Actions: Trigger computations on RDDs and return results. Examples include:
    - `collect`: Returns all elements of the RDD as a list.
    - `count`: Returns the number of elements in the RDD.
    - `first`: Returns the first element of the RDD.
    - `saveAsTextFile`: Saves the RDD as a text file.

### RDDs v. DataFrame
While RDDs provide a low-level API for working with distributed data, Spark also provides a higher-level API based on DataFrames. DataFrames are structured collections of data that can be manipulated using SQL-like syntax. DataFrames are built on top of RDDs and offer a more convenient way to work with data in many cases.


# `SparkSession` v. `SparkContext`
SparkSession and SparkContext are both essential components of Apache Spark, but they serve different purposes and have different levels of abstraction.

### `SparkContext`
- Lower-level API: Provides direct access to the core functionality of Spark, including RDDs, transformations, and actions.
- Direct control: Offers granular control over Spark's execution and configuration.
- Historical significance: Was the primary entry point for Spark applications in earlier versions.

### `SparkSession`
- Higher-level API: Provides a unified interface for various Spark components, including SQL, DataFrame, and Dataset APIs.
- Simplified usage: Simplifies application development by encapsulating common tasks.
- Modern approach: Represents the preferred way to create Spark applications in recent versions.

### When to use which
- SparkContext:
    - If you need fine-grained control over Spark's execution or configuration.
    - For legacy applications that were built using SparkContext.
- SparkSession:
    - For most new Spark applications.
    - If you want to use the SQL, DataFrame, or Dataset APIs.
    - To simplify application development.


# Excercise
1. Install `pyspark` using the command, `pip install pyspark`.
2. Install Java. Link: https://www.java.com/en/download/manual.jsp.
3. Create a Jupyter Notebook with the name, `excercise.ipynb`.
4. Import the `SparkSession` class from `pyspark.sql` module, which is the entry point for interacting with Apache Spark SQL. It provides a unified interface for creating Spark SQL sessions, reading and writing data, and executing SQL queries. Code,
```Python
from pyspark.sql import SparkSession
```
5. Create an object `spark` of the `SparkSession` class. Code,
```Python
spark = SparkSession.builder.master("local[*]").getOrCreate()
```
- `SparkSession.builder` starts the builder for creating a SparkSession. `master("local[*]")` sets the master URL for the Spark application. The `local[*]` value indicates that the application should run in the local mode, using all available cores on the current machine. The `getOrCreate()` method tries to create a new SparkSession if one doesn't already exist. If a SparkSession already exists, it returns the existing instance.
6. Run a `curl` command in the terminal to download the data and save it in a `.csv` file. Code,
```Shell
curl https://raw.githubusercontent.com/markumreed/colab_pyspark/main/WMT.csv >> WMT.csv
```
7. Read the downloaded data into a Pandas Dataframe. Code,
```Python
import pandas as pd

df_pandas = pd.read_csv("WMT.csv")
df_pandas.head()

df_pandas.info()
```
8. Now read the data into a Spark DataFrame. Code,
```Python
df = spark.read.csv("WMT.csv", inferSchema = True, header = True)
df.head()
```
- `df = spark.read.csv(...)` radds a CSV file named `WMT.csv` into a Spark DataFrame object named `df`. `inferSchema = True`, tells Spark to automatically infer the data types of each column in the CSV file based on the data it samples. `header = True` specifies that the first row of the CSV file contains the column names.
9. The output of the above line of code is only the first row in the DataFrame. This is due to the concept of lazy evaluation used in Spark.
10. In order to display the DataFrame in Spark, the `show()` method is used. Code,
```Python
df.show()
```
11. The type of the DataFrame can be checked using the `type()` method. Code,
```Python
# Pandas DataFrame
type(df_pandas)

# Spark DataFrame
type(df)
```
12. The column names can be listed using `.columns`. Code,
```Python
df.columns
```
13. To view the schema of a Spark DataFrame, `printSchema()` method is used. Code,
```Python
df.printSchema()
```
14. To view the statistical description of a Spark DataFrame, `describe()` method can be used along with `show()`. Code,
```Python
df.describe().show()
```
15. The null values can be dropped using the code below,
```Python
df.na.drop(how = "all")
```
16. To filer the data based on a condition, the `filter()` method is used. Code,
```Python
df.filter(df["Close"] < 62).show()
```
17. Import the class `SparkContext`, and create an object named `sc` of the class `SparkContext`. Code,
```Python
from pyspark import SparkContext

sc = SparkContext.getOrCreate()
```
18. Convert the Spark DataFrame into an RDD. Code,
```Python
rdd_obj = df.rdd

# check the data type of the RDD
type(rdd_obj)
```
19. To extract the number of partitions, `getNumPartitions()` method is used. Code,
```Python
rdd_obj.getNumPartitions()
```
20. The data can be repartitioned using the method `repartition()`. Code,
```Python
new_rdd = rdd_obj.repartition(4)
new_rdd.getNumPartitions()
```
# Comparison between Apache Spark and Pandas,
1. Create a list of 1000000 numbers,
```Python
numbers = list(range(1000000))
```
2. Perform an operation to include only the odd numbers from the sequence. Code
```Python
%time
res = filter(lambda i: i % 2, numbers)
```
3. Create an RDD from the sequence and print the number of partitions inRDD. Code,
```Python
RDD_example = sc.parallelize(numbers)
print(RDD_example.getNumPartitions())
```
4. Apply the same filtering operation as before to compare Spark and Pandas. Code,
```Python
%time
res2 = RDD_example.filter(lambda i: i % 2)
```
5. The following is another take, but this time with a bigger data size. Code,
```Python
data = {'col1': [i for i in range(10**6)],
        'col2': [i+1 for i in range(10**6)],
        'col3': ['A' if i % 2 == 0 else 'B' for i in range(10**6)]}
df = pd.DataFrame(data)

# Convert Pandas DataFrame to PySpark DataFrame
spark_df = spark.createDataFrame(df)

# Perform a simple filter operation
pandas_filtered = df[df['col3'] == 'A']
spark_filtered = spark_df.filter(spark_df.col3 == 'A')

# Measure the time taken by Pandas DataFrame
%timeit pandas_filtered = df[df['col3'] == 'A']

# Measure the time taken by PySpark DataFrame
%timeit spark_filtered = spark_df.filter(spark_df.col3 == 'A')
```