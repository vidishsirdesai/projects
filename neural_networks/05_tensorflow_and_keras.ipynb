{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Case: healthyfi.me\n",
    "healthyfi.me has labelled its customers based on the highest intensity of work they could perform, namely, `A`, `B`, `C` and `D`.\n",
    "\n",
    "Given a new customer, help healthyfi.me in recommending customized workout plans by predicting the customer's class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dictionary\n",
    "| id | features | description |\n",
    "| :-: | :-: | :-: |\n",
    "| 01 | `age` | age of the customer |\n",
    "| 02 | `gender` | gender of the customer (M/F) |\n",
    "| 03 | `height_cm` | height of the customer in cm |\n",
    "| 04 | `weight_kg` | weight of the customer in kg |\n",
    "| 05 | `body_fat_`% | % of fat in customer's body |\n",
    "| 06 | `diastolic` | diastolic blood pressure, measures the pressure in your arteries when your heart rests between beats |\n",
    "| 07 | `systolic` | systolic blood pressure, measures the pressure in your arteries when your heart beats |\n",
    "| 08 | `gripForce` | strength of customer's grip |\n",
    "| 09 | `sit and bend forward_cm` | to measure flexibility of customer |\n",
    "| 10 | `sit-ups counts` | count of sit-ups customer can perform |\n",
    "| 11 | `broad jump_cm` | It is the max jump customer can perform in cm |\n",
    "| 12 | `class` | Category of customer based on the intensity of workout |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"healthyfime.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from the above snippet of the data,\n",
    "- The `age` feature is very crucial in predicting the class of highest intensity workout.\n",
    "- Features like, `height`, `weight`, `diastolic`, etc are crucial in predicting the workout intensity. The reason: A high intensity cannot be predicted to candidates with high blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "- `gender` has 2 unique values and `class` has 4 unique values.\n",
    "- Binary encoding (`0` and `1`) can be applied on `gender`.\n",
    "- Since `class` is ordinal in nature, label encoding (`0`, `1`, `2`, `3`) can be applied on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"gender\"].replace({\"M\": 0, \"F\": 1}, inplace = True)\n",
    "# df.head()\n",
    "df.replace({\"M\": 0, \"F\": 1}, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(df[\"class\"].unique())\n",
    "mapping_dict = {ch: i for i, ch in enumerate(sorted(classes, reverse = True))}\n",
    "print(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].replace(mapping_dict, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above step,\n",
    "- All the strings are mapped to the integers in a reversed manner, i.e., `D` is mapped to `0` and `A` is mapped to `3`.\n",
    "- Then all the values in the `class` features are replaced with the mapped values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "sns.heatmap(df.corr(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no storng correlations between any of the features.\n",
    "\n",
    "A Boxplot can be used to understand the range and distribution of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "df.boxplot(rot = 0, vert = False)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating The Independent And Dependent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns = [\"class\"])\n",
    "y = df[\"class\"]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split The Data Into Train, Test And Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state = 42)\n",
    "\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling The Data Using `StandardScaler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train, y_train)\n",
    "x_val = scaler.fit_transform(x_val)\n",
    "x_test = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "pd.DataFrame(x_train, columns = df.columns[:-1]).boxplot(rot = 0, vert = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is ready for model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# checking the version of tensorflow\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow?\n",
    "TensorFlow (`tensorflow`) is a powerflow open-source library developed by Google for Machine Learning and Artificial Intelligence. It is designed to make it easier to build and train complex Neural Networks\n",
    "\n",
    "Key features:\n",
    "- Data flow graphs: TensorFlow uses a computational graph approach, where nodes represent mathematical operations and edges represent the data (tensors) flowing between them. This allows for efficient execution on various hardware platforms.\n",
    "- Flexible architecture: It can run on a single device (CPU or GPU) or be scaled to distributed systems with multiple GPUs or TPUs.\n",
    "- High-level API (Keras (`keras`)): Keras, integrated into TensorFlow, provides a user-friendly interface for building and training models, making it accessible to both beginners and experienced developers.\n",
    "- Low-level API (TensorFlow Core): For advanced users, TensorFlow Core offers fine-grained controls over the underlying computations, allowing for custom optimizations and experimentation.\n",
    "- Wide range of applications: TensorFlow is used for various tasks, including image recognition, natural language processing, speech recognition and more.\n",
    "\n",
    "TensorFlow modules,\n",
    "- `tf.keras`\n",
    "- `tf.data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tf.keras`\n",
    "\n",
    "### What is Keras?\n",
    "Keras is a high-level Neural Network API that runs on top of TensorFlow (and other backends like Theano and CNTK). It is designed to be user-friendly and flexible, making it a popular choice for both beginners and experienced Machine Learning practitioners.\n",
    "\n",
    "Key features,\n",
    "- User-friendly API: Kerasa provides a simple and intuitive API, making it easy to build and train Neural Networks.\n",
    "- Modularity: Keras is built on the principle of modularity, which allows to easily combine different componenta (layers, optimizers, etc) to create complex models.\n",
    "- Flexibility: While it offers a high-level API, Keras also provides access to the underlying TensorFlow backend for more advanced customization.\n",
    "- Rapid prototyping: Keras is designed to enable rapid experimentation and model development.\n",
    "- Cross-platform compatibility: Keras can run on various hardware platforms including CPUs, GPU and TPUs.\n",
    "\n",
    "### How is it different from TensorFlow?\n",
    "While both Keras and TensorFlow are powerful tools for machine learning, they differ in their level of abstraction and complexity.\n",
    "\n",
    "TensorFlow,\n",
    "- Low-level API: It provides a low-level API, giving you fine-grained control over the underlying computations. This allows for highly customized and optimized models.   \n",
    "- Flexibility: You can build complex and custom architectures, but it requires a deeper understanding of machine learning concepts and TensorFlow's framework.   \n",
    "- Scalability: TensorFlow is designed to handle large-scale datasets and complex models, making it suitable for research and production environments.\n",
    "\n",
    "Keras,\n",
    "- High-level API: It offers a simpler and more user-friendly API, making it easier to build and train models quickly.   \n",
    "- Ease of use: Keras abstracts away many of the complexities of deep learning, allowing you to focus on model architecture and training.   \n",
    "- Rapid prototyping: It's ideal for rapid prototyping and experimentation.\n",
    "\n",
    "In essence, Keras is built on top of TensorFlow. It provides a higher-level interface that simplifies the process of building and training neural networks. You can think of Keras as a user-friendly wrapper around TensorFlow.\n",
    "\n",
    "When to use which,\n",
    "- Keras: Ideal for beginners and rapid prototyping.   \n",
    "- TensorFlow: Suitable for advanced users who need more control over the underlying computations and for large-scale, complex models.\n",
    "\n",
    "### Advantages of Keras in TensorFlow\n",
    "- Simplified API: Keras offers a user-friendly API that significantly reduces the complexity of building and training deep learning models. It is designed to be intuitive and easy to learn even for beginners.\n",
    "- Seamless integration: Keras is now directly integrated into TensorFlow 2, eliminating the need for separate installation. This streamlined process makes it even more accessible to developers.\n",
    "- Rapid prototyping: Keras' high-level abstractions enable rapid experimentation and model development, which allows to quickly iterate on ideas and explore different architectures.\n",
    "- Flexibility: While Keras provides a simple interface for common tasks, it also offers flexibility for advanced users who need to customize their models or delve into the underlying TensorFlow operations.\n",
    "- Broad community support: Keras has a large and active community, providing extensive documentation, tutorials and forums for support and knowledge sharing.\n",
    "\n",
    "The following are the various activation functions available inside Keras,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tf.keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tf.keras.activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tf.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 ways using which code can be written in Keras,\n",
    "1. Sequential API.\n",
    "2. Functional API.\n",
    "\n",
    "For the sake of this document, the Seqential API is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Sequential API\n",
    "The Sequential API in Keras is a straightforward way to define a neural network model by stacking layers sequentially. It's ideal for simple, linear models where the output of one layer feeds directly into the input of the next.\n",
    "\n",
    "### Limitations of Sequential API:\n",
    "While the Sequential API is simple and easy to use, it's not suitable for complex architectures with shared layers, multiple inputs, or multiple outputs. For these cases, you'll need to use the more flexible Functional API.\n",
    "\n",
    "The following is done in the code lines below,\n",
    "- The `Sequential` class from `tnsorflow.keras.models` is imported in which the layers of the Neural Network are sequentially positioned.\n",
    "- The `Dense` layer is also imported from `tensorflow.keras.layers`. A `Dense` layer helps in defining one layer of a feed forward Neural Network. In a `Dense` layer, every neuron is connected to every neuron in the preceding layer, hence the name \"dense.\" This ensures that each neuron considers the input from all previous neurons.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_57.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is done in the code lines below,\n",
    "- An instance (object) of the `Sequential` class is created with the name `model`.\n",
    "- It is same as creating an instance as was done in `sklearn`, just that, in this case, the model is defined as well.\n",
    "- `Sequential` model will take a list of layers as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a feed forward network with a single hidden layer\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = \"relu\"), # hidden dense layer with 64 neuron units\n",
    "        Dense(4, activation = \"softmax\") # output layer wit 4 units and softmax activation\n",
    "    ]\n",
    ")\n",
    "# the activation argument is optional, if it is not passed, then there will linear or no activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is activation function needed?\n",
    "It provides non-linearity to problems.\n",
    "\n",
    "### In what case, an activation function will not be passed?\n",
    "Output layer of regression model.\n",
    "\n",
    "### How to find if there is a method to check the weights of a model?\n",
    "`dir(model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output means that the model has not created weights yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input shape\n",
    "- Since the model does not know about the input size yet, tensorflow does not have any information to create weights and biase yet.\n",
    "- the input size can be passed as an argument in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = \"relu\", input_shape = (11, )),\n",
    "        Dense(4, activation = \"softmax\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code,\n",
    "- 1 training example will be a 11 dimensional feature vector `(11, )`.\n",
    "- For each of the feature vector from the input, there will be an output of 64-dimensional feature vector `(64, )`.\n",
    "- In case of a `Dense` layer, $y = x * w + b$, where $x$ has all the features as columns.\n",
    "- The output of the first layer will have the dimensions `(m, 64)`, where `m` is the number of input data samples.\n",
    "\n",
    "### Why were weights not defined in second `Dense` layer?\n",
    "The layers in the sequential model interact with each other. Therefore, there is no need to define the input shape for all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights will be created now\n",
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.weights:\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below shows how the model's outputs look like for each layer,\n",
    "- For the first layer, if an observation of dimension `1x11` is passed then an output of dimension `1x64` will be generated. Similarly it will be done for `m` observations.\n",
    "- For the second layer, when an input of dimension `(1, 64)` is passed then an output of dimension `1x4` will be generated. Similarly it will be done for `m` observations.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_58.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way to define the same model using `model.add()`\n",
    "Instead of passing the list of layers as an argument will creating a model instance, the `add()` method can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation = \"relu\", input_shape = (11, )))\n",
    "model.add(Dense(4, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to choose (list of layers or `add()`)?\n",
    "This is a choice that is left to the user. If the model depends on a condition or may add some $x$ number of similar layers in the loop, `model.add` can be used. The condition can be checked using `if` and layer to the model can be added using `add()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if the input had been a multi-dimensional data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "model_2d = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape = (28, 28)), # flatten the data to make it (784, ) to be based further\n",
    "        Dense(64, activation = \"relu\"),\n",
    "        Dense(4, activation = \"softmax\")\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Multi-dimensional input can also be passed directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model summary\n",
    "There is another short way to check the dimensions and parameters of each layer. This can be done by printing the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are number of parameters computed in the above summary?\n",
    "- Input is of shape `11 * (batch_size)` which is densly connected to 64 Neurons. So parameters are computed as, `(11 * 64) weights + 64 biases = 768`.\n",
    "- Similarly, for the second layer, `4 * 64 + 4 = 260`\n",
    "\n",
    "What does `None` represent in the above model's `Output Shape`?\n",
    "- None makes the model capable of handling the multiple points.\n",
    "- In the image seen above, where the dimension were being calculated, the dimensions were were calculated for a single example.\n",
    "- But in an ideal scenario, there will be multiple points aand the number of points were not defined while defining the model. Therefore, the model is keeping it `None` for handling multiple number of passed observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning custom names to the layers\n",
    "As seen in the model summary, Keras has assigned the names by itself. At times, there may be need to assign custome names to the layers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = \"relu\", input_shape = (11, ), name = \"hidden_1\"),\n",
    "        Dense(4, activation = \"softmax\", name = \"output\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can also be plotted as a graph and can be saved as a `.png` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(\n",
    "    model,\n",
    "    to_file = \"model.png\",\n",
    "    show_shapes = True,\n",
    "    show_layer_names = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it is observed that the first layer is the input layer and then the input and output shapes of each layers `hidden_1` and `output` are given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and bias initializer\n",
    "When Neural Network was implemented from scratch, $w$ was initialized with `np.ranfdom.randn` and $b$ was initialized with `np.zeros`.\n",
    "\n",
    "The following are the various weight initialization techniques,\n",
    "- Glorot Normal: $w_{ij}^k \\sim N(0,\\sigma_{ij}). \\text{Where, } \\sigma_{ij} = \\frac{2}{fanin+fanout}$.\n",
    "- Glorot Uniform:  $w_{ij}^k \\sim \\text{Uniform }\\bigg[ \\frac{-\\sqrt{6}}{\\sqrt{fanin+fanout}}, \\frac{\\sqrt{6}}{\\sqrt{fanin+fanout}}\\bigg]$.\n",
    "- He Normal:  $N(0,\\sigma), \\text{Where, } \\sigma = \\frac{2}{fanin}$\n",
    "- He Uniform:  $\\text{Uniform }\\bigg[ \\frac{-\\sqrt{6}}{\\sqrt{fanin}}, \\frac{\\sqrt{6}}{\\sqrt{fanin}}\\bigg]$\n",
    "\n",
    "The end results of the classification and regression get affected by the initialization of the weights and biases.\n",
    "\n",
    "The following is how Keras implements them,\n",
    "- In the `Dense` layer of Keras,\n",
    "    1. The biases are set to 0 (`zeros`) by default.\n",
    "    2. The weights are set according to `glorot_uniform` (Glorot Uniform initializer) by default.\n",
    "- For example,\n",
    "    - $c = \\frac{\\sqrt{6}}{\\sqrt{11+64}} = 0.28$. For the first hidden layer of the model, $fanin$ (input) is 11 and $fanout$ (output) is 64.\n",
    "\n",
    "Note,\n",
    "- There are several researches proposing different ways of randomly initializing the weights of the layers.\n",
    "- But `glorot_uniform` has been most widely used one in most of the Deep Learning frameworks today.\n",
    "\n",
    "### What if there is need to custom initialize the weights and biases (maybe for research)?\n",
    "Each layer has optional arguments `kernel_initializer` and `bias_initializer` to set the weights and biases respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_x = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = \"relu\", input_shape = (11, ), name = \"hidden_1\", kernel_initializer = \"random_uniform\", bias_initializer = \"zeros\"),\n",
    "        Dense(4, activation = \"softmax\", name = \"output\", kernel_initializer = \"he_uniform\", bias_initializer = \"ones\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scratch approach can also be opted for by using `keras.initializer` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense(\n",
    "    64,\n",
    "    kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0.0, stddev = 0.05),\n",
    "    bias_initializer = tf.keras.initializers.Constant(value = 0.4),\n",
    "    activation = \"relu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers of the model can be retrieved as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the histograms of initialized weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize = (5, 5))\n",
    "fig.subplots_adjust(hspace = 0.5, wspace = 0.5)\n",
    "# extracting weights from the layers\n",
    "weight_layers = [layer for layer in model.layers]\n",
    "for i, layer in enumerate(weight_layers):\n",
    "    for j in [0, 1]:\n",
    "        axes[i, j].hist(layer.weights[j].numpy().flatten(), align = \"left\")\n",
    "        axes[i, j].set_title(layer.weights[j].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.layers` consists a list of layers.\n",
    "\n",
    "In the code, the weights and biases of all the layers are extracted using `layer.weights`, they are then converted to a `numpy` array and are then flattened to just get an array of initialized weights and biases.\n",
    "\n",
    "It is observed from the plot that all the biases are initialized to `0` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Compilation: Loss And Optimizer\n",
    "So far, the model's architecture has been defined. Now, the model has to be compiled.\n",
    "\n",
    "### What specific information should be passed to the model while compiling it?\n",
    "1. Loss function: To measure the model's performance as it trains.\n",
    "2. Optimizer (like gradient descent): To perform the gradient update.\n",
    "\n",
    "The above is done using 2 arguments of the `compile()` method, `optimizer` and `loss`.\n",
    "\n",
    "The following is an example of a binary classification,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_binary = Sequential(\n",
    "    [\n",
    "        Dense(64, activation = \"relu\", input_shape = (11, )),\n",
    "        Dense(1, activation = \"sigmoid\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_binary.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \" binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple options for optimizers, loss and metrics (check further documentation).\n",
    "\n",
    "All the strings which have been passed as arguments i.e., \"`adam`\", \"`binary_crossentropy`\" and \"`accuracy`\" are reference to some default objects defined in Keras.\n",
    "\n",
    "These custom objects can also be directly instantiated with the classes defined in Keras submodules for example `opt = keras.optimizers.Adam(learning_rate = 0.01)`.\n",
    "\n",
    "Customized loss and optimizer functions can be passed in Keras models.\n",
    "\n",
    "In the following code lines, the learning rate is changed by initializing a custom object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, a list of metrics can be defined which could be tracked during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = \"relu\", input_shape = (11, ), name = \"hidden_1\"),\n",
    "        Dense(8, activation = \"relu\", name = \"hidden_2\"),\n",
    "        Dense(4, activation = \"softmax\", name = \"output\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is sparse categorical cross entropy different from categorical cross entropy?\n",
    "1. Use `categorical_crossentropy` if target is one hot-encoded, for example, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1].\n",
    "2. Use `sparse_categorical_crossentropy` if target vector is ordinal integer values, 0, 1, 2, 3.\n",
    "\n",
    "These metrics will be calculated and saved after each epoch.\n",
    "\n",
    "### What is an epoch?\n",
    "When the data size is too big for the memory (RAM), the data is passed in small batches instead of one big batch. Each pass of these small batches is called an iteration.\n",
    "\n",
    "Each pass of the whole dataset is called an epoch.\n",
    "\n",
    "1 epoch means that each sample in the training dataset has had an opportuinity to update the internal model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to check if the loss and optimizers for the model are set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Using `fit()`\n",
    "Model training involves updating the weights using the optimizer and loss functions on the dataset.\n",
    "\n",
    "`model.fit(x_train, y_train)`\n",
    "\n",
    "Where,\n",
    "- `x_train` = `(num_samples, num_features)`.\n",
    "- `y_train` = `(num_samples, num_classes)` or `(num_samples, )`.\n",
    "\n",
    "### Arguments\n",
    "- Number of epochs: \n",
    "    - `model.fit(x_train, y_train, epochs = 500)`.\n",
    "- Batch size:\n",
    "    - `model.fit(x_train, y_train, batch_size = 256)`.\n",
    "    - Usually the batch size set is in the power of 2 (i.e., 2, 4, 8, 16, 32, 64, ...).\n",
    "    - `batch_size = 16`, means 16 training samples are passed in each iteration.\n",
    "    - Number of iterations in an epoch = Number of samples/ Batch size.\n",
    "- `validation_split = 0.1` means that 10% of the training data will be used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seed in Machine Learning is the initial state of a pseudo-random number generator.\n",
    "\n",
    "Setting a specific seed ensures a deterministic sequence of numbers generated by the `random()` method.\n",
    "\n",
    "Using the same seed guarantees identical weight initialization patterns, leading to consistent behavior across multiple runs of the same code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training a model for a few epochs\n",
    "model.fit(x_train, y_train, epochs = 10, batch_size = 256, validation_split = 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has been trained for 10 epochs.\n",
    "\n",
    "Observe that `model.fit` is printing all the metrics like accuracy, loss, validation loss, validation accuracy, etc.\n",
    "\n",
    "### How can all this information be used for analyzing the training process?\n",
    "To understand this, observe that the `model.fit` is returning a history object which contains the record of progress of NN training.\n",
    "\n",
    "This history object contains records of loss and metrics values for each epoch. This history object is an example of something called \"callback\" (check further documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training the model for 500 epochs\n",
    "history = model.fit(x_train, y_train, epochs = 500, batch_size = 256, validation_split = 0.1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are plots of weights and biases after training. This is to see if there are any differences post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize = (5, 5))\n",
    "fig.subplots_adjust(hspace = 0.5, wspace = 0.5)\n",
    "# extracting weights from the layers\n",
    "weight_layers = [layer for layer in model.layers]\n",
    "for i, layer in enumerate(weight_layers):\n",
    "    for j in [0, 1]:\n",
    "        axes[i, j].hist(layer.weights[j].numpy().flatten(), align = \"left\")\n",
    "        axes[i, j].set_title(layer.weights[j].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights now follow normal distribution and the biases are non-zero.\n",
    "\n",
    "Each iteration is 1 forward + 1 backward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `validation_data`\n",
    "- Validation set can also be used explicitly by using the `validation_data` argument in the `fit` method.\n",
    "- But, the model will have to be re-initialized after defining with `validation_data`, else, the model will start getting trained from its current stage.\n",
    "- Ideally, the code should be written as a function so that the model definition would not have to be written over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = \"relu\", input_shape = (11, ), name = \"hidden_1\"),\n",
    "            Dense(16, activation = \"relu\", name = \"hidden_2\"),\n",
    "            Dense(4, activation = \"softmax\", name = \"output\")\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics = [\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`verbose` is now set to `0` to make the training process silent. This prevents the printing of loss and other metrics for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data = (x_val, y_val), epochs = 500, batch_size = 512, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieve all the keys associated with the history object, `__dict__` attribute can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the history object's dictionary has another dictionary with \"`history`\" inside it.\n",
    "\n",
    "This can be called as it is available in keys associated with `model.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.fit` has saved all the loss and metric values for each epoch inside the `history` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = history.epoch\n",
    "loss = history.history[\"loss\"]\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss and accuracy curves fro both training and validation data\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, label = \"training\")\n",
    "plt.plot(epochs, val_loss, label = \"validation\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss VS Epochs\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, accuracy, label = \"training\")\n",
    "plt.plot(epochs, val_accuracy, label = \"validation\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy VS Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations,\n",
    "- Both training and validation loss decrease with epochs.\n",
    "- After around 120 epochs, training loss still keeps on decreasing but validation loss starts to increase.\n",
    "- This means that the model start to overfit the training dataset after 120 epochs. Meaning, the params learnt after 120 epochs are the reason for overfitting on training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Functional API\n",
    "Consider the following model,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_59.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "The above model cannot be designed using Sequential API, as there no way to pass 2 inputs to 1 layer in Sequential API.\n",
    "\n",
    "The Functional API of Keras is instead used to design such complex models.\n",
    "\n",
    "### Why is Functional API needed instead of Sequential API?\n",
    "Functional API gives more flexibility. This API can handle multiple inputs and outputs. Say that there is image and text description as the training or there is a need of a model that outputs 2 or more target variables, for example, a weather forecast model predicting minimum and maximum temperature at the same time, a Sequential API will not be able to do this.\n",
    "\n",
    "The following image displays why Sequential API cannot be used for complex tasks.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_60.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "Functional API gives more flexibility for network architectures and architectures are not always in sequential manner, there can be 2 layers in parallel.\n",
    "\n",
    "Although it is recommended to use the simplest methods while building networks (according to Occam's Razor), there sometimes might be a requirement for a more flexible approach while dealing with complex problems that require complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model_sequential = Sequential(\n",
    "    [\n",
    "        Dense(16, activation = \"relu\", input_shape = (11, ), name = \"hidden_1\"),\n",
    "        Dense(8, activation = \"relu\", name = \"hidden_2\"),\n",
    "        Dense(4, activation = \"softmax\", name = \"output\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, models have been created using Sequential API.\n",
    "\n",
    "In the following lines, the same model is created using Functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In models created using Sequential API, the input shape was passed in the first layer. In the following models, an additional Input layer which explicitly represents the input data will be used.\n",
    "\n",
    "Instead of `Sequential` class, the `Model` class from `tensorflow.keras.models.Model` will be used in Functional API.\n",
    "\n",
    "At first an input layer with the shape of the dataframe is created,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape = (11, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the first 2 layers of the model are created. In here, the previous layer is passed in the current layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = Dense(16, activation = \"relu\", name = \"hidden_1\")(inp)\n",
    "h2 = Dense(4, activation = \"relu\", name = \"hidden_2\")(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last the final output layer is created. `hidden_2` layer is passed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Dense(4, activation = \"softmax\", name = \"output\")(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the flow of the model, the model is built using `tensorflow.keras.models.Model`. All the inputs and the output are passed here (refer the code in the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model_functional = Model(inputs = inp, outputs = out, name = \"simple_nn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalizing the code within a method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_functional():\n",
    "\n",
    "    inp = Input(shape = (11, 1))\n",
    "\n",
    "    h1 = Dense(16, activation = \"relu\", name = \"hidden_1\")(inp)\n",
    "    h2 = Dense(8, activation = \"relu\", name = \"hidden_2\")(h1)\n",
    "    \n",
    "    out = Dense(4, activation = \"softmax\", name = \"output\")(h2)\n",
    "\n",
    "    model = Model(inputs = inp, outputs = out, name = \"simple_nn\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model_functional = create_model_functional()\n",
    "model_functional.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_functional, show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a complex model using Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_multiple_outputs():\n",
    "\n",
    "    inp = Input(shape = (11, 1))\n",
    "\n",
    "    h1 = Dense(16, activation = \"relu\", name = \"hidden_1\")(inp)\n",
    "    h2 = Dense(8, activation = \"relu\", name = \"hidden_2\")(h1)\n",
    "    h3 = Dense(4, activation = \"relu\", name = \"hidden_3\")(h2)\n",
    "\n",
    "    out1 = Dense(1, activation = \"sigmoid\", name = \"output_1\")(h3)\n",
    "    out2 = Dense(1, activation = \"relu\", name = \"output_2\")(h3)\n",
    "\n",
    "    model = Model(inputs = inp, outputs = [out1, out2], name = \"simple_nn\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model_multiple_outputs = create_model_multiple_outputs()\n",
    "\n",
    "tf.keras.utils.plot_model(model_multiple_outputs, show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction And Evaluation\n",
    "The predictions are made using the model created using the Sequential API.\n",
    "\n",
    "### Model Evaluation\n",
    "`model.evaluate(x_test, y_test)`\n",
    "- This returns the loss value and the metrics value for the model.\n",
    "- It is important to note that, weights or parameters are not updated during evaluation (and prediction).\n",
    "- This by extension means that only forward pass (propagation) is occurring and no backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train)\n",
    "print(\"Train Set\")\n",
    "print(f\"Loss value = {loss}\")\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_val, y_val)\n",
    "print(\"Validation Set\")\n",
    "print(f\"Loss value = {loss}\")\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Set\")\n",
    "print(f\"Loss value = {loss}\")\n",
    "print(f\"Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction\n",
    "The `predict` method is used to get the predictions on unseen data. Raw output is returned from the model (i.e., probabilities of an observation belong to each one of the 4 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities of an observation belonging to each 1 of the 4 classes is equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pred, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class to which an observation belongs to can be found by finding the index having the highest probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = np.argmax(pred, axis = 1)\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cross-verify, the accuracy of the model can be checked using the `sklearn.metrics.accuracy_score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "In the earlier part of this document,\n",
    "- with `verbose = 1`, model training prints the associated data after every epoch.\n",
    "- with `verbose = 0`, model training prints nothing.\n",
    "\n",
    "Callbacks are used to customize the printing behavior.\n",
    "\n",
    "### What are callbacks?\n",
    "A callback defines a set of function which are executed at different stages of the training procedure. For example, a callback function may run,\n",
    "1. A function `on_epoch_begin` before every epoch.\n",
    "2. A function `on_epoch_end` after every epoch.\n",
    "\n",
    "### How can these callbacks be useful?\n",
    "They can be used to view internal states of the model during training. For example, there might be a need to print loss, accuracy or learning rate after every 2000-th epoch. For this, a condition, `if epoch % 2000 == 0:`, then perform a certain task like the ones mentioned above may be added.\n",
    "\n",
    "In the code below a customized callback class is created to print the loss and accuracy for every 50-th epoch. The custom class will inherit from `tensorflow.keras.callbacks.Callback`. This means that all the attributes and methods available in the `keras.callbacks.Callback` class will be available for the customized class (they can also be overridden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerboseCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    # runs only before the training starts\n",
    "    def on_train_begin(self, logs = None):\n",
    "        print(\"Start of training\")\n",
    "\n",
    "    # runs after every epoch\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch - {str(epoch).zfill(3)}, Loss - {logs['loss']}, Accuracy - {logs['accuracy']}\")\n",
    "\n",
    "    # runs once training is finished\n",
    "    def on_train_end(self, logs = None):\n",
    "        print(\"End of training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, a sentence is being printed at the start of training and also at the end of training. Apart from this, the loss and accuracy is printed after every 50-th percentile\n",
    "\n",
    "`logs` is a dictionary that callback method takes as an argument that will consist of the keys for quantiles relevant to the current batch or epoch like loss, accuracy, etc.\n",
    "\n",
    "A list of callback objects will have to be passed to the `callbacks` argument of the `fit` method. Optionally, callback objects can be passed to `evaluate` and `predict` method as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 500, batch_size = 256, validation_split = 0.1, verbose = 0, callbacks = [VerboseCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parent class `tf.keras.callbacks.Callbacks` supports various kinds of methods which can be overidden.\n",
    "- Global methods, at the beginning or ending of training.\n",
    "- Batch-level methods, at the beginning or ending of a batch.\n",
    "- Epoch-level method, at the beginning or ending of an epoch.\n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs = None):\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs = None):\n",
    "        print(f\"Training: Starting batch {batch}\")\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs = None):\n",
    "        print(f\"Training: Finished batch {batch}, loss is {logs['loss']}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        print(f\"Finished epoch {epoch}, loss is {logs['loss']}, accuracy is {logs['accuracy']}\")\n",
    "\n",
    "    def on_train_end(self, logs = None):\n",
    "        print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some pre-defined callback classes such as `CSVLogger`, `EarlyStopping`, `LearningRateScheduler`. They can be useful for various tasks, for example, `EarlyStopping` can be used to stop the training process as soon as the validation loss starts to increase.\n",
    "\n",
    "Other examples include,\n",
    "- `CSVLogger`: Save history object in a `.csv` file, `csv_logger = keras.callbacks.CSVLogger(\"file_name.csv\")`.\n",
    "- `EarlyStopping`: Stop the training when the model starts to overfit.\n",
    "- `ModelCheckpoint`: Saves the intermediate model weights.\n",
    "- `LearningRateScheduler`: Control or change the learning rate between epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "It is always a good practice to closely monitor the related parameters in the training process, like, changes in the loss, performance or any changes in any parameters. The values of these can be extracted using the `history` method, but they must be explictly plotted to visualize them.\n",
    "\n",
    "TensorBoard is a dashboard that allows to visualize information regarding the training process like,\n",
    "- Metrics: Loss, Accuracy.\n",
    "- Visualize the model graphs.\n",
    "- Histograms of $w$, $b$ or other tensors as they change during the training.\n",
    "- Displaying images, text and audio data.\n",
    "\n",
    "### Installation\n",
    "- Using PIP: `pip install tensorboard`.\n",
    "- In Conda Environment: `conda install -c conda-forge tensorboard`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading TensorBoard in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A log directory, say `logs`, needs to be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard will sotre all the logs in in this log directory. It will read from these logs in order to display the various visualizations. The `reload_ext` magic method is used to reload the TensorBoard extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current logs (if there are any) have to be cleared, before saving new logs to the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.callbacks.TensorBoard` has to be imported inorder to use TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback arguments\n",
    "- `log_dir` (Path): Specifies the directory where training logs will be saved. This directory should be dedicated solely to the callback's logging purposes.\n",
    "- `update_freq` (Integer or String): Controls the frequency of loss and metric updates during training.\n",
    "    - `batch`: Updates are performed after every batch or iteration.\n",
    "    - `N` (integer): Updates occur every `N` batches.\n",
    "    - `epoch`: Updates are made at the end of each epoch.\n",
    "- `histogram_freq` (Integer): Determines how often histograms of weight distributions are computed and saved. A value of `0` indicates that histograms will not be generated.\n",
    "- `write_graph` (Boolean): Enables or disables visualization of the training graph.\n",
    "    - `True`: Visualizes the training process.\n",
    "    - `False`: No visualization is generated.\n",
    "- `write_images` (Boolean): Controls the visualization of model weights.\n",
    "    - `True`: Visualizes the model weights.\n",
    "    - `False`: No weight visualization is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = TensorBoard(log_dir = log_folder, histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be trained again, this time using Tensorboard callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "history = model.fit(x_train, y_train, epochs = 500, batch_size = 512, validation_data = (x_val, y_val), verbose = 0, callbacks = [tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the TensorBoard dashboard.\n",
    "- Scalars: Shows loss and metrics.\n",
    "- Graphs: Shows model training structure.\n",
    "- Distributions: Distribution of $w$ and $b$.\n",
    "- Histograms: Histograms of $w$ and $b$.\n",
    "\n",
    "TensorBoard is launched using the following command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir={log_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goto this [link](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.03035&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) to see a visual demo of neural network training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
