{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "So far, the models built had only 3 neurons. But, what if more neurons were added to the network in the model to make it more complex in order to get the model to learn more complex patterns.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_23.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "There are 2 features which are passed as inputs and 3 classes which are obtained as output.\n",
    "\n",
    "In the above figure, superscript notation is used to represent the layer. Therefore, representing with $f_1$, $f_1^1$ is used (i.e., neuron 1 of layer 1).\n",
    "\n",
    "The inputs (layer 0 or input layer) is connected with 4 neurons of layer 1. These 4 neurons are connected with 3 neurons of the output layers (layer 2).\n",
    "\n",
    "This intermediate layer in between the input and output is called hidden layer.\n",
    "\n",
    "### Why is it called hidden layer?\n",
    "Both input and output are hidden from this layer and this layer is not directly dealt with. Hence, it is called a hidden layer.\n",
    "\n",
    "The network can be made complex by increasing the number of neurons in the hidden layer or by increasing the number of hidden layers.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_24.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "### What will the activation function of the output layer be?\n",
    "Since this is a multi-class classification problem, the activation function will be softmax.\n",
    "\n",
    "### What will the activation function of the hidden layer be?\n",
    "Since the model needs to learn a non-linear decision boundary, the activation functions will also have to be non-linear.\n",
    "\n",
    "### What happens if each activation function in MLP is linear?\n",
    "Consider the following example to understand,\n",
    "\n",
    "Given, $f(x) = 2x + 1$, $g(x) = 3x + 2$, will $f(g(x))$ be linear or non-linear?\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_25.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "Composition of 2 linear function is linear. If the activation function used is linear, then the model will also be linear. But in this case, a non-linear model is required.\n",
    "\n",
    "### How can a non-linear activation function help?\n",
    "Consider, $f(x) = x^2 + 1$, $g(x) = 2x + 1$, will $g(f(x))$ be linear or non-linear?\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_26.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "### What if another non-linear function stacked?\n",
    "Consider, $h(x) = x^2 + 1$, $f(x) = 2x^2 + 1$, and $g(x) = x + 1$,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_27.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "Therefore, stacking a non-linear function over a linear function and repeating the process may create complex features.\n",
    "\n",
    "Suppose there are 2 inputs $x_1$ and $x_2$, the computation graph will look as follows,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_28.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "Explanation,\n",
    "- $z$ is the weighted sum, hence it will be linear, $z = 3x_1 + 4x_2$.\n",
    "- $a_1$ will be non-linear and complex, $a_1 = 9x_1^2 + 16x_2^2 + 24x_1x_2$.\n",
    "- $a_2$ will be non-linear and much more complex than $a_1$, $a_2 = (9x_1^2 + 16x_2^2 + 24x_1x_2 +1)^2 = (9x_1^2 + 16x_2^2)^2 + 2(9x_1^2 + 16x_2^2)(24x_1x_2 +1) + (24x_1x_2 +1)^2$\n",
    "\n",
    "### What non-linear function should be used as the activation function?\n",
    "Recall that, sigmoid is one of the non-linear activation function.\n",
    "\n",
    "### Should different activation functions be used in different layers?\n",
    "Theoritically, it is very much possible to use different activation functions in different layers. But, studies have shown that doing so does not add much value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "Sigmoid can be used as an activation function for the hidden layer. Note that,\n",
    "- Domain of sigmoid is, $(-\\infty, \\infty)$.\n",
    "- Range of sigmoid is, $(0, 1)$.\n",
    "- Derivative of sigmoid also lies between $(0, 1)$.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_29.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "Other activation functions: $\\tan{h}$\n",
    "\n",
    "### $\\tan{h}$\n",
    "- $\\tan{h}$ is a shifted version of sigmoid function.\n",
    "- $\\tan{h}$ works better than sigmoid all the time, the mean value is 0.\n",
    "- Input lies in the range, $(-\\infty, \\infty)$.\n",
    "- Output lies in the range $(-1, 1)$.\n",
    "- $\\tan{h}$ is not very used often, unless the output is required to lie in the range of $(-1, 1)$.\n",
    "- $\\tan{h}$ is mathematically represented as, $\\tan{h}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$.\n",
    "- Derivative of $\\tan{h}$ function is, $\\frac{d(\\tan{h})}{dz} = 1 - \\tan{h}^2(z)$.\n",
    "- The range of this is, $(0, 1)$.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_30.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "### Issue with sigmoid and $\\tan{h}$\n",
    "- Vanishing gradients:\n",
    "    - The downside of both sigmoid and $\\tan{h}$ is that their gradient is lesser than 1 for most of the values of z.\n",
    "    - This hampers the gradient descent process and the calculated gradients will be very small.\n",
    "\n",
    "### Why does small gradient hamper gradient descent process?\n",
    "The derivative of sigmoid and $\\tan{h}$ lies between $(0, 1)$. Therefore, the product of these terms in the range will become very small. In fact, as the number of layers in the NN increase, the product will become smaller and smaller.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_31.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "The equation to update the weight $w$ is, $w_{new} = w_{old} - \\eta * \\frac{\\partial loss}{\\partial w} \\biggr\\vert_{{w}_{old}}$.\n",
    "\n",
    "- The partial derivative value becomes miniscule as the number of layers increase.\n",
    "- As a result, the Neural Network gets trained very slowly.\n",
    "- In fact, for close to 2 decades, people could not imagine using a NN with more than 3 to 4 layers.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_32.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "### What should be the properties of an ideal activation function?\n",
    "- Differentiable.\n",
    "- Non-linear.\n",
    "- Easy to calculate.\n",
    "- Gradient is greater than or equal to 1 for a big range of $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "ReLU stands for Rectified Linear Unit. It says that for any positive value, the ReLU function returns that value as it is, i.e., $\\text{ReLU}(z) = z$, if $z > 0$. Else, the ReLU function returns a 0, i.e., $\\text{ReLU}(z) = 0$, if $z <= 0$.\n",
    "\n",
    "The ReLU function is defined as, $\\text{ReLU}(z) = \\max{(0, z)}$. This means that, for any input $z$,\n",
    "- If $z$ is positive, the output is $z$ itself.\n",
    "- If $z$ is negative or zero, the output is 0.\n",
    "\n",
    "### Derivative of ReLU\n",
    "The derivative of ReLU function is,\n",
    "\n",
    "$\\text{ReLU}'(z) = \\left\\{\\begin{matrix}1, if \\ z>0\\\\0, if \\ z<0\\end{matrix}\\right.$\n",
    "\n",
    "This means that the derivative is 1 for positive inputs and 0 for negative inputs.\n",
    "\n",
    "Since it is not continuous, it is not differentiable at 0. Therefore, an approximation is taken for it work. Hence, the derivative of ReLU is, $\\text{ReLU}'(z) = \\left\\{\\begin{matrix}1, if \\ z > 0\\\\0, if \\ z <= 0\\end{matrix}\\right.$\n",
    "\n",
    "### Why is the derivative at 0 is ignored?\n",
    "While the ReLU function is not strictly differentiable at 0, in practice, the derivative as defined above is often used. This is because,\n",
    "- Practical implementation: Most deep learning frameworks, like TensorFlow and PyTorch, implement ReLU with this derivative approximation.\n",
    "- Numerical stability: The gradient at 0 is typically a small and ignoring it does not significantly impact the training process.\n",
    "- Sparsity: The ReLU function can introduce sparsity in the network, which can help in training and regularization.\n",
    "\n",
    "### Advantage of ReLU\n",
    "- Efficient computation: ReLU is computationally efficient compared to other activation functions like sigmoid and $\\tan{h}$.\n",
    "- Reduced vanishing gradient problem: The ReLU function can help mitigate the vanishing gradient problem, especially in deeper networks.\n",
    "- Sparsity: It can introduce sparsity in the network, which can lead to faster training and better generalization.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_33.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "### Is there a problem with practically using ReLU as the activation function?\n",
    "Yes. Even though it is the most widely used activation function in Deep Learning, there is a problem with ReLU,\n",
    "- If one derivative term in calculation of $\\frac{\\partial Loss}{\\partial w}$ gets the value 0, the entire term will become 0.\n",
    "- As a result, there is no update in the value of weight.\n",
    "- This is known as \"dying ReLU\".\n",
    "- Therefore, there is a potential vanishing gradient problem.\n",
    "\n",
    "While ReLU is a widely used activation function in deep learning, it has a potential drawback known as the \"dying ReLU\" problem. If a ReLU neuron consistently receives negative inputs, its output will always be zero, and its gradient during backpropagation will also be zero. This can lead to a vanishing gradient problem, where the network fails to learn effectively.\n",
    "\n",
    "To mitigate this issue, various activation functions have been introduced, such as Leaky ReLU, Parametric ReLU, Exponential Linear Unit (ELU), and Scaled Exponential Linear Unit (SELU). These functions introduce non-linearity and help alleviate the vanishing gradient problem, enabling the training of deeper neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky ReLU\n",
    "This is very similar to ReLU. In case of negative values, a small gradient ($\\alpha$) associated with it is added instead of havinf 0.\n",
    "\n",
    "Therefore,\n",
    "$\\text{Leaky ReLU}'(z) = \\left\\{\\begin{matrix}1, if \\ z>0\\\\Î±, if \\ z<=0\\end{matrix}\\right.$\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_34.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations For N Layer NN\n",
    "As the number of layers increase, the earlier weight notation will not work. Therefore, layer number is added to weight notation as well.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_35.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "For weight, $w_{ij}^L$, where,\n",
    "- $L$ represents the layer number.\n",
    "- $i$ represents the from Neuron.\n",
    "- $j$ represents the to Neuron.\n",
    "\n",
    "For bias, $b_i^L$, where,\n",
    "- $L$ represents the layer number.\n",
    "- $i$ represents the Neuron.\n",
    "\n",
    "There are 2 sets of weights and biases for the first and second layers, $w^1$, $w^2$, $b^1$, $b^2$.\n",
    "\n",
    "### Dimensions of $w^1$, $w^2$, $b^1$, $b^2$\n",
    "- $w^1$:\n",
    "    - There are 2 inputs that are fed to 4 Neurons.\n",
    "    - Each pair of input and Neuron has a unique weight value.\n",
    "    - Therefore, $w^1$ will be a `2x4` matrix, $w^1 = \\begin{bmatrix}w_{11}^1 & w_{12}^1 & w_{13}^1 & w_{14}^1\\\\w_{21}^1 & w_{22}^1 & w_{23}^1 & w_{24}^1\\end{bmatrix}_{2 \\times 4}$.\n",
    "- $b^1$:\n",
    "    - There are 4 Neurons in the first layer.\n",
    "    - Each of which has a unique bias value.\n",
    "    - Therefore, $b^1$ will be a `1x4` matrix, $b^1 = \\begin{bmatrix}b_1^1 & b_2^1 & b_3^1 & b_4^1\\end{bmatrix}_{1 \\times 4}$.\n",
    "- $w^2$:\n",
    "    - The outputs of layer 1 are inputs to layer 2.\n",
    "    - Layer 2 Neurons do not need to know if its an output from an earlier layer or a raw input.\n",
    "    - Here 4 inputs are fed to 3 neurons.\n",
    "    - Therefore, $w^2$ will be a `4x3` matrix, $w^2 = \\begin{bmatrix}w_{11}^2 & w_{12}^2 & w_{13}^2\\\\w_{21}^2 & w_{22}^2 & w_{23}^2\\\\w_{31}^2 & w_{32}^2 & w_{33}^2\\\\w_{41}^2 & w_{42}^2 & w_{43}^2\\end{bmatrix}_{4 \\times 3}$.\n",
    "- $b^2$:\n",
    "    - There are 3 neurons in the second layer, each of which will have a bias associated with them.\n",
    "    - Therefore, $b^2$ will be a `1x3` matrix, $b^2 = \\begin{bmatrix}b_1^2 & b_2^2 & b_3^2\\end{bmatrix}_{1 \\times 3}$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
