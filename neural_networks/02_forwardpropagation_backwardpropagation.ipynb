{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building A Model For Multi-Class Classification\n",
    "A simple Logistic Regression cannot be used to for multi-class classification. A simple Logistic Regression is equivalent to single-neuron model and can only be used to classify 2 classes. In order to implement Logistic Regression for multi-class classification multiple Logistic Regression models will have to be built (OvR).\n",
    "\n",
    "Therefore, if there are 3 classes, then 3 models will have to be trained.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_11.png\" alt = \"drawing\" width = \"500\"/>\n",
    "\n",
    "### Can the above be accomplished using a single model?\n",
    "In multi-class classification,\n",
    "- The probability of a given data point belonging to either one of the classes (A, B or C) is calculated.\n",
    "- The data point would be classified into the class for which the probability value was the highest.\n",
    "\n",
    "An intuition that the output layer should have 3 outputs (one for each class) can be drawn from the above.\n",
    "\n",
    "Therefore, the Neural Network looks as follows,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_12.png\" alt = \"drawing\" width = \"500\"/>\n",
    "\n",
    "Observations,\n",
    "- The number of outputs are the same.\n",
    "- The number of connections are the same.\n",
    "\n",
    "The difference,\n",
    "- Computation happens together.\n",
    "\n",
    "So, instead of a weight vector, the weight matrix is multiplied with data matrix.\n",
    "\n",
    "A model built by using multiple neurons is called a Neural Network (NN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notations In Neural Networks\n",
    "### Inputs\n",
    "Consider that there are 2 features, $x_1$ and $x_2$ and $m$ data points. This is represeted as a matrix where each row is a data point. It looks as,\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & x_{12} \\\\\n",
    "x_{21} & x_{i2} \\\\\n",
    "...  & ...  \\\\\n",
    "x_{m1} & x_{m2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Neuron\n",
    "Neuron is represented using $f_i$. \n",
    "\n",
    "Where,\n",
    "- $i$ = neuron number. For example, $f_1$ is representing the first neuron.\n",
    "\n",
    "### Weights\n",
    "Weights are defined by the notation $w_{ij}$.\n",
    "\n",
    "Where,\n",
    "- $i$ = source neuron\n",
    "- $j$ = destination neuron\n",
    "\n",
    "The weight associated with input $x_1$ going to neuron $f_2$ is represented as $w_{12}$. Similarly, other weights are represented as, $w_{11}$, $w_{13}$, $w_{21}$, $w_{22}$, $w_{23}$.\n",
    "\n",
    "### Bias\n",
    "Each neuron will have a bias term associated with it. The bias matrix is represented as,\n",
    "\n",
    "$b = \\begin{bmatrix}\n",
    "b_1 \\space b_2 \\space b_3\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### Z value\n",
    "Z value represents the linear operation, i.e., additive multiplication of inputs with their respective weights.\n",
    "- $z_1 = w_{11} * x_1 + w_{21} * x_2$.\n",
    "- $z_2 = w_{12} * x_1 + w_{22} * x_2$.\n",
    "- $z_3 = w_{13} * x_1 + w_{23} * x_2$.\n",
    "\n",
    "### Output\n",
    "Each neuron will apply its activation function on the x values to the outputs: $a_1'$, $a_2'$, $a_3'$.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_13.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "### Problem with this formulation\n",
    "What if the model predict a probability value greater than 0.5 for multiple classes (i.e., $A$, $B$, $C$ = $\\begin{bmatrix}1 \\space 1 \\space 0\\end{bmatrix}$ or $A$, $B$, $C$ = $\\begin{bmatrix}1 \\space 1 \\space 1\\end{bmatrix}$)?\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_14.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "The actual requirement is that, the sum of all output probabilities should be equal to 1.\n",
    "\n",
    "### How can multiple outputs be 1?\n",
    "Since the sigmoid activation function is applied to each output and the range of the sigmoid function is, $\\sigma \\in (0, 1)$. Therefore, multiple probabilitiy values can be greater than 0.5 and hence multiple class labels can be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier\n",
    "Consider 3 outputs, $z_1$, $z_2$ and $z_3$. A function that should map $z_1$, $z_2$ and $z_3$ such that the sum total of the output probabilities is equal to 1 is required.\n",
    "\n",
    "The softmax function helps in achieving exactly this. The softmax function is mathematicall represented as,\n",
    "\n",
    "$p_i = \\frac{e^{z_i}}{\\sum_{i = 0}^{k} e^{z_i}}$.\n",
    "\n",
    "Here $p_i$ refers to the probability of the data point belonging to class i. There denominator in the equation is the normalization term to make $p_1 + p_2 + p_3 = 1$.\n",
    "\n",
    "Softmax can be thought of as sigmoid like function for multi-class setting.\n",
    "\n",
    "### Why not directly use $\\frac{{z_i}}{\\sum_{i = 0}^{k} {z_i}}$? Why exponentiate $z_i$?\n",
    "The intuitive reason is that it ensures that values are non-negative and lie only between 0 and 1 (the value of $z_i$ ranges from $-\\infty\\$ and $\\infty$).\n",
    "\n",
    "Besides this, softmax function has some other desirable properties,\n",
    "1. It is nicely differentiable $\\frac{de^{x}}{dx} = e_x$.\n",
    "2. The output probabilities can be interpreted as log likelihoods (log odds).\n",
    "\n",
    "Now consider the following representation,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_15.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "If the normalized z values were instead of exponential,\n",
    "- The ration of the probability would be 1: 3: 6.\n",
    "- However, softmax pushes the probability of largest number closer to 1. Hence the term softmax.\n",
    "\n",
    "Hence, softmax is used as the activation function in NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Neural Network\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
