{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Introduction\n",
    "Consider the following Neural Network,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_36.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "The computation graph for the above looks as follows,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_37.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "In forward propagation, the propagation is from left to right. The following is done during a forward pass (forward propagation),\n",
    "- Calculate the value of $z_i$.\n",
    "- Apply activation function on top of it.\n",
    "- Then pass it to the Neuron in front of it.\n",
    "- Ultimately, the probabilities are obtained.\n",
    "- Then these probabilities are used to calculate the loss. Since it is multi-class classification problem, the loss function used is categorical cross entropy.\n",
    "\n",
    "The final objective is to compute $z^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000650</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.017661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007487</td>\n",
       "      <td>0.029364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2  y\n",
       "0  0.000000  0.000000  0\n",
       "1 -0.000650  0.010080  0\n",
       "2  0.009809  0.017661  0\n",
       "3  0.007487  0.029364  0\n",
       "4 -0.000027  0.040404  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spiral.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 2), (300,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating features and labels\n",
    "x = df.drop(columns = [\"y\"])\n",
    "y = df[\"y\"]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters at random\n",
    "d = 2 # dimensions or number of inputs\n",
    "n = 3 # number of classes or number of neurons in the output layer\n",
    "h = 4 # number of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4), (1, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input layer to the hidden layer\n",
    "# weight and bias of layer 1\n",
    "w1 = 0.01 * np.random.randn(d, h)\n",
    "b1 = np.zeros((1, h))\n",
    "w1.shape, b1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $z^1$\n",
    "Each row of $x$ is multiplied with each column of $w_1$ and bias is added to the result of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z1 = np.dot(x, w) + b\n",
    "z1 = np.dot(x, w1) + b1\n",
    "z1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $a^1$\n",
    "The ReLU function is applied to $z^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReLU activation function\n",
    "a1 = np.maximum(0, z1)\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (1, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden layer to the output layer\n",
    "# weight and bias of layer 2\n",
    "w2 = 0.01 * np.random.randn(h, n)\n",
    "b2 = np.zeros((1, n))\n",
    "w2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $z^2$\n",
    "In order to calculate $z^2$, $a^1$ is multiplied with $w_2$ and the bias $b^2$ is added to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = np.dot(a1, w2) + b2\n",
    "z2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $a^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the softmax function to compute a2\n",
    "z2_exp = np.exp(z2)\n",
    "a2 = z2_exp/ np.sum(z2_exp, axis = 1, keepdims = True)\n",
    "probs = a2\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../artifacts/neural_networks_38.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Calculation\n",
    "### Will the loss function change?\n",
    "No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "### Will the gradient calculation change in case of n layer Neural Network?\n",
    "No. But, there is an additional requirement to back propagate the gradients for one additional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of data points (training samples)\n",
    "m = y.shape[0]\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $dz^2$\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_39.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "$dz^2 = \\frac{\\partial L}{\\partial z^2}$\n",
    "\n",
    "So,\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z^2} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2}$\n",
    "\n",
    "Here, $a^2$ is the output probabilities.\n",
    "\n",
    "Replace $a^2$ with $p$, $\\frac{\\partial L}{\\partial z^2} = \\frac{\\partial L}{\\partial p} * \\frac{\\partial p}{\\partial z^2}$\n",
    "\n",
    "The above equation is similar to what was calculated previously, i.e., derivative of loss with respect to $z$.\n",
    "\n",
    "$dz = \\frac{\\partial J}{\\partial p} * \\frac{\\partial p}{\\partial z}$.\n",
    "\n",
    "The derivative came out to be, $dz = (p_i - I(i = y))$\n",
    "\n",
    "Hence, $dz^2 = (p_i - I(i = y))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz2 = probs\n",
    "dz2[range(m), y] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of $dz^2$ is the same as the shape of probabilities, `(m, n)` (i.e., in this case `(300, 3)`).\n",
    "\n",
    "### Calculating $dw^2$ and $db^2$\n",
    "Gradient calculation for $dw^2$ and $db^2$ will also be similar to $dw$ and $db$ as it was in the softmax classifier.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_40.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "$dw^2 = \\frac{\\partial L}{\\partial w^2} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial w^2}$.\n",
    "\n",
    "$dw^2 = dz^2 * \\frac{\\partial z^2}{\\partial w^2}$.\n",
    "\n",
    "Here, $z^2 = w^{2^T} * a^1 + b^2$.\n",
    "\n",
    "So, $\\frac{\\partial z^2}{\\partial w^2} = a^1$\n",
    "\n",
    "$dw^2 = \\frac{\\partial L}{\\partial w^2} = dz^2 * a^1$\n",
    "\n",
    "The shape of $dz^2$ = `(300, 3)` and the shape of $a^1$ = `(300, 4)`.\n",
    "\n",
    "$dw^2$ will be used to update $w^2$. Therefore, the shape of $dw^2$ should be same as $w^2$, i.e., `(4, 3)`.\n",
    "\n",
    "Hence, $dz^2$ and $a^1$ should be multiplied such that, the resulting matrix has the shape `(4, 3)`.\n",
    "\n",
    "Therefore, the transpose of $a^1$ is multiplied with $dz^2$, $a^{1^T} * dz^2$.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_41.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw2 = np.dot(a1.T, dz2)/ m\n",
    "dw2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The division by `m` is because, in Gradient Descent, since all the data points are used for calculating the updated $w$, the average is taken by dividing the total number of data points.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_42.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "Why is there a need to divide by m? The goal is to update weights and biases, it can be done by,\n",
    "1. calculating the derivatives $dw^2$, $db^2$, $dw^1$, $db^1$.\n",
    "2. Updating the weights, $w^1 = w^1 - \\eta * dw^1 *\\frac{1}{m}$.\n",
    "\n",
    "$db^2$ can also be calculated in a similar way.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_43.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "$db^2 = \\frac{\\partial L}{\\partial b^2} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial b^2}$\n",
    "\n",
    "Now, $\\frac{\\partial z^2}{\\partial b^2} = \\frac{\\partial (w^2 * a^1 + b^2)}{db^2} = 1$.\n",
    "\n",
    "$db^2 = \\frac{\\partial L}{\\partial b^2} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * 1 = dz^2$.\n",
    "\n",
    "$db^2$ will be used to update $b^2$. Therefore, the shape of $db^2$ will be same as $b^2$, i.e., `(1, 3)`.\n",
    "\n",
    "But the shape of $dz^2$ is `(300, 3)`. Since gradient descent and not stochastic gradient descent is being performed, the derivatives have to summed up across the rows and then average of them has to taken before using it for the updating.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_44.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2 = np.sum(dz2, axis = 0, keepdims = True)/ m\n",
    "db2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $da^1$\n",
    "<img src = \"../artifacts/neural_networks_45.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "$da^1 = \\frac{\\partial L}{\\partial a^1} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^1} * \\frac{\\partial z^2}{\\partial a^1}$\n",
    "\n",
    "Since, $\\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} = sz^2$.\n",
    "\n",
    "Now,\n",
    "\n",
    "$\\frac{\\partial z^2}{\\partial a^1} = \\frac{\\partial (w^2 * a^1 + b^2)}{da^1} = w^2$.\n",
    "\n",
    "$da^1 = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * w^2 = dz^2 * w^2$.\n",
    "\n",
    "The shape of $da^1$ will be same as $a^1$, i.e., `(300, 4)`.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_46.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "The shape of $dz^2$ = `(300, 3)` and the shape of $w^2$ = `(4, 3)`\n",
    "\n",
    "$dw^2$ will be used to update $w^2$. Therefore, the shape of $dw^2$ should be same as $w^2$, i.e., `(4, 3)`.\n",
    "\n",
    "Hence, $dz^2$ and $w^2$ should be multiplied such that, the resulting matrix has the shape `(4, 3)`.\n",
    "\n",
    "Therefore, the transpose of $w^2$ is multiplied with $dz^2$, $da^1 = w^{2^T} * dz^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da1 = np.dot(dz2, w2.T)\n",
    "da1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $dz^1$\n",
    "To calculate the gradient of $dz^1$, the ReLU layer has to to passed backwards.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_47.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "$\\frac{\\partial L}{\\partial z^1} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial a^1} * \\frac{\\partial a^1}{\\partial z^1}$\n",
    "\n",
    "It is known that, $\\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial a^1} = da^1$\n",
    "\n",
    "$\\frac{\\partial a^1}{\\partial z^1}$ has to be calculated.\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_48.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_49.png\" alt = \"drawing\" width = \"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da1[z1 <= 0] = 0\n",
    "dz1 = da1\n",
    "dz1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is $da^1$ being directly updated without creating a copy of it?\n",
    "- The purpose of calculating $da^1$ and $dz^1$ is to ultimately calculate $dw^1$ and $db^1$. Both of them are being used for intermediatory purpose.\n",
    "- Therefore, making changes in $da^1$ will not change anything as $dz^1$ is already calculated.\n",
    "- And $da^1$ will not be used anywhere else expect for calculation of $dz^1$.\n",
    "\n",
    "This also means that the intermediate output values from the forward pass have to be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating $dw^1$ and $db^1$\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_50.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_51.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w^1} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial a^1} * \\frac{\\partial a^1}{\\partial z^1} * \\frac{\\partial z^1}{\\partial w^1}$.\n",
    "\n",
    "It is known that, $\\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial a^1} * \\frac{\\partial a^1}{\\partial z^1} = dz^1$.\n",
    "\n",
    "$\\frac{\\partial z^1}{\\partial w^1}$ has to be calculated.\n",
    "\n",
    "$\\frac{\\partial z^1}{\\partial w^1} = \\frac{\\partial (w^1 * x + b^1)}{\\partial w^1} = x$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w^1} = dz^1 * x$.\n",
    "\n",
    "$db^1$ can similarly be calculated as,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_52.png\" alt = \"drawing\" width = \"500\">\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b^1} = \\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial a^1} * \\frac{\\partial a^1}{\\partial z^1} * \\frac{\\partial z^1}{\\partial b^1}$.\n",
    "\n",
    "It is known that, $\\frac{\\partial L}{\\partial a^2} * \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial a^1} * \\frac{\\partial a^1}{\\partial z^1} = dz^1$.\n",
    "\n",
    "$\\frac{\\partial z^1}{\\partial b^1}$ has to be calculated.\n",
    "\n",
    "$\\frac{\\partial z^1}{\\partial b^1} = \\frac{\\partial (w^1 * x + b^1)}{\\partial b^1} = 1$\n",
    "\n",
    "Therefore, $\\frac{\\partial L}{\\partial b^1} = dz^1 * 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 4), (1, 4))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw1 = np.dot(x.T, dz1)/ m\n",
    "db1 = np.sum(dz1, axis = 0, keepdims = True)/ m\n",
    "dw1.shape, db1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the gradients have been found, the weights and biases can be updated as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.01956925, -0.01055101, -0.00950208,  0.01987932],\n",
       "        [-0.00500924,  0.00625357,  0.00573331,  0.00569931]]),\n",
       " array([[-5.63074880e-04, -1.56068427e-03, -1.78393075e-05,\n",
       "         -4.18104901e-05]]),\n",
       " array([[-0.01591582, -0.00317806, -0.02072609],\n",
       "        [ 0.01133266, -0.00710962, -0.00109631],\n",
       "        [ 0.00953313,  0.01570165, -0.00829044],\n",
       "        [ 0.00407602,  0.02038706,  0.00403669]]),\n",
       " array([[-1.30325831e-06, -2.09805232e-05,  2.22837815e-05]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the parameters\n",
    "w1 += -lr * dw1\n",
    "b1 += -lr * db1\n",
    "w2 += -lr * dw2\n",
    "b2 += -lr * db2\n",
    "w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters are updated until the convergence takes place (error goes down)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the entire process\n",
    "A single gradient descent for weight update looks as follows,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_53.png\" alt = \"drawing\" width = 500>\n",
    "\n",
    "The derivatives are as follows,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_54.png\" alt = \"drawing\" width = 500>\n",
    "\n",
    "Notice that,\n",
    "- $dz^2$ is used for the calculation of $dw^2$, $db^2$ and $da^1$.\n",
    "- Similarly, $da^1$ is used for the calculation of $dz^1$.\n",
    "- And, $dz^1$ is used used for the calculation of $dw^1$ and $db^1$.\n",
    "\n",
    "In order to not calculate the values of deeper derivatives, i.e., $da^1$, $dz^1$ over and over again, the derivatives of deeper layers are calculated and stored. The stored values can be used to calculate the derivative of the shallow layers. This is called memoization, it is also used in dynamic programming.\n",
    "\n",
    "The following is the simplified flowchart of single cycle of updation,\n",
    "\n",
    "<img src = \"../artifacts/neural_networks_55.png\" alt = \"drawing\" width = 500>\n",
    "\n",
    "During forward propagation,\n",
    "- The values of $z^j$, $w^j$, $b^j$ in order to use them during back propagation.\n",
    "- For example, $da^1$ used $w^2$ for its calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
